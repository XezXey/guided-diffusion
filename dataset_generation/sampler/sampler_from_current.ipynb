{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] V08 has 36566 sj...\n",
      "[#] from path: /data/mint/dataset_generation_symlink//mount/v08/random_target//log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250///ema_085000/train/render_face/reverse_sampling/*\n",
      "[#] V10 has 38858 sj...\n",
      "[#] from path: /data/mint/dataset_generation_symlink//mount/v10/random_target//log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250///ema_085000/train/render_face/reverse_sampling/*\n",
      "[#] V09 has 60000 sj...\n",
      "[#] from path: /data/mint/dataset_generation_symlink//mount/v09/random_target//log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250///ema_085000/train/render_face/reverse_sampling/*\n",
      "[#] Total time: 1.440260648727417\n",
      "[#] Total sj: 60000\n",
      "[#] Unvisited sj: set()\n",
      "[#] Unvisited sj: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, json\n",
    "import argparse\n",
    "import os, time, tqdm, datetime\n",
    "import subprocess\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--vid', type=int, nargs='+', default=[])\n",
    "parser.add_argument('--local', action='store_true', default=False)\n",
    "parser.add_argument('--mount_dir', required=True)\n",
    "parser.add_argument('--dataset_dir', required=True)\n",
    "parser.add_argument('--curr_vid', type=int, default=9)\n",
    "parser.add_argument('--gen_missing_sj', action='store_true', default=False)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    vid = [8, 10],\n",
    "    curr_vid = 9,\n",
    "    mount_dir = '/data/mint/dataset_generation_symlink/',\n",
    "    dataset_dir = '/data/mint/dataset_generation/',\n",
    "    gen_missing_sj = False,\n",
    ")\n",
    "\n",
    "# Generate dataset path\n",
    "name = 'random_target'\n",
    "model_name = \"/log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250/\"\n",
    "misc = \"/ema_085000/train/render_face/reverse_sampling\"\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    t_start = time.time()\n",
    "    sj_dict_v = {}\n",
    "    sj_dict_all = {}\n",
    "    for id in args.vid + [args.curr_vid]:\n",
    "        sj_dict_v[id] = {}\n",
    "        sj_folder = glob.iglob(f'{args.mount_dir}/mount/v{id:02d}/{name}/{model_name}/{misc}/*')\n",
    "        for sj in sj_folder:\n",
    "            if 'filebrowser' in sj: continue\n",
    "            sj_name = sj.split('/')[-1]\n",
    "            target_name = glob.iglob(f'{args.mount_dir}/mount/v{id:02d}/{name}/{model_name}/{misc}/{sj_name}/*')\n",
    "            sj_dict_v[id][sj_name] = 1\n",
    "            if sj_name not in sj_dict_all.keys():\n",
    "                sj_dict_all[sj_name] = {'count': 1}\n",
    "            else: \n",
    "                sj_dict_all[sj_name]['count'] += 1\n",
    "        print(f\"[#] V{id:02d} has {len(sj_dict_v[id])} sj...\")\n",
    "        print(f\"[#] from path: {args.mount_dir}/mount/v{id:02d}/{name}/{model_name}/{misc}/*\")\n",
    "            \n",
    "    print(f\"[#] Total time: {time.time() - t_start}\")\n",
    "    print(f\"[#] Total sj: {len(sj_dict_all.keys())}\")\n",
    "    \n",
    "    template_sj = dict.fromkeys([f'src={i}.jpg' for i in range(0, 60000)])\n",
    "    unvisited_sj = set(template_sj.keys()) - set(sj_dict_all.keys())\n",
    "    print(f\"[#] Unvisited sj: {unvisited_sj}\")\n",
    "    print(f\"[#] Unvisited sj: {len(unvisited_sj)}\")\n",
    "    \n",
    "    if args.gen_missing_sj:\n",
    "        # Find index of unvisited sj in /home/mint/Dev/DiFaReli/difareli-faster/dataset_generation/sampler/generated_dataset_seed=47.json\n",
    "        with open('/home/mint/Dev/DiFaReli/difareli-faster/dataset_generation/sampler/generated_dataset_seed=47.json', 'r') as f:\n",
    "            dat = json.load(f)\n",
    "            \n",
    "        # Find pair-id based on unvisited_sj\n",
    "        found_pairs = []\n",
    "        found_id = []\n",
    "        found_pairs_json = {'pair': {}}\n",
    "        \n",
    "        for sj in tqdm.tqdm(list(unvisited_sj)):\n",
    "            for pair_id, pair_data in dat['pair'].items():\n",
    "                if f'src={pair_data[\"src\"]}' == sj:\n",
    "                    found_pairs.append(pair_id)\n",
    "                    found_id.append(pair_data[\"src\"].split('=')[-1].split('.')[0])\n",
    "                    found_pairs_json['pair'][pair_id] = pair_data\n",
    "                    break\n",
    "\n",
    "        # Print or use the found pair-ids as needed\n",
    "        print(\"[#] Found pair-ids:\", found_pairs)\n",
    "        print(\"[#] Found pair-ids:\", len(found_pairs))\n",
    "        found_pairs_json['time'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        with open(f'/home/mint/Dev/DiFaReli/difareli-faster/sample_scripts/dataset_generation/sample_json/fill_sj_generated_dataset_seed=47.json', 'w') as f:\n",
    "            json.dump(found_pairs_json, f, indent=4)\n",
    "            \n",
    "    # Just get the first pair of each sj to fulfill the coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36566it [00:18, 1965.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] V08 has 36566 sj...\n",
      "[#] from path: /data/mint/dataset_generation_symlink//mount/v08/random_target//log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250///ema_085000/train/render_face/reverse_sampling/*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38859it [00:25, 1531.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] V10 has 38858 sj...\n",
      "[#] from path: /data/mint/dataset_generation_symlink//mount/v10/random_target//log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250///ema_085000/train/render_face/reverse_sampling/*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [00:01, 32332.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] V09 has 60000 sj...\n",
      "[#] from path: /data/mint/dataset_generation_symlink//mount/v09/random_target//log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250///ema_085000/train/render_face/reverse_sampling/*\n",
      "[#] Total time: 45.855520486831665\n",
      "[#] Total sj: 60000\n",
      "[#] Unvisited sj: set()\n",
      "[#] Unvisited sj: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, json\n",
    "import argparse\n",
    "import os, time, tqdm, datetime\n",
    "import subprocess\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--vid', type=int, nargs='+', default=[])\n",
    "parser.add_argument('--local', action='store_true', default=False)\n",
    "parser.add_argument('--mount_dir', required=True)\n",
    "parser.add_argument('--dataset_dir', required=True)\n",
    "parser.add_argument('--curr_vid', type=int, default=9)\n",
    "parser.add_argument('--gen_missing_sj', action='store_true', default=False)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    vid = [8, 10],\n",
    "    curr_vid = 9,\n",
    "    mount_dir = '/data/mint/dataset_generation_symlink/',\n",
    "    dataset_dir = '/data/mint/dataset_generation/',\n",
    "    gen_missing_sj = False,\n",
    ")\n",
    "\n",
    "# Generate dataset path\n",
    "name = 'random_target'\n",
    "model_name = \"/log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250/\"\n",
    "misc = \"/ema_085000/train/render_face/reverse_sampling\"\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    t_start = time.time()\n",
    "    sj_dict_v = {}\n",
    "    sj_dict_all = {}\n",
    "    count = 0\n",
    "    generate_progress = {'pair': {}}\n",
    "    \n",
    "    \n",
    "    for id in args.vid + [args.curr_vid]:\n",
    "        sj_dict_v[id] = {}\n",
    "        sj_folder = glob.iglob(f'{args.mount_dir}/mount/v{id:02d}/{name}/{model_name}/{misc}/*')\n",
    "        for sj in tqdm.tqdm(sj_folder):\n",
    "            if 'filebrowser' in sj: continue\n",
    "            src_sj_name = sj.split('/')[-1]\n",
    "            target_name = glob.iglob(f'{args.mount_dir}/mount/v{id:02d}/{name}/{model_name}/{misc}/{src_sj_name}/*')\n",
    "            sj_dict_v[id][src_sj_name] = 1\n",
    "            if src_sj_name not in sj_dict_all.keys():\n",
    "                sj_dict_all[src_sj_name] = {'count': 1}\n",
    "            else: \n",
    "                sj_dict_all[src_sj_name]['count'] += 1\n",
    "            for tmp in target_name:\n",
    "                dst_sj_name = tmp.split('/')[-1]\n",
    "                generate_progress['pair'][f'pair{count}'] = {\n",
    "                    \"src\": f\"{src_sj_name.split('=')[-1]}\",\n",
    "                    \"dst\": f\"{dst_sj_name.split('=')[-1]}\",\n",
    "                }\n",
    "                count+=1\n",
    "                \n",
    "        print(f\"[#] V{id:02d} has {len(sj_dict_v[id])} sj...\")\n",
    "        print(f\"[#] from path: {args.mount_dir}/mount/v{id:02d}/{name}/{model_name}/{misc}/*\")\n",
    "            \n",
    "    print(f\"[#] Total time: {time.time() - t_start}\")\n",
    "    print(f\"[#] Total sj: {len(sj_dict_all.keys())}\")\n",
    "    \n",
    "    template_sj = dict.fromkeys([f'src={i}.jpg' for i in range(0, 60000)])\n",
    "    unvisited_sj = set(template_sj.keys()) - set(sj_dict_all.keys())\n",
    "    print(f\"[#] Unvisited sj: {unvisited_sj}\")\n",
    "    print(f\"[#] Unvisited sj: {len(unvisited_sj)}\")\n",
    "    \n",
    "    generate_progress['time'] = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(f'./sampling_progress_sj_generated_dataset.json', 'w') as f:\n",
    "        json.dump(generate_progress, f, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max len :  {'n': 9, 'src_sj': '20586.jpg', 'dst_sj': {'56963.jpg', '4271.jpg', '27192.jpg', '58238.jpg', '38521.jpg', '25799.jpg', '24230.jpg', '36883.jpg', '31756.jpg'}}\n",
      "Min len :  {'n': 1, 'src_sj': '21856.jpg', 'dst_sj': {'26058.jpg'}}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, json\n",
    "import argparse\n",
    "import os, time, tqdm, datetime\n",
    "import subprocess\n",
    "\n",
    "with open('./sampling_progress_sj_generated_dataset.json', 'r') as f:\n",
    "    pairs = json.load(f)['pair']\n",
    "    \n",
    "sj_dict = {}\n",
    "for k, v in pairs.items():\n",
    "    if v['src'] not in sj_dict.keys():\n",
    "        sj_dict[v['src']] = [v['dst']]\n",
    "    else:\n",
    "        sj_dict[v['src']].append(v['dst'])\n",
    "\n",
    "max_len = {\"n\":0, \"src_sj\":None, \"dst_sj\":None}\n",
    "min_len = {\"n\":100, \"src_sj\":None, \"dst_sj\":None}\n",
    "sj_dict_f = {}\n",
    "for k, v in sj_dict.items():\n",
    "    # print(\"Before remove duplicate : \", k, v, len(v))\n",
    "    # print(\"After remove duplicate : \", k, set(v), len(set(v)))\n",
    "    sj_dict_f[k] = list(set(v))\n",
    "    if len(set(v)) > max_len['n']:\n",
    "        max_len['n'] = len(set(v))\n",
    "        max_len['src_sj'] = k\n",
    "        max_len['dst_sj'] = set(v)\n",
    "    if len(set(v)) < min_len['n']:\n",
    "        min_len['n'] = len(set(v))\n",
    "        min_len['src_sj'] = k\n",
    "        min_len['dst_sj'] = set(v)\n",
    "        \n",
    "    # print(\"=\"*50)\n",
    "\n",
    "print(\"Max len : \", max_len)\n",
    "print(\"Min len : \", min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 47\n",
      "100%|██████████| 60000/60000 [24:33<00:00, 40.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# Sampling more sj to match the max_len\n",
    "import copy\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "seed = 47\n",
    "pl.seed_everything(seed)\n",
    "pool = [f'{i}.jpg' for i in range(0, 60000)]\n",
    "n = max_len['n']\n",
    "out = copy.deepcopy(sj_dict_f)\n",
    "output_save = {}\n",
    "for k, v in tqdm.tqdm(sj_dict_f.items()):\n",
    "    if n <= len(sj_dict_f[k]):\n",
    "        out[k] = v[:n]\n",
    "    elif n > len(sj_dict_f[k]):\n",
    "        to_add = n - len(sj_dict_f[k])\n",
    "        available_sj = sorted(set(pool) - set(sj_dict_f[k]))\n",
    "        tmp = np.random.choice(available_sj, to_add, replace=False)\n",
    "        out[k] = v + list(tmp)\n",
    "    else:\n",
    "        print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22516.jpg ['6969.jpg', '19471.jpg', '44981.jpg', '26015.jpg', '35548.jpg', '7349.jpg', '954.jpg', '26867.jpg', '44528.jpg'] ['6969.jpg']\n"
     ]
    }
   ],
   "source": [
    "# Save file of the new sampling\n",
    "import multiprocessing\n",
    "print(k, out[k], sj_dict_f[k])\n",
    "\n",
    "\n",
    "def process_set(data, index):\n",
    "    out_save = {'pair': {}}\n",
    "    count = 0\n",
    "    num_sj = len(data.keys())\n",
    "    for i in range(0, num_sj):\n",
    "        src_sj = f'{i}.jpg'\n",
    "        out_save['pair'][f'pair{count}'] = {\n",
    "            'src': src_sj,\n",
    "            'dst': data[src_sj][index]\n",
    "        }\n",
    "        count += 1\n",
    "    with open(f'./sampling_generated_dataset/generated_dataset_{index+1}_seed={seed}.json', 'w') as f:\n",
    "        json.dump(out_save, f, indent=4)\n",
    "        \n",
    "os.makedirs('./sampling_generated_dataset/', exist_ok=True)\n",
    "with multiprocessing.Pool(processes=24) as pool:\n",
    "        # Map process_chunk function to each chunk\n",
    "        results = pool.starmap(process_set, zip([out]*n, range(0, n)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check progress of sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4767/60000 [00:33<06:58, 132.07it/s]"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob, json\n",
    "import argparse\n",
    "import os, time, tqdm, datetime\n",
    "import subprocess\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--vid', type=int, nargs='+', default=[])\n",
    "parser.add_argument('--local', action='store_true', default=False)\n",
    "parser.add_argument('--mount_dir', required=True)\n",
    "parser.add_argument('--dataset_dir', required=True)\n",
    "parser.add_argument('--curr_vid', type=int, default=9)\n",
    "parser.add_argument('--gen_missing_sj', action='store_true', default=False)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    vid = [8, 10],\n",
    "    curr_vid = 9,\n",
    "    mount_dir = '/data/mint/dataset_generation_symlink/',\n",
    "    dataset_dir = '/data/mint/dataset_generation/',\n",
    "    gen_missing_sj = False,\n",
    ")\n",
    "\n",
    "\n",
    "def check_progress(idx):\n",
    "    with open(f'./sampling_generated_dataset/generated_dataset_{idx}_seed=47.json') as f:\n",
    "        data = json.load(f)['pair']\n",
    "    name = 'random_target'\n",
    "    model_name = \"/log=Masked_Face_woclip+BgNoHead+shadow_256_cfg=Masked_Face_woclip+BgNoHead+shadow_256.yaml_step=250/\"\n",
    "    misc = \"/ema_085000/train/render_face/reverse_sampling\"\n",
    "    postfix = 'Lerp_1000/n_frames=2/'\n",
    "    \n",
    "    found = []\n",
    "    for k, v in tqdm.tqdm(data.items()):\n",
    "        src_sj = v['src']\n",
    "        dst_sj = v['dst']\n",
    "        for id in args.vid + [args.curr_vid]:\n",
    "            c_folder = f'{args.mount_dir}/mount/v{id:02d}/{name}/{model_name}/{misc}/src={src_sj}/dst={dst_sj}'\n",
    "            c_file_recon = f'{c_folder}/{postfix}/res_frame0.png'\n",
    "            c_file_relit = f'{c_folder}/{postfix}/res_frame1.png'\n",
    "            if os.path.exists(c_file_recon) and os.path.exists(c_file_relit) and os.path.exists(c_folder):\n",
    "                found.append(k)\n",
    "                break\n",
    "    return found\n",
    "\n",
    "n_check = 3\n",
    "with multiprocessing.Pool(processes=24) as pool:\n",
    "        # Map process_chunk function to each chunk\n",
    "        results = pool.starmap(check_progress, zip(range(0, n_check)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "difareli_faster_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
