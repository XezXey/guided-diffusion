{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image sample (DDPM - guided diffusion - Diffusion beats gans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function \n",
    "import argparse\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "\n",
    "from guided_diffusion import logger, deca_dpm\n",
    "\n",
    "from guided_diffusion.script_util import (\n",
    "    NUM_CLASSES,\n",
    "    model_and_diffusion_defaults,\n",
    "    create_deca_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    "    seed_all,\n",
    "    diffusion_defaults,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argparser(log_dir, img_model_path, deca_model_path, image_size, in_ch, out_ch, **kwargs):\n",
    "    defaults = dict(\n",
    "        clip_denoised=True,\n",
    "        num_samples=1,\n",
    "        batch_size=15,\n",
    "        use_ddim=False,\n",
    "        img_model_path=img_model_path,\n",
    "        deca_model_path=deca_model_path,\n",
    "        log_dir=log_dir,\n",
    "        diffusion_step=1000,\n",
    "        timestep_respacing=1000,\n",
    "        image_size=image_size,\n",
    "        in_image=\"raw\"\n",
    "    )\n",
    "\n",
    "    defaults.update(model_and_diffusion_defaults(image_size=image_size, in_ch=in_ch, out_ch=out_ch))\n",
    "    return namedtuple('GenericDict', defaults.keys())(**defaults)\n",
    "\n",
    "def model_and_diffusion_defaults(image_size, in_ch, out_ch):\n",
    "    \"\"\"\n",
    "    Defaults for image training.\n",
    "    \"\"\"\n",
    "    res = dict(\n",
    "        image_size=image_size,\n",
    "        num_channels=128,\n",
    "        in_channels=in_ch,\n",
    "        out_channels=out_ch,\n",
    "        num_res_blocks=2,\n",
    "        num_heads=4,\n",
    "        num_heads_upsample=-1,\n",
    "        num_head_channels=-1,\n",
    "        attention_resolutions=\"16,8\",\n",
    "        channel_mult=\"\",\n",
    "        dropout=0.0,\n",
    "        class_cond=False,\n",
    "        use_checkpoint=False,\n",
    "        use_scale_shift_norm=True,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        z_cond=False,\n",
    "    )\n",
    "    res.update(diffusion_defaults())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['flower_1k_hsv_100k', 'flower_1k_rbg', 'flower_1k_hls_100k', 'DECA_128_raw', 'flower_1k_hsv', 'flower_1k_rbg_100k', 'flower_1k_hls', 'flower_1k_ycrcb_100k', 'flower_1k_ycrcb']\n",
      "creating raw model and diffusion...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DECADense(\n",
       "  (activation): LeakyReLU(negative_slope=0.01)\n",
       "  (emb_layers): Sequential(\n",
       "    (0): SiLU()\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (input_mlp): ModuleList(\n",
       "    (0): Linear(in_features=159, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (mid_mlp): Sequential(\n",
       "    (0): Linear(in_features=33280, out_features=512, bias=True)\n",
       "  )\n",
       "  (output_mlp): ModuleList(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): Linear(in_features=512, out_features=159, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List model_logs\n",
    "ct = \"v7\"\n",
    "sshfs_path = \"/home/mint/mnt_tl_puntawat-ms-7c37\"\n",
    "# sshfs_path = \"\"\n",
    "model_logs_path = f\"{sshfs_path}/home/puntawat/Mint/Work/Vision/Diffusion/mount/{ct}/\"\n",
    "print(os.listdir(model_logs_path))\n",
    "\n",
    "# args\n",
    "log_dir = \"DECA_128_raw\"\n",
    "\n",
    "step = \"140000\"\n",
    "ckpt = f\"model{step}\"\n",
    "# ckpt = f\"ema_0.9999_{step}\"\n",
    "# model_logs_path = \"/home2/mint/model_logs_mount/v8_model_logs/\"\n",
    "img_model_path = f\"{model_logs_path}/{log_dir}/img_{ckpt}.pt\"\n",
    "deca_model_path = f\"{model_logs_path}/{log_dir}/DECA_{ckpt}.pt\"\n",
    "\n",
    "image_size=128\n",
    "in_ch = 3\n",
    "out_ch = 3\n",
    "args = create_argparser(log_dir=log_dir, img_model_path=img_model_path, deca_model_path=deca_model_path, image_size=image_size, in_ch=in_ch, out_ch=out_ch)\n",
    "\n",
    "# Check model_logs\n",
    "if not os.path.isdir(os.path.join(model_logs_path, args.log_dir)):\n",
    "    print(\"No logs folder\")\n",
    "    raise FileNotFoundError\n",
    "else: \n",
    "    if not os.path.isdir(os.path.join(model_logs_path, args.log_dir, \"samples\")):\n",
    "        os.makedirs(os.path.join(model_logs_path, args.log_dir, \"samples\"))\n",
    "\n",
    "\n",
    "# dist_util.setup_dist()\n",
    "# logger.configure()\n",
    "\n",
    "if args.in_image in ['raw', 'raw+uvdn']:\n",
    "    model_and_diffusion = model_and_diffusion_defaults(image_size=image_size, in_ch=in_ch, out_ch=out_ch)\n",
    "    print(\"creating {} model and diffusion...\".format(args.in_image))\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "# for k, v in model_and_diffusion.items():\n",
    "    # print(k, v)\n",
    "\n",
    "img_model, deca_model, diffusion = create_deca_and_diffusion(\n",
    "    **args_to_dict(args, model_and_diffusion.keys())\n",
    ")\n",
    "\n",
    "\n",
    "img_model.load_state_dict(\n",
    "    th.load(args.img_model_path, map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "deca_model.load_state_dict(\n",
    "    th.load(args.deca_model_path, map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "img_model.to('cuda')\n",
    "deca_model.to('cuda')\n",
    "img_model.eval()\n",
    "deca_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef7305bdfb840c2b9740a80968dcb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "class PLInference(pl.LightningModule):\n",
    "    def __init__(self, img_model, deca_model, sample_fn):\n",
    "        super(PLInference, self).__init__()\n",
    "        self.img_model=img_model\n",
    "        self.deca_model=deca_model\n",
    "        self.sample_fn = sample_fn\n",
    "        self.deca_dpm = deca_dpm.Diffusion_DECA(img_model=self.img_model, deca_model=self.deca_model, diffusion=diffusion, progress=True)\n",
    "\n",
    "    def forward(self):\n",
    "        seed_all(33)\n",
    "        if self.sample_fn == 'p_sample_loop':\n",
    "            img_output, deca_output = self.deca_dpm.p_sample_loop(\n",
    "                                        shape_dict={'img':(args.batch_size, in_ch, args.image_size, args.image_size),\n",
    "                                                    'deca':(args.batch_size, 159)})\n",
    "\n",
    "        return {\"img_output\":img_output, \"deca_output\":deca_output}\n",
    "\n",
    "pl_inference = PLInference(img_model=img_model, deca_model=deca_model, sample_fn='p_sample_loop')\n",
    "sample = pl_inference()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decolor(s, out_c='rgb'):\n",
    "    if out_c in ['rgb', 'rbg', 'brg', 'bgr', 'grb', 'gbr']:\n",
    "        s_ = ((s + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    elif out_c == 'luv':\n",
    "        s_ = ((s + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    elif out_c == 'ycrcb':\n",
    "        s_ = ((s + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    elif out_c in ['hsv', 'hls']:\n",
    "        h = (s[..., [0]] + 1) * 90.0 \n",
    "        l_s = (s[..., [1]] + 1) * 127.5\n",
    "        v = (s[..., [2]] + 1) * 127.5\n",
    "        s_ = th.cat((h, l_s, v), axis=2).clamp(0, 255).to(th.uint8)\n",
    "    elif out_c == 'sepia':\n",
    "        s_ = ((s + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "\n",
    "    else: raise NotImplementedError\n",
    "\n",
    "    return s_\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 6\n",
    "rows = 10\n",
    "fig = plt.figure(figsize=(20, 20), dpi=100)\n",
    "sample_ = sample['img_output'].permute(0, 2, 3, 1) # BxHxWxC\n",
    "pt = 0\n",
    "for i in range(0, sample_.shape[0]):\n",
    "    s_ = decolor(s=sample_[i][..., :3], out_c='rgb')\n",
    "    s_ = s_.detach().cpu().numpy()\n",
    "    fig.add_subplot(rows, columns, pt+1)\n",
    "    plt.imshow(s_)\n",
    "    pt += 1\n",
    "\n",
    "    if sample_[i].shape[-1] != 3:\n",
    "        fig.add_subplot(rows, columns, pt+1)\n",
    "        s_ = decolor(s=sample_[i][..., 3:], out_c='rgb')\n",
    "        s_ = s_.detach().cpu().numpy()\n",
    "        plt.imshow(s_)\n",
    "        pt += 1\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.65, \n",
    "                    top=0.9, \n",
    "                    wspace=0.1, \n",
    "                    hspace=0.2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating the FLAME Decoder\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b637e1b1ff01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mflame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAME\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFLAME\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflame_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deca_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deca_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
     ]
    }
   ],
   "source": [
    "from ast import Expression, arg, parse\n",
    "from pickle import PickleError\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import glob, os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from model_3d.FLAME import FLAME\n",
    "from model_3d.FLAME.config import cfg as flame_cfg\n",
    "from collections import defaultdict\n",
    "from model_3d.FLAME.utils.renderer import SRenderY\n",
    "import model_3d.FLAME.utils.util as util\n",
    "import model_3d.FLAME.utils.detectors as detectors\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import estimate_transform, warp, resize, rescale\n",
    "\n",
    "flame = FLAME.FLAME(flame_cfg.model).cuda()\n",
    "sample['deca_output']\n",
    "\n",
    "for i in range(sample['deca_output'].shape[0]):\n",
    "    deca_params = sample['deca_output'][i]\n",
    "    shape = deca_params[:100]\n",
    "    exp = deca_params[100:150]\n",
    "    pose = deca_params[150:156]\n",
    "    cam = deca_params[156:158]\n",
    "\n",
    "\n",
    "def params_to_model(shape, exp, pose, cam, uvdn=None):\n",
    "    verts, landmarks2d, landmarks3d = flame(shape_params=shape, \n",
    "            expression_params=exp, \n",
    "            pose_params=pose)\n",
    "\n",
    "    renderer = SRenderY(image_size=256, obj_filename=flame_cfg.model.topology_path, uv_size=flame_cfg.model.uv_size).cuda()\n",
    "\n",
    "    ## projection\n",
    "    landmarks2d = util.batch_orth_proj(landmarks2d, cam)[:,:,:2]; landmarks2d[:,:,1:] = -landmarks2d[:,:,1:]#; landmarks2d = landmarks2d*self.image_size/2 + self.image_size/2\n",
    "    landmarks3d = util.batch_orth_proj(landmarks3d, cam); landmarks3d[:,:,1:] = -landmarks3d[:,:,1:] #; landmarks3d = landmarks3d*self.image_size/2 + self.image_size/2\n",
    "    trans_verts = util.batch_orth_proj(verts, cam); trans_verts[:,:,1:] = -trans_verts[:,:,1:]\n",
    "    albedo = th.zeros([1, 3, flame_cfg.model.uv_size, flame_cfg.model.uv_size], device='cuda')+1 \n",
    "\n",
    "    ## rendering\n",
    "    shape_images = renderer.render_shape(verts, trans_verts)\n",
    "\n",
    "    # img_uvdn = th.unsqueeze(th.tensor(params['uv_detail_normals']).permute((2, 0, 1)), dim=0).cuda().to(flame.dtype)\n",
    "\n",
    "    # detail_normal_images = F.grid_sample(img_uvdn, ops['grid'], align_corners=False)*ops['alpha_images']\n",
    "    # shape_detail_images = renderer.render_shape(verts, trans_verts, detail_normal_images=detail_normal_images)\n",
    "\n",
    "    # plt.imshow(np.concatenate((shape_images[0].detach().cpu().numpy().transpose([1, 2, 0]), \n",
    "            # ops['images'][0].detach().cpu().numpy().transpose([1, 2, 0]),\n",
    "            # shape_detail_images[0].detach().cpu().numpy().transpose([1, 2, 0])\n",
    "            # ), axis=1))\n",
    "    # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
