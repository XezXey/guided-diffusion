{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything\n",
    "import torch as th\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "import sys\n",
    "sys.path.insert(0, '../../sample_scripts/')\n",
    "import glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sample_scripts.sample_utils.inference_utils import to_tensor\n",
    "from sample_scripts.sample_utils.vis_utils import plot_image\n",
    "from sample_scripts.sample_utils import (\n",
    "    ckpt_utils, \n",
    "    file_utils,\n",
    "    params_utils,\n",
    ")\n",
    "from guided_diffusion.dataloader.img_deca_datasets import load_data_img_deca\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(imgs, size=17):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(size, size))\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        \n",
    "def get_cfg(self):\n",
    "    from config.base_config import parse_args\n",
    "    cfg_file_path = glob.glob(\"/home/mint/guided-diffusion/config/*/*\", recursive=True)\n",
    "    cfg_file_path = [cfg_path for cfg_path in cfg_file_path if f\"/{self.cfg_name}\" in cfg_path]    # Add /{}/ to achieve a case-sensitive of folder\n",
    "    print(\"[#] Config Path : \", cfg_file_path)\n",
    "    assert len(cfg_file_path) <= 1\n",
    "    assert len(cfg_file_path) > 0\n",
    "    cfg_file = cfg_file_path[0]\n",
    "    cfg = parse_args(ipynb={'mode':True, 'cfg':cfg_file})\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_loader = ckpt_utils.CkptLoader(log_dir=\"UNetCond_Spatial_Concat_Shape\", cfg_name=\"UNetCond_Spatial_Concat_Shape.yaml\")\n",
    "cfg = ckpt_loader.cfg\n",
    "cfg.img_cond_model.in_image = cfg.img_cond_model.in_image + ['faceseg_bg_noface&nohair'] + ['faceseg_eyes']\n",
    "cfg.img_cond_model.prep_image = [None, None, None]\n",
    "cfg.img_model.image_size = 256\n",
    "# Load dataset\n",
    "img_dataset_path = f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/ffhq_256/\"\n",
    "deca_dataset_path = f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/params/\"\n",
    "\n",
    "loader, dataset, _ = load_data_img_deca(\n",
    "    data_dir=img_dataset_path,\n",
    "    deca_dir=deca_dataset_path,\n",
    "    batch_size=int(1e7),\n",
    "    image_size=cfg.img_model.image_size,\n",
    "    deterministic=cfg.train.deterministic,\n",
    "    augment_mode=cfg.img_model.augment_mode,\n",
    "    resize_mode=cfg.img_model.resize_mode,\n",
    "    in_image_UNet=cfg.img_model.in_image,\n",
    "    params_selector=cfg.param_model.params_selector + ['albedo'],\n",
    "    rmv_params=cfg.param_model.rmv_params,\n",
    "    set_='valid',\n",
    "    cfg=cfg,\n",
    ")\n",
    "\n",
    "# _, _, avg_dict = load_data_img_deca(\n",
    "#     data_dir=img_dataset_path,\n",
    "#     deca_dir=deca_dataset_path,\n",
    "#     batch_size=int(1e7),\n",
    "#     image_size=cfg.img_model.image_size,\n",
    "#     deterministic=cfg.train.deterministic,\n",
    "#     augment_mode=cfg.img_model.augment_mode,\n",
    "#     resize_mode=cfg.img_model.resize_mode,\n",
    "#     in_image_UNet=cfg.img_model.in_image,\n",
    "#     params_selector=cfg.param_model.params_selector + ['albedo'],\n",
    "#     rmv_params=cfg.param_model.rmv_params,\n",
    "#     set_='train',\n",
    "#     cfg=cfg,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(params_utils)\n",
    "\n",
    "f_mask = np.load('./FLAME_masks_face-id.pkl', allow_pickle=True, encoding='latin1')\n",
    "v_mask = np.load('./FLAME_masks.pkl', allow_pickle=True, encoding='latin1')\n",
    "mask={\n",
    "    'v_mask':v_mask['face'].tolist(),\n",
    "    'f_mask':f_mask['face'].tolist() \n",
    "}        \n",
    "\n",
    "img_path = file_utils._list_image_files_recursively(f\"{img_dataset_path}/valid/\")\n",
    "# img_idx = file_utils.search_index_from_listpath(list_path=img_path, search=['60002.jpg'])\n",
    "img_idx = file_utils.search_index_from_listpath(list_path=img_path, search=['60065.jpg', '60000.jpg', '60001.jpg', '60002.jpg', '60004.jpg'])\n",
    "dat = th.utils.data.Subset(dataset, indices=img_idx)\n",
    "subset_loader = th.utils.data.DataLoader(dat, batch_size=1000,\n",
    "                                    shuffle=False, num_workers=24)\n",
    "                            \n",
    "dat, model_kwargs = next(iter(subset_loader))\n",
    "rendered_image, orig_visdict = params_utils.render_deca(deca_params=model_kwargs, idx=0, n=20, useTex=True, extractTex=True, deca_mode='', use_detail=True, mask=mask, repeat=False)\n",
    "\n",
    "grid = torchvision.utils.make_grid(orig_visdict['shape_images'].mul(255).add_(0.5).clamp_(0, 255).cpu())\n",
    "show(grid/255.0)\n",
    "grid = torchvision.utils.make_grid(orig_visdict['rendered_images'].mul(255).add_(0.5).clamp_(0, 255).cpu())\n",
    "show(grid/255.0)\n",
    "grid = torchvision.utils.make_grid(orig_visdict['rendered_images_uv_texture'].mul(255).add_(0.5).clamp_(0, 255).cpu())\n",
    "show(grid/255.0)\n",
    "grid = torchvision.utils.make_grid(model_kwargs['raw_image'].cpu())\n",
    "show(grid/255.0)\n",
    "grid = torchvision.utils.make_grid((((model_kwargs['faceseg_bg_noface&nohair_img'] + 1) * 127.5)).float().cpu())\n",
    "show(grid/255.0)\n",
    "grid = torchvision.utils.make_grid((((model_kwargs['faceseg_eyes_img'] + 1) * 127.5)).float().cpu())\n",
    "show(grid/255.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_kwargs.keys())\n",
    "import cv2\n",
    "e = []\n",
    "for i in range(len(model_kwargs['raw_image'])):\n",
    "    edges = cv2.Canny(np.transpose(model_kwargs['raw_image'][i].cpu().numpy().astype(np.uint8), (1, 2, 0)), 150, 200)\n",
    "    e.append(edges)\n",
    "plt.figure(figsize=(20, 20))\n",
    "e = np.concatenate(e, 1)\n",
    "plt.imshow(e, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "e = []\n",
    "for i in range(len(model_kwargs['raw_image'])):\n",
    "    edges = cv2.Canny(np.transpose(model_kwargs['raw_image'][i].cpu().numpy().astype(np.uint8), (1, 2, 0)), 150, 200)\n",
    "    edges = cv2.resize(edges, (128, 128))\n",
    "    e.append(edges)\n",
    "plt.figure(figsize=(20, 20))\n",
    "e = np.concatenate(e, 1)\n",
    "plt.imshow(e, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "e = []\n",
    "for i in range(len(model_kwargs['raw_image'])):\n",
    "    edges = cv2.Canny(np.transpose(model_kwargs['raw_image'][i].cpu().numpy().astype(np.uint8), (1, 2, 0)), 150, 200)\n",
    "    edges = cv2.resize(edges, (64, 64))\n",
    "    e.append(edges)\n",
    "plt.figure(figsize=(20, 20))\n",
    "e = np.concatenate(e, 1)\n",
    "plt.imshow(e, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "sys.path.insert(0, '../../sample_scripts/cond_utils/DECA/')\n",
    "from decalib.deca import DECA\n",
    "from decalib.datasets import datasets \n",
    "from decalib.utils import util\n",
    "from decalib.utils.config import cfg as deca_cfg\n",
    "deca_cfg.model.extract_tex = True\n",
    "deca_cfg.rasterizer_type = 'standard'\n",
    "deca_cfg.model.use_tex = False\n",
    "deca = DECA(config = deca_cfg, device='cuda', mode='only_renderer', mask=mask)\n",
    "reload(params_utils)\n",
    "\n",
    "f_mask = np.load('./FLAME_masks_face-id.pkl', allow_pickle=True, encoding='latin1')\n",
    "v_mask = np.load('./FLAME_masks.pkl', allow_pickle=True, encoding='latin1')\n",
    "mask={\n",
    "    'v_mask':v_mask['face'].tolist(),\n",
    "    'f_mask':f_mask['face'].tolist()\n",
    "}\n",
    "\n",
    "img_idx = file_utils.search_index_from_listpath(list_path=img_path, search=['60004.jpg'])\n",
    "dat = th.utils.data.Subset(dataset, indices=img_idx)\n",
    "subset_loader = th.utils.data.DataLoader(dat, batch_size=1000,\n",
    "                                    shuffle=False, num_workers=24)\n",
    "                            \n",
    "dat, model_kwargs = next(iter(subset_loader))\n",
    "rendered_image, orig_visdict = params_utils.render_deca(deca_params=model_kwargs, idx=0, n=20, useTex=True, extractTex=True, deca_mode='', use_detail=True, mask=None, repeat=False)\n",
    "plt.figure(figsize=(25, 25))\n",
    "plt.imshow(deca.visualize(orig_visdict)[..., ::-1])\n",
    "plt.show()\n",
    "\n",
    "rendered_image, orig_visdict = params_utils.render_deca(deca_params=model_kwargs, idx=0, n=20, useTex=True, extractTex=True, deca_mode='', use_detail=True, mask=mask, repeat=False)\n",
    "plt.figure(figsize=(25, 25))\n",
    "plt.imshow(deca.visualize(orig_visdict)[..., ::-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(params_utils)\n",
    "import tqdm\n",
    "\n",
    "def gen_masked_face3d(batch_size, mask, set_, path):\n",
    "    img_path = file_utils._list_image_files_recursively(f\"{img_dataset_path}/{set_}/\")\n",
    "    avail_img_name = [i.split('/')[-1] for i in img_path]\n",
    "    img_idx = file_utils.search_index_from_listpath(list_path=img_path, search=avail_img_name)\n",
    "    dat = th.utils.data.Subset(dataset, indices=img_idx)\n",
    "    subset_loader = th.utils.data.DataLoader(dat, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=24, drop_last=False)\n",
    "    \n",
    "    sys.path.insert(0, '../../sample_scripts/cond_utils/DECA/')\n",
    "    from decalib.deca import DECA\n",
    "    from decalib.datasets import datasets \n",
    "    from decalib.utils import util\n",
    "    from decalib.utils.config import cfg as deca_cfg\n",
    "    deca_cfg.model.extract_tex = True\n",
    "    deca_cfg.rasterizer_type = 'standard'\n",
    "    deca_cfg.model.use_tex = False\n",
    "    deca = DECA(config = deca_cfg, device='cuda', mode='only_renderer', mask=mask)\n",
    "    \n",
    "    clip_path = f\"{path}_wclip/{set_}\"\n",
    "    woclip_path = f\"{path}_woclip/{set_}\"\n",
    "    os.makedirs(clip_path, exist_ok=True)\n",
    "    os.makedirs(woclip_path, exist_ok=True)\n",
    "    for batch_ndx, sample in tqdm.tqdm(enumerate(subset_loader)):\n",
    "        dat, model_kwargs = sample\n",
    "    \n",
    "        _, orig_visdict = params_utils.render_deca(deca_params=model_kwargs, idx=0, n=batch_size, mask=mask, repeat=False, deca_obj=deca)\n",
    "        # grid = torchvision.utils.make_grid(orig_visdict['shape_images'].mul(255).add_(0.5).clamp_(0, 255).cpu())\n",
    "        # show(grid/255.0)\n",
    "        \n",
    "        rendered_image = orig_visdict['shape_images']\n",
    "        rendered_image = rendered_image.permute((0, 2, 3, 1))   # BxHxWxC\n",
    "        for i in range(rendered_image.shape[0]):\n",
    "            name = model_kwargs['image_name'][i].split('.')[0]\n",
    "            # np.save(file=f\"{woclip_path}/{name}.npy\", arr=rendered_image[i].cpu().numpy())\n",
    "            torchvision.utils.save_image(tensor=rendered_image[i].permute((2, 0, 1)).cpu(), fp=f\"{clip_path}/{name}.png\")\n",
    "            \n",
    "        _, orig_visdict = params_utils.render_deca(deca_params=model_kwargs, idx=0, n=batch_size, mask=None, repeat=False, deca_obj=None)\n",
    "        rendered_image = orig_visdict['shape_images']\n",
    "        rendered_image = rendered_image.permute((0, 2, 3, 1))   # BxHxWxC\n",
    "        for i in range(rendered_image.shape[0]):\n",
    "            name = model_kwargs['image_name'][i].split('.')[0]\n",
    "            torchvision.utils.save_image(tensor=rendered_image[i].permute((2, 0, 1)).cpu(), fp=f\"{clip_path}/{name}_full.png\")\n",
    "        break\n",
    "f_mask = np.load('./FLAME_masks_face-id.pkl', allow_pickle=True, encoding='latin1')\n",
    "v_mask = np.load('./FLAME_masks.pkl', allow_pickle=True, encoding='latin1')\n",
    "\n",
    "print(f_mask.keys())\n",
    "mask={\n",
    "    'v_mask':v_mask['face'].tolist(),\n",
    "    'f_mask':f_mask['face'].tolist() + f_mask['boundary'].tolist() + f_mask['neck'].tolist() + f_mask['scalp'].tolist(),\n",
    "    # 'f_mask':sum([f_mask[k].tolist() for k in f_mask.keys()], [])\n",
    "}\n",
    "gen_masked_face3d(1, mask, set_=\"valid\", path=\"/data/mint/DPM_Dataset/ffhq_256_with_anno/rendered_images/deca_masked_face_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(params_utils)\n",
    "import tqdm\n",
    "\n",
    "def gen_masked_face3d(batch_size, mask, set_, path):\n",
    "    img_path = file_utils._list_image_files_recursively(f\"{img_dataset_path}/{set_}/\")\n",
    "    avail_img_name = [i.split('/')[-1] for i in img_path]\n",
    "    img_idx = file_utils.search_index_from_listpath(list_path=img_path, search=avail_img_name)\n",
    "    dat = th.utils.data.Subset(dataset, indices=img_idx)\n",
    "    subset_loader = th.utils.data.DataLoader(dat, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=24, drop_last=False)\n",
    "    \n",
    "    sys.path.insert(0, '../../sample_scripts/cond_utils/DECA/')\n",
    "    from decalib.deca import DECA\n",
    "    from decalib.datasets import datasets \n",
    "    from decalib.utils import util\n",
    "    from decalib.utils.config import cfg as deca_cfg\n",
    "    deca_cfg.model.extract_tex = True\n",
    "    deca_cfg.rasterizer_type = 'standard'\n",
    "    deca_cfg.model.use_tex = False\n",
    "    deca = DECA(config = deca_cfg, device='cuda', mode='only_renderer', mask=mask)\n",
    "    \n",
    "    clip_path = f\"{path}_wclip/{set_}\"\n",
    "    woclip_path = f\"{path}_woclip/{set_}\"\n",
    "    os.makedirs(clip_path, exist_ok=True)\n",
    "    os.makedirs(woclip_path, exist_ok=True)\n",
    "    for batch_ndx, sample in tqdm.tqdm(enumerate(subset_loader)):\n",
    "        dat, model_kwargs = sample\n",
    "    \n",
    "        _, orig_visdict = params_utils.render_deca(deca_params=model_kwargs, idx=0, n=batch_size, mask=mask, repeat=False, deca_obj=deca)\n",
    "        # grid = torchvision.utils.make_grid(orig_visdict['shape_images'].mul(255).add_(0.5).clamp_(0, 255).cpu())\n",
    "        # show(grid/255.0)\n",
    "        \n",
    "        rendered_image = orig_visdict['shape_images']\n",
    "        rendered_image = rendered_image.permute((0, 2, 3, 1))   # BxHxWxC\n",
    "        for i in range(rendered_image.shape[0]):\n",
    "            name = model_kwargs['image_name'][i].split('.')[0]\n",
    "            # np.save(file=f\"{woclip_path}/{name}.npy\", arr=rendered_image[i].cpu().numpy())\n",
    "            torchvision.utils.save_image(tensor=rendered_image[i].permute((2, 0, 1)).cpu(), fp=f\"{clip_path}/{name}.png\")\n",
    "       \n",
    "f_mask = np.load('./FLAME_masks_face-id.pkl', allow_pickle=True, encoding='latin1')\n",
    "v_mask = np.load('./FLAME_masks.pkl', allow_pickle=True, encoding='latin1')\n",
    "\n",
    "print(f_mask.keys())\n",
    "mask={\n",
    "    'v_mask':v_mask['face'].tolist(),\n",
    "    'f_mask':f_mask['face'].tolist() + f_mask['boundary'].tolist() + f_mask['neck'].tolist() + f_mask['scalp'].tolist(),\n",
    "    # 'f_mask':sum([f_mask[k].tolist() for k in f_mask.keys()], [])\n",
    "}\n",
    "gen_masked_face3d(1, mask, set_=\"valid\", path=\"/data/mint/DPM_Dataset/ffhq_256_with_anno/rendered_images/deca_masked_face_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_kwargs.keys())\n",
    "import cv2\n",
    "edges = cv2.Canny(np.transpose(model_kwargs['raw_image'][0].cpu().numpy().astype(np.uint8), (1, 2, 0)), 190, 200)\n",
    "plt.imshow(edges, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchvision.utils.save_image(orig_visdict['shape_images'][[0]], fp='./tmp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the masking faces indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_3d.FLAME.config import cfg as flame_cfg\n",
    "import model_3d.FLAME.FLAME as FLAME\n",
    "import model_3d.FLAME.utils.renderer as renderer_lib\n",
    "import model_3d.FLAME.utils.util as util\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "reload(renderer_lib)\n",
    "reload(FLAME)\n",
    "\n",
    "flame = FLAME.FLAME(flame_cfg.model).cuda()\n",
    "renderer = renderer_lib.SRenderY(image_size=256, obj_filename=flame_cfg.model.topology_path, uv_size=flame_cfg.model.uv_size).cuda()\n",
    "print(renderer.faces.shape)\n",
    "print(renderer.face_colors.shape)\n",
    "\n",
    "\n",
    "def index_masking_faces(mask, faces):\n",
    "    print(\"[#] Create indexing masked of faces...\")\n",
    "    import tqdm, pickle\n",
    "    rm_faces = {}\n",
    "    for k in mask.keys():\n",
    "        rm_faces[k] = []\n",
    "        print(\"Processing : \", k)\n",
    "        for m in tqdm.tqdm(mask[k].tolist()):\n",
    "            for i, f in enumerate(faces[0]):\n",
    "                if m in f:\n",
    "                    rm_faces[k].append(i)\n",
    "        rm_faces[k] = np.array(rm_faces[k])\n",
    "    \n",
    "    with open('FLAME_masks_face-id.pkl', 'wb') as output:\n",
    "        pickle.dump(rm_faces, output)\n",
    "\n",
    "mask = np.load('./FLAME_masks.pkl', allow_pickle=True, encoding='latin1')\n",
    "count = 0\n",
    "mask_list = []\n",
    "for k, v in mask.items():\n",
    "    count += len(v)\n",
    "    print(f\"{k} - {v.shape}\")\n",
    "    mask_list.append(v)\n",
    "mask_list = np.concatenate(mask_list, -1)\n",
    "\n",
    "print(renderer.faces.shape)\n",
    "print(type(mask['face'][0]))\n",
    "print(\"#\"*50)\n",
    "index_masking_faces(mask, renderer.faces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('3dr_sampling_deca')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26e0653f07e2e56b7224425fe2a01c8e1786a80966b064d6caee4370e24786e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
