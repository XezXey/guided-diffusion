{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image sample (DDPM - guided diffusion - Diffusion beats gans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function \n",
    "import argparse\n",
    "import os, sys, glob\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0, 1, 2, 3\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import pytorch_lightning as pl\n",
    "from collections import namedtuple\n",
    "sys.path.insert(0, '../')\n",
    "from config.base_config import parse_args\n",
    "from guided_diffusion.script_util import (\n",
    "    create_img_and_diffusion,\n",
    "    seed_all,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config file\n",
    "def get_cfg(cfg_name):\n",
    "    cfg_file_path = glob.glob(\"../config/*/*\", recursive=True)\n",
    "    cfg_file_path = [cfg_path for cfg_path in cfg_file_path if f\"/{cfg_name}\" in cfg_path]    # Add /{}/ to achieve a case-sensitive of folder\n",
    "    assert len(cfg_file_path) <= 1\n",
    "    cfg_file = cfg_file_path[0]\n",
    "    cfg = parse_args(ipynb={'mode':True, 'cfg':cfg_file})\n",
    "    return cfg\n",
    "\n",
    "# Log & Checkpoint file \n",
    "def get_model_path(log_dir):\n",
    "    model_logs_path = glob.glob(f\"{sshfs_mount_path}/*/*/\", recursive=True) + glob.glob(f\"{sshfs_path}/*/*/\", recursive=True)\n",
    "    model_path = [m_log for m_log in model_logs_path if f\"/{log_dir}/\" in m_log]    # Add /{}/ to achieve a case-sensitive of folder\n",
    "    assert len(model_path) <= 1\n",
    "    return model_path[0]\n",
    "\n",
    "\n",
    "# Settings \n",
    "\n",
    "# List model_logs\n",
    "sshfs_path = \"/data/mint/model_logs/\"\n",
    "sshfs_mount_path = \"/data/mint/model_logs_mount/\"\n",
    "cfg_name = \"cond_img64_by_deca_arcface.yaml\"\n",
    "log_dir = \"img64_cond_by_deca_arcface\"\n",
    "# cfg_name = \"imgsize_64_condition.yaml\"\n",
    "# log_dir = \"imgsize_64_condition\"\n",
    "m_path = get_model_path(log_dir)\n",
    "cfg = get_cfg(cfg_name)\n",
    "step = \"600000\"\n",
    "set_ = \"valid\"\n",
    "name = cfg.img_model.name\n",
    "ckpt_selector = \"ema\"\n",
    "\n",
    "if ckpt_selector == \"ema\":\n",
    "    ckpt = f\"ema_0.9999_{step}\"\n",
    "elif ckpt_selector == \"model\":\n",
    "    ckpt = f\"model{step}\"\n",
    "else: raise NotImplementedError\n",
    "\n",
    "print(glob.glob(f\"{m_path}/*.pt\"))\n",
    "img_model_path = f\"{m_path}/{name}_{ckpt}.pt\"\n",
    "img_model, diffusion = create_img_and_diffusion(cfg)\n",
    "img_model.load_state_dict(\n",
    "    th.load(img_model_path, map_location=\"cpu\")\n",
    ")\n",
    "img_model.to('cuda')\n",
    "img_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from guided_diffusion.dataloader.img_util import decolor\n",
    "\n",
    "def plot_sample(img, **kwargs):\n",
    "    columns = 6\n",
    "    rows = 15\n",
    "    fig = plt.figure(figsize=(20, 20), dpi=100)\n",
    "    img = img.permute(0, 2, 3, 1) # BxHxWxC\n",
    "    pt = 0\n",
    "    for i in range(0, img.shape[0]):\n",
    "        s_ = decolor(s=img[i], out_c='rgb')\n",
    "        s_ = s_.detach().cpu().numpy()\n",
    "        fig.add_subplot(rows, columns, pt+1)\n",
    "        plt.imshow(s_)\n",
    "        pt += 1\n",
    "\n",
    "        if kwargs is not None:\n",
    "            # Plot other images\n",
    "            for k in kwargs:\n",
    "                fig.add_subplot(rows, columns, pt+1)\n",
    "                s_ = decolor(s=kwargs[k][i].permute(1, 2, 0), out_c='rgb')\n",
    "                s_ = s_.detach().cpu().numpy()\n",
    "                plt.imshow(s_)\n",
    "                pt += 1\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                        bottom=0.1, \n",
    "                        right=0.65, \n",
    "                        top=0.9, \n",
    "                        wspace=0.1, \n",
    "                        hspace=0.2)\n",
    "    plt.show()\n",
    "\n",
    "def plot_deca(sample):\n",
    "\n",
    "    img_ = []\n",
    "    from tqdm.auto import tqdm\n",
    "    for i in tqdm(range(sample['deca_output'].shape[0])):\n",
    "        deca_params = sample['deca_output'][i].clone()\n",
    "        deca_params = denormalize(deca_params, min_val=th.tensor(min_value_train).cuda(), max_val=th.tensor(max_value_train).cuda(), a=-cfg.param_model.bound, b=cfg.param_model.bound).float()\n",
    "        shape = deca_params[None, :100]\n",
    "        pose = deca_params[None, 100:106]\n",
    "        exp = deca_params[None, 106:156]\n",
    "        cam = deca_params[None, 156:]\n",
    "        img = params_to_model(shape=shape, exp=exp, pose=pose, cam=cam, i=i)\n",
    "        img_.append(img[\"shape_images\"])\n",
    "\n",
    "    plot_sample(th.cat(img_, dim=0))\n",
    "    return th.cat(img_, dim=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Expression, arg, parse\n",
    "from pickle import PickleError\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import glob, os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from model_3d.FLAME import FLAME\n",
    "from model_3d.FLAME.config import cfg as flame_cfg\n",
    "from collections import defaultdict\n",
    "from model_3d.FLAME.utils.renderer import SRenderY\n",
    "import model_3d.FLAME.utils.util as util\n",
    "import model_3d.FLAME.utils.detectors as detectors\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import estimate_transform, warp, resize, rescale\n",
    "\n",
    "flame = FLAME.FLAME(flame_cfg.model).cuda()\n",
    "\n",
    "def params_to_model(shape, exp, pose, cam, i, uvdn=None):\n",
    "    verts, landmarks2d, landmarks3d = flame(shape_params=shape, \n",
    "            expression_params=exp, \n",
    "            pose_params=pose)\n",
    "    renderer = SRenderY(image_size=256, obj_filename=flame_cfg.model.topology_path, uv_size=flame_cfg.model.uv_size).cuda()\n",
    "\n",
    "    ## projection\n",
    "    landmarks2d = util.batch_orth_proj(landmarks2d, cam)[:,:,:2]; landmarks2d[:,:,1:] = -landmarks2d[:,:,1:]#; landmarks2d = landmarks2d*self.image_size/2 + self.image_size/2\n",
    "    landmarks3d = util.batch_orth_proj(landmarks3d, cam); landmarks3d[:,:,1:] = -landmarks3d[:,:,1:] #; landmarks3d = landmarks3d*self.image_size/2 + self.image_size/2\n",
    "    trans_verts = util.batch_orth_proj(verts, cam); trans_verts[:,:,1:] = -trans_verts[:,:,1:]\n",
    "\n",
    "    ## rendering\n",
    "    shape_images = renderer.render_shape(verts, trans_verts)\n",
    "\n",
    "    opdict = {'verts' : verts,}\n",
    "\n",
    "    os.makedirs('./rendered_obj', exist_ok=True)\n",
    "    save_obj(renderer=renderer, filename=(f'./rendered_obj/{i}.obj'), opdict=opdict)\n",
    "    \n",
    "    return {\"shape_images\":shape_images, \"landmarks2d\":landmarks2d, \"landmarks3d\":landmarks3d}\n",
    "\n",
    "def save_obj(renderer, filename, opdict):\n",
    "    '''\n",
    "    vertices: [nv, 3], tensor\n",
    "    texture: [3, h, w], tensor\n",
    "    '''\n",
    "    i = 0\n",
    "    vertices = opdict['verts'][i].cpu().numpy()\n",
    "    faces = renderer.faces[0].cpu().numpy()\n",
    "    colors = np.ones(shape=vertices.shape) * 127.5\n",
    "\n",
    "    # save coarse mesh\n",
    "    util.write_obj(filename, vertices, faces, colors=colors)\n",
    "\n",
    "def read_params(path):\n",
    "    params = pd.read_csv(path, header=None, sep=\" \", index_col=False, lineterminator='\\n')\n",
    "    params.rename(columns={0:'img_name'}, inplace=True)\n",
    "    params = params.set_index('img_name').T.to_dict('list')\n",
    "    return params\n",
    "\n",
    "def swap_key(params):\n",
    "    params_s = defaultdict(dict)\n",
    "    for params_name, v in params.items():\n",
    "        for img_name, params_value in v.items():\n",
    "            params_s[img_name][params_name] = np.array(params_value).astype(np.float64)\n",
    "\n",
    "    return params_s\n",
    "\n",
    "def normalize(arr, min_val=None, max_val=None, a=-1, b=1):\n",
    "    '''\n",
    "    Normalize any vars to [a, b]\n",
    "    :param a: new minimum value\n",
    "    :param b: new maximum value\n",
    "    :param arr: np.array shape=(N, #params_dim) e.g. deca's params_dim = 159\n",
    "    ref : https://stats.stackexchange.com/questions/178626/how-to-normalize-data-between-1-and-1\n",
    "    '''\n",
    "    if max_val is None and min_val is None:\n",
    "        max_val = np.max(arr, axis=0)    \n",
    "        min_val = np.min(arr, axis=0)\n",
    "\n",
    "    arr_norm = ((b-a) * (arr - min_val) / (max_val - min_val)) + a\n",
    "    return arr_norm, min_val, max_val\n",
    "\n",
    "def denormalize(arr_norm, min_val, max_val, a=-1, b=1):\n",
    "    arr_denorm = (((arr_norm - a) * (max_val - min_val)) / (b - a)) + min_val\n",
    "    return arr_denorm\n",
    "\n",
    "def load_params(path, params_key):\n",
    "\n",
    "    anno_path = glob.glob(f'{path}/*.txt')\n",
    "    params = {}\n",
    "    for k in params_key:\n",
    "        for p in anno_path:\n",
    "            # Params\n",
    "            if k in p:\n",
    "                print(f'Key=> {k} : Filename=>{p}')\n",
    "                params[k] = read_params(path=p)\n",
    "\n",
    "    params_s = swap_key(params)\n",
    "\n",
    "    all_params = []\n",
    "    for img_name in params_s:\n",
    "        each_img = []\n",
    "        for k in params_key:\n",
    "            each_img.append(params_s[img_name][k])\n",
    "        all_params.append(np.concatenate(each_img))\n",
    "    all_params = np.stack(all_params)\n",
    "\n",
    "    return params_s, all_params\n",
    "\n",
    "# Load params\n",
    "params_key = ['shape', 'pose', 'exp', 'cam', 'light', 'faceemb']\n",
    "params_train, params_train_arr = load_params(path=\"/data/mint/ffhq_256_with_anno/params/train/\", params_key=params_key)\n",
    "_, min_value_train, max_value_train = normalize(a=-cfg.param_model.bound, b=cfg.param_model.bound, arr=params_train_arr.copy())\n",
    "\n",
    "params_valid, params_valid_arr = load_params(path=\"/data/mint/ffhq_256_with_anno/params/valid/\", params_key=params_key)\n",
    "_, min_value_valid, max_value_valid = normalize(a=-cfg.param_model.bound, b=cfg.param_model.bound, arr=params_valid_arr.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image for condition (if needed)\n",
    "import blobfile as bf\n",
    "import random\n",
    "from guided_diffusion.dataloader.img_util import (\n",
    "    random_crop_arr,\n",
    "    center_crop_arr,\n",
    "    resize_arr\n",
    ")\n",
    "\n",
    "def _list_image_files_recursively(data_dir):\n",
    "    results = []\n",
    "    for entry in sorted(bf.listdir(data_dir)):\n",
    "        full_path = bf.join(data_dir, entry)\n",
    "        ext = entry.split(\".\")[-1]\n",
    "        if \".\" in entry and ext.lower() in [\"jpg\", \"jpeg\", \"png\", \"gif\"]:\n",
    "            results.append(full_path)\n",
    "        elif bf.isdir(full_path):\n",
    "            results.extend(_list_image_files_recursively(full_path))\n",
    "    return results\n",
    "\n",
    "def augmentation(pil_image):\n",
    "        # Resize image by resizing/cropping to match the resolution\n",
    "        if cfg.img_model.resize_mode == 'random_crop':\n",
    "            arr = random_crop_arr(pil_image, cfg.img_model.image_size)\n",
    "        elif cfg.img_model.resize_mode == 'center_crop':\n",
    "            arr = center_crop_arr(pil_image, cfg.img_model.image_size)\n",
    "        elif cfg.img_model.resize_mode == 'resize':\n",
    "            arr = resize_arr(pil_image, cfg.img_model.image_size)\n",
    "        else: raise NotImplemented\n",
    "\n",
    "        return arr\n",
    "\n",
    "path = f\"/data/mint/ffhq_256_with_anno/ffhq_256/{set_}\"\n",
    "all_files = _list_image_files_recursively(path)\n",
    "\n",
    "def load_img_condition(n=10, vis=False):\n",
    "    imgs = []\n",
    "    for path in all_files[:n]:\n",
    "        with bf.BlobFile(path, \"rb\") as f:\n",
    "            pil_image = PIL.Image.open(f)\n",
    "            pil_image.load()\n",
    "            pil_image = pil_image.convert(\"RGB\")\n",
    "\n",
    "            raw_img = augmentation(pil_image=pil_image)\n",
    "\n",
    "            raw_img = (raw_img / 127.5) - 1\n",
    "\n",
    "        imgs.append(np.transpose(raw_img, (2, 0, 1)))\n",
    "    imgs = np.stack(imgs)\n",
    "    if vis:\n",
    "        plot_sample(th.tensor(imgs))\n",
    "    return {'image':th.tensor(imgs).cuda()}\n",
    "\n",
    "def load_deca_condition(params, n=10, r_idx=None):\n",
    "    '''\n",
    "    Load deca condition and stack all of thems into 1D-vector\n",
    "    '''\n",
    "    if r_idx is None:\n",
    "        r_idx = list(np.random.choice(a=np.arange(0, len(params)), size=n, replace=False))\n",
    "    img_name = [list(params.keys())[i] for i in r_idx]\n",
    "\n",
    "    all = []\n",
    "\n",
    "    # Choose only param in params_selector\n",
    "    params_selector = cfg.param_model.params_selector\n",
    "    for name in img_name:\n",
    "        each_param = [params[name][p_name] for p_name in params_selector]\n",
    "        all.append(np.concatenate(each_param))\n",
    "\n",
    "    all = np.stack(all, axis=0)        \n",
    "    return {'cond_params':th.tensor(all).cuda(), 'image_name':img_name, 'r_idx':r_idx}\n",
    "\n",
    "class PLInference(pl.LightningModule):\n",
    "    def __init__(self, img_model, sample_fn):\n",
    "        super(PLInference, self).__init__()\n",
    "        self.img_model = img_model\n",
    "        self.sample_fn = sample_fn\n",
    "\n",
    "    def forward(self, model_kwargs, noise):\n",
    "        # seed_all(33)\n",
    "        sample = self.sample_fn(\n",
    "            model=self.img_model,\n",
    "            shape=noise.shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=cfg.diffusion.clip_denoised,\n",
    "            model_kwargs=model_kwargs\n",
    "        )\n",
    "        return {\"img_output\":sample}\n",
    "\n",
    "def cond_params_location():\n",
    "    '''\n",
    "    Return the idx [i, j] for vector[i:j] that the given parameter is located.\n",
    "    e.g. shape is 1-50, etc.\n",
    "\n",
    "    :param p: p in ['shape', 'pose', 'exp', ...]\n",
    "    '''\n",
    "    params_dict = {'shape':100, 'pose':6, 'exp':50, 'cam':3, 'light':27, 'faceemb':512,}\n",
    "    params_selected_loc = {}\n",
    "    params_ptr = 0\n",
    "    for param in cfg.param_model.params_selector:\n",
    "        params_selected_loc[param] = [params_ptr, params_ptr + params_dict[param]]\n",
    "        params_ptr += params_dict[param]\n",
    "    return params_selected_loc\n",
    "    \n",
    "\n",
    "def get_cond_params(batch_size, mode, interchange=None, r_idx=None, base_idx=0, model_kwargs=None):\n",
    "    '''\n",
    "    Return the condition parameters used to condition the network.\n",
    "    :params mode: \n",
    "    '''\n",
    "    assert base_idx < batch_size\n",
    "    if model_kwargs is None:\n",
    "        model_kwargs = {}\n",
    "        model_kwargs.update(load_deca_condition(n=batch_size, params=params_train if set_ == 'train' else params_valid, r_idx=r_idx))\n",
    "    if mode == 'fixed_cond':\n",
    "        cond_params = th.stack([model_kwargs['cond_params'][model_kwargs['r_idx'][base_idx]]]*batch_size, dim=0)\n",
    "        image_name = [model_kwargs['image_name'][base_idx]] * batch_size\n",
    "    elif mode == 'vary_cond':\n",
    "        cond_params = model_kwargs['cond_params']\n",
    "        image_name = model_kwargs['image_name']\n",
    "\n",
    "    else: raise NotImplementedError\n",
    "        \n",
    "    if interchange is not None:\n",
    "        # Fixed the first image\n",
    "        params_selector = cfg.param_model.params_selector\n",
    "        params_selected_loc = cond_params_location()\n",
    "        cond_params_itc = cond_params[[base_idx]].clone().repeat(batch_size, 1)\n",
    "        for itc in interchange:\n",
    "            assert itc in params_selector\n",
    "            i, j = params_selected_loc[itc]\n",
    "            cond_params_itc[1:, i:j] = cond_params[1:, i:j]\n",
    "\n",
    "        cond_params = cond_params_itc\n",
    "\n",
    "    return {'cond_params':cond_params, 'image_name':image_name}\n",
    "\n",
    "def get_init_noise(batch_size, mode, img_size):\n",
    "    '''\n",
    "    Return the init_noise used as input.\n",
    "    :params mode: mode for sampling noise => 'vary_noise', 'fixed_noise'\n",
    "    '''\n",
    "    if mode == 'vary_noise':\n",
    "        init_noise = th.randn((batch_size, 3, img_size, img_size)).cuda()\n",
    "    elif mode == 'fixed_noise':\n",
    "        init_noise = th.cat([th.randn((1, 3, img_size, img_size)).cuda()] * batch_size, dim=0)\n",
    "    else: raise NotImplementedError\n",
    "\n",
    "    return init_noise\n",
    "\n",
    "def prep_model_input(batch_size, mode, interchange, r_idx=None, base_idx=0):\n",
    "    '''\n",
    "    Prepare model input e.g. noise, condition, etc.\n",
    "    :params batch_size:  \n",
    "    :params mode: Dict of fixed/vary noise and condition mode\n",
    "    :params interchange: List of interchange parameters between images by fixed the first image as a base parameters \n",
    "        e.g. ['pose', 'exp'] => use cond[0] as base condition and change only 'pose' and 'exp' following other images in batch\n",
    "    :params r_idx:\n",
    "    :params base_idx: \n",
    "    '''\n",
    "\n",
    "    img_size = cfg.img_model.image_size\n",
    "    init_noise = get_init_noise(batch_size=batch_size, mode=mode['init_noise'], img_size=img_size)\n",
    "    model_kwargs = get_cond_params(batch_size=batch_size, mode=mode['cond_params'], interchange=interchange, r_idx=r_idx, base_idx=base_idx)\n",
    "    return init_noise, model_kwargs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "base_idx = 4\n",
    "mode = {'init_noise':'fixed_noise', 'cond_params':'vary_cond'}\n",
    "interchange=['light']\n",
    "seed_all(47)\n",
    "init_noise, model_kwargs = prep_model_input(batch_size=batch_size, mode=mode, interchange=interchange, base_idx=base_idx)\n",
    "\n",
    "pl_inference = PLInference(img_model=img_model, sample_fn=diffusion.ddim_sample_loop)\n",
    "sample_ddim = pl_inference(noise=init_noise, model_kwargs=model_kwargs)\n",
    "plot_sample(img=sample_ddim['img_output'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "settings = {'mode':[{'init_noise':'fixed_noise', 'cond_params':'vary_cond'}], 'seed':[47], 'interchange':[['faceemb'], ['pose'], ['shape', 'pose'], ['exp', 'shape'], ['exp', 'pose'], ['shape', 'exp', 'pose'], ['pose', 'faceemb'], ['shape', 'pose', 'faceemb'], ['exp', 'shape', 'faceemb'], ['exp', 'pose', 'faceemb'], ['shape', 'exp', 'pose', 'faceemb'], ['shape', 'faceemb'], ['exp', 'faceemb']]}\n",
    "\n",
    "for m in settings['mode']:\n",
    "    for s in settings['seed']:\n",
    "        for itc in settings['interchange']:\n",
    "            seed_all(s)\n",
    "            base_idx = 4\n",
    "            interchange=itc\n",
    "            init_noise, model_kwargs = prep_model_input(batch_size=batch_size, mode=m, interchange=itc, base_idx=base_idx)\n",
    "            pl_inference = PLInference(img_model=img_model, sample_fn=diffusion.ddim_sample_loop)#p_sample_loop)\n",
    "            sample_ddim = pl_inference(noise=init_noise, model_kwargs=model_kwargs)\n",
    "            print(f\"{m}-{s}-{itc}\")\n",
    "            plot_sample(img=sample_ddim['img_output'])\n",
    "            plot_sample(img=src_img, render_img=render_img, sampling_img=sample_ddim['img_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Conditioned for each of sampling image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dataset_path = f'/data/mint/ffhq_256_with_anno/ffhq_256/{set_}/'\n",
    "if cfg.img_model.conditioning:\n",
    "    print(\"Use conditioning\")\n",
    "    img_name_list = model_kwargs['image_name']\n",
    "    # img_name_list = [model_kwargs['image_name'][7]] *batch_size\n",
    "else:\n",
    "    img_name_list = [str(n) + '.jpg' for n in list(np.random.randint(0, 60000, 10))]\n",
    "\n",
    "if set_ == 'valid':\n",
    "    params = params_valid\n",
    "elif set_ == 'train':\n",
    "    params = params_train\n",
    "else: raise NotImplementedError\n",
    "\n",
    "render_img_list = []\n",
    "src_img_list = []\n",
    "img_list = []\n",
    "for img_name in img_name_list:\n",
    "    shape = th.tensor(params[img_name]['shape'][None, :]).float().cuda()\n",
    "    pose = th.tensor(params[img_name]['pose'][None, :]).float().cuda()\n",
    "    exp = th.tensor(params[img_name]['exp'][None, :]).float().cuda()\n",
    "    cam = th.tensor(params[img_name]['cam'][None, :]).float().cuda()\n",
    "\n",
    "    src_img = PIL.Image.open(img_dataset_path + img_name).resize((cfg.img_model.image_size, cfg.img_model.image_size))\n",
    "    src_img = (th.tensor(np.transpose(src_img, (2, 0, 1)))[None, :] / 127.5) - 1\n",
    "    src_img_list.append(src_img)\n",
    "\n",
    "    render_img = params_to_model(shape=shape, exp=exp, pose=pose, cam=cam, i=img_name)\n",
    "    render_img_list.append(render_img[\"shape_images\"])\n",
    "\n",
    "src_img = th.cat(src_img_list, dim=0)\n",
    "render_img = th.cat(render_img_list, dim=0)\n",
    "plot_sample(img=src_img, render_img=render_img, sampling_img=sample_ddim['img_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-the-wild Image\n",
    "import torch as th\n",
    "import sys, os\n",
    "\n",
    "sys.path.insert(0, './cond_utils/arcface/')\n",
    "sys.path.insert(0, './cond_utils/deca/')\n",
    "from cond_utils.arcface import get_arcface_emb\n",
    "from cond_utils.deca import get_deca_emb\n",
    "itw_path = \"./itw_images/\"\n",
    "device = 'cuda:3'\n",
    "\n",
    "# ArcFace\n",
    "faceemb_itw, emb = get_arcface_emb.get_arcface_emb(img_path=itw_path, device=device)\n",
    "\n",
    "# DECA\n",
    "params_dict = {'shape':100, 'pose':6, 'exp':50, 'cam':3, 'light':27, 'faceemb':512,}\n",
    "deca_itw = get_deca_emb.get_deca_emb(img_path=itw_path, device=device)\n",
    "\n",
    "assert deca_itw.keys() == faceemb_itw.keys()\n",
    "params_itw = {}\n",
    "for img_name in deca_itw.keys():\n",
    "    params_itw[img_name] = deca_itw[img_name]\n",
    "    params_itw[img_name].update(faceemb_itw[img_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_kwargs = {}\n",
    "itw_kwargs = load_deca_condition(params=params_itw, n=len(params_itw.keys()), r_idx=[12, 5, 14, 0, 10, 13, 1, 17, 4, 20, 11, 15, 6, 9, 16, 3, 19, 7, 2, 18, 8])\n",
    "src_img_list = []\n",
    "for img_name in itw_kwargs['image_name']:\n",
    "    src_img = PIL.Image.open(itw_path + img_name).resize((cfg.img_model.image_size, cfg.img_model.image_size))\n",
    "    src_img = (th.tensor(np.transpose(src_img, (2, 0, 1)))[None, :] / 127.5) - 1\n",
    "    src_img_list.append(src_img)\n",
    "\n",
    "src_img = th.cat(src_img_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_all(47)\n",
    "\n",
    "pl_inference = PLInference(img_model=img_model, sample_fn=diffusion.ddim_sample_loop)#p_sample_loop)\n",
    "img_size = cfg.img_model.image_size\n",
    "mode = {'init_noise':'fixed_noise', 'cond_params':'vary_cond'}\n",
    "itw_noise = get_init_noise(batch_size=len(params_itw.keys()), mode=mode['init_noise'], img_size=img_size)\n",
    "sample_ddim = pl_inference(noise=itw_noise, model_kwargs=itw_kwargs)\n",
    "plot_sample(img=src_img, sampling_img=sample_ddim['img_output'])\n",
    "\n",
    "pl_inference = PLInference(img_model=img_model, sample_fn=diffusion.p_sample_loop)\n",
    "img_size = cfg.img_model.image_size\n",
    "mode = {'init_noise':'fixed_noise', 'cond_params':'vary_cond'}\n",
    "itw_noise = get_init_noise(batch_size=len(params_itw.keys()), mode=mode['init_noise'], img_size=img_size)\n",
    "sample_p = pl_inference(noise=itw_noise, model_kwargs=itw_kwargs)\n",
    "plot_sample(img=src_img, sampling_img=sample_p['img_output'],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDIM Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_arr(pil_image, image_size):\n",
    "    img = pil_image.resize((image_size, image_size), PIL.Image.ANTIALIAS)\n",
    "    return np.array(img)\n",
    "\n",
    "def prep_images(path):\n",
    "    with bf.BlobFile(path, \"rb\") as f:\n",
    "        pil_image = PIL.Image.open(f)\n",
    "        pil_image.load()\n",
    "    pil_image = pil_image.convert(\"RGB\")\n",
    "\n",
    "    raw_img = resize_arr(pil_image=pil_image, image_size=cfg.img_model.image_size)\n",
    "    raw_img = (raw_img / 127.5) - 1\n",
    "\n",
    "    return np.transpose(raw_img, [2, 0, 1])\n",
    "\n",
    "class PLReverseSampling(pl.LightningModule):\n",
    "    def __init__(self, img_model, sample_fn, device):\n",
    "        super(PLReverseSampling, self).__init__()\n",
    "        self.sample_fn = sample_fn\n",
    "        self.img_model = img_model.to(device)\n",
    "\n",
    "    def forward(self, x, model_kwargs, progress=True):\n",
    "        # Mimic the ddim_sample_loop or p_sample_loop\n",
    "        # seed_all(33)\n",
    "\n",
    "        if self.sample_fn == diffusion.ddim_reverse_sample_loop:\n",
    "            sample = self.sample_fn(\n",
    "                model=self.img_model,\n",
    "                x=x,\n",
    "                clip_denoised=True,\n",
    "                model_kwargs=model_kwargs,\n",
    "                progress=progress\n",
    "            )\n",
    "        elif self.sample_fn == diffusion.q_sample:\n",
    "            sample = self.sample_fn(\n",
    "                x_start=x,\n",
    "                t = 999\n",
    "            )\n",
    "        elif self.sample_fn == diffusion.ddim_reverse_sample:\n",
    "            sample = self.sample_fn(\n",
    "                model=self.img_model,\n",
    "                x=x,\n",
    "                t = th.tensor(np.stack([999] * x.shape[0], axis=0)).cuda(),\n",
    "                model_kwargs=model_kwargs\n",
    "            )\n",
    "        else: raise NotImplementedError\n",
    "\n",
    "        return {\"img_output\":sample}\n",
    "\n",
    "class PLSampling(pl.LightningModule):\n",
    "    def __init__(self, img_model, sample_fn):\n",
    "        super(PLSampling, self).__init__()\n",
    "        self.img_model = img_model\n",
    "        self.sample_fn = sample_fn\n",
    "\n",
    "    def forward(self, model_kwargs, noise):\n",
    "        # seed_all(33)\n",
    "        sample = self.sample_fn(\n",
    "            model=self.img_model,\n",
    "            shape=noise.shape,\n",
    "            noise=noise,\n",
    "            clip_denoised=cfg.diffusion.clip_denoised,\n",
    "            model_kwargs=model_kwargs\n",
    "        )\n",
    "        return {\"img_output\":sample}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_path = \"./itw_images/\"\n",
    "device = 'cuda:0'\n",
    "base_idx = 10\n",
    "all_itw_files = _list_image_files_recursively(itw_path)\n",
    "r_idx = range(len(all_itw_files))\n",
    "itw_images = th.tensor(np.stack([prep_images(all_itw_files[i]) for i in range(len(all_itw_files))], axis=0)).to(device)\n",
    "itw_kwargs = {}\n",
    "itw_kwargs = load_deca_condition(params=params_itw, n=len(params_itw.keys()), r_idx=r_idx)\n",
    "\n",
    "itw_kwargs['cond_params'] = itw_kwargs['cond_params'].to(device)\n",
    "pl_reverse_sampling = PLReverseSampling(img_model=img_model, sample_fn=diffusion.ddim_reverse_sample, device=device)\n",
    "# pl_reverse_sampling = PLReverseSampling(img_model=img_model, sample_fn=diffusion.ddim_reverse_sample_loop, device=device)\n",
    "# pl_reverse_sampling = PLReverseSampling(img_model=img_model, sample_fn=diffusion.q_sample, device=device)\n",
    "img_size = cfg.img_model.image_size\n",
    "reverse_ddim_sample = pl_reverse_sampling(x=itw_images, model_kwargs=itw_kwargs, progress=True)\n",
    "\n",
    "pl_sampling = PLSampling(img_model=img_model, sample_fn=diffusion.ddim_sample_loop)\n",
    "sample_ddim = pl_sampling(noise=reverse_ddim_sample['img_output'], model_kwargs=itw_kwargs)\n",
    "plot_sample(img=itw_images, reverse_sampling_images=reverse_ddim_sample['img_output'], sampling_img=sample_ddim['img_output'])\n",
    "\n",
    "\n",
    "interchange = ['pose']\n",
    "mode = {'init_noise':'fixed_noise', 'cond_params':'vary_cond'}\n",
    "# Interchange the condition\n",
    "itw_kwargs = get_cond_params(batch_size=len(r_idx), mode=mode['cond_params'], interchange=interchange, r_idx=r_idx, base_idx=base_idx, model_kwargs=itw_kwargs)\n",
    "pl_sampling = PLSampling(img_model=img_model, sample_fn=diffusion.ddim_sample_loop)\n",
    "sample_ddim = pl_sampling(noise=th.cat([reverse_ddim_sample['img_output'][[base_idx]]] * len(r_idx), dim=0), model_kwargs=itw_kwargs)\n",
    "plot_sample(img=itw_images, reverse_sampling_images=reverse_ddim_sample['img_output'], sampling_img=sample_ddim['img_output'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load driven video/head poses\n",
    "def _list_video_files_recursively(data_dir):\n",
    "    results = []\n",
    "    for entry in sorted(bf.listdir(data_dir)):\n",
    "        full_path = bf.join(data_dir, entry)\n",
    "        ext = entry.split(\".\")[-1]\n",
    "        if \".\" in entry and ext.lower() in [\"mp4\"]:\n",
    "            results.append(full_path)\n",
    "        elif bf.isdir(full_path):\n",
    "            results.extend(_list_video_files_recursively(full_path))\n",
    "    return results\n",
    "\n",
    "# # In-the-wild Image\n",
    "import torch as th\n",
    "import sys, os\n",
    "\n",
    "import blobfile as bf\n",
    "sys.path.insert(0, './cond_utils/arcface/')\n",
    "sys.path.insert(0, './cond_utils/deca/')\n",
    "from cond_utils.arcface import get_arcface_emb\n",
    "from cond_utils.deca import get_deca_emb\n",
    "import cv2\n",
    "\n",
    "def video2sequence(video_path):\n",
    "    videofolder = video_path.split('.')[0]\n",
    "    os.makedirs(videofolder, exist_ok=True)\n",
    "    video_name = video_path.split('/')[-1].split('.')[0]\n",
    "    vidcap = cv2.VideoCapture(video_path)\n",
    "    success,image = vidcap.read()\n",
    "    count = 0\n",
    "    imagepath_list = []\n",
    "    while success:\n",
    "        imagepath = '{}/{}_frame{:04d}.jpg'.format(videofolder, video_name, count)\n",
    "        cv2.imwrite(imagepath, image)     # save frame as JPEG file\n",
    "        success,image = vidcap.read()\n",
    "        count += 1\n",
    "        imagepath_list.append(imagepath)\n",
    "    print('video frames are stored in {}'.format(videofolder))\n",
    "    return imagepath_list\n",
    "\n",
    "itw_path = \"/home/mint/guided-diffusion/sample_scripts/itw_videos/\"\n",
    "itw_path = _list_video_files_recursively(itw_path)\n",
    "device = 'cuda:3'\n",
    "\n",
    "for vid in itw_path:\n",
    "    _ = video2sequence(vid)\n",
    "\n",
    "video_name = \"-7TMJtnhiPM_0000_S1202_E1607_L345_T26_R857_B538\" \n",
    "img_path = f\"/home/mint/guided-diffusion/sample_scripts/itw_videos/cropped_clips/{video_name}/\"\n",
    "\n",
    "params_dict = {'shape':100, 'pose':6, 'exp':50, 'cam':3, 'light':27, 'faceemb':512,}\n",
    "deca_itw = get_deca_emb.get_deca_emb(img_path=img_path, device='cuda:3', vis=False)\n",
    "\n",
    "# ArcFace\n",
    "faceemb_itw, emb = get_arcface_emb.get_arcface_emb(img_path=img_path, device=device)\n",
    "\n",
    "assert deca_itw.keys() == faceemb_itw.keys()\n",
    "params_itw = {}\n",
    "for img_name in deca_itw.keys():\n",
    "    params_itw[img_name] = deca_itw[img_name]\n",
    "    params_itw[img_name].update(faceemb_itw[img_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itw_kwargs = {}\n",
    "itw_kwargs = load_deca_condition(params=params_itw, n=len(params_itw.keys()), r_idx=range(len(params_itw.keys())))\n",
    "src_img_list = []\n",
    "for img_name in itw_kwargs['image_name']:\n",
    "    src_img = PIL.Image.open(img_path + img_name).resize((cfg.img_model.image_size, cfg.img_model.image_size))\n",
    "    src_img = (th.tensor(np.transpose(src_img, (2, 0, 1)))[None, :] / 127.5) - 1\n",
    "    src_img_list.append(src_img)\n",
    "\n",
    "src_img = th.cat(src_img_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interchange = ['pose']\n",
    "mode = {'init_noise':'fixed_noise', 'cond_params':'vary_cond'}\n",
    "base_idx = 0\n",
    "# Interchange the condition\n",
    "itw_kwargs = get_cond_params(batch_size=len(r_idx), mode=mode['cond_params'], interchange=interchange, r_idx=r_idx, base_idx=base_idx, model_kwargs=itw_kwargs)\n",
    "pl_sampling = PLSampling(img_model=img_model, sample_fn=diffusion.ddim_sample_loop)\n",
    "sample_ddim = pl_sampling(noise=th.cat([reverse_ddim_sample['img_output'][[base_idx]]] * len(r_idx), dim=0), model_kwargs=itw_kwargs)\n",
    "plot_sample(img=itw_images, reverse_sampling_images=reverse_ddim_sample['img_output'], sampling_img=sample_ddim['img_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence2video\n",
    "def sequence2video(img_path, video_name):\n",
    "    fourcc = cv2.VideoWriter_fourcc('F', 'M', 'P', '4')\n",
    "    video = cv2.VideoWriter(\"./gg.avi\", fourcc, 1, (cfg.img_model.image_size, cfg.img_model.image_size)) \n",
    "  \n",
    "    import tqdm\n",
    "    # Appending the images to the video one by one\n",
    "    for image in tqdm.tqdm(img_path): \n",
    "        frame = cv2.imread(image)\n",
    "        frame = cv2.resize(frame, (cfg.img_model.image_size, cfg.img_model.image_size))\n",
    "        video.write(frame)\n",
    "      \n",
    "    video.release()  # releasing the video generated\n",
    "\n",
    "img_path = _list_image_files_recursively(data_dir=f\"./itw_videos/cropped_clips/{video_name}/\")\n",
    "sequence2video(img_path=img_path, video_name=video_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
