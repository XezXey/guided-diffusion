{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "# Dataset\n",
    "parser.add_argument('--set', type=str, default='valid')\n",
    "# Model/Config\n",
    "parser.add_argument('--step', type=str, default='050000')\n",
    "parser.add_argument('--ckpt_selector', type=str, default='ema')\n",
    "parser.add_argument('--cfg_name', type=str, default=None)\n",
    "parser.add_argument('--log_dir', type=str, default=None)\n",
    "# Interpolation\n",
    "parser.add_argument('--interpolate', nargs='+', default=None)\n",
    "parser.add_argument('--interpolate_step', type=int, default=15)\n",
    "parser.add_argument('--interpolate_noise', action='store_true', default=False)\n",
    "parser.add_argument('--lerp', action='store_true', default=False)\n",
    "parser.add_argument('--slerp', action='store_true', default=False)\n",
    "parser.add_argument('--uncond_sampling', action='store_true', default=False)\n",
    "parser.add_argument('--uncond_sampling_iters', type=int, default=1)\n",
    "parser.add_argument('--reverse_sampling', action='store_true', default=False)\n",
    "parser.add_argument('--separate_reverse_sampling', action='store_true', default=False)\n",
    "# Samples selection\n",
    "parser.add_argument('--n_subject', type=int, default=-1)\n",
    "parser.add_argument('--sample_pair_json', type=str, default=None)\n",
    "parser.add_argument('--sample_pair_mode', type=str, default=None)\n",
    "parser.add_argument('--src_dst', nargs='+', default=[])\n",
    "# Pertubation the image condition\n",
    "parser.add_argument('--perturb_img_cond', action='store_true', default=False)\n",
    "parser.add_argument('--perturb_mode', type=str, default='zero')\n",
    "parser.add_argument('--perturb_where', nargs='+', default=[])\n",
    "\n",
    "# Rendering\n",
    "parser.add_argument('--render_mode', type=str, default=\"shape\")\n",
    "parser.add_argument('--rotate_normals', action='store_true', default=False)\n",
    "# Diffusion\n",
    "parser.add_argument('--diffusion_steps', type=int, default=1000)\n",
    "parser.add_argument('--denoised_clamp', type=float, default=None)\n",
    "# Misc.\n",
    "parser.add_argument('--seed', type=int, default=23)\n",
    "parser.add_argument('--gpu_id', type=str, default=\"0\")\n",
    "parser.add_argument('--save_intermediate', action='store_true', default=False)\n",
    "parser.add_argument('--postfix', type=str, default='')\n",
    "parser.add_argument('--ovr_img', type=str, default=None)\n",
    "parser.add_argument('--ovr_mod', action='store_true', default=False)\n",
    "parser.add_argument('--norm_img', action='store_true', default=False)\n",
    "parser.add_argument('--use_global_norm', action='store_true', default=False)\n",
    "parser.add_argument('--norm_space', type=str, default='rgb')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "import os, sys, glob\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import PIL, cv2\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import scipy\n",
    "sys.path.insert(0, '../../../')\n",
    "from guided_diffusion.script_util import (\n",
    "    seed_all,\n",
    ")\n",
    "\n",
    "from guided_diffusion.tensor_util import (\n",
    "    make_deepcopyable,\n",
    "    dict_slice,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(imgs, figsize=(8, 6)):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "from guided_diffusion.dataloader.img_deca_datasets import load_data_img_deca\n",
    "\n",
    "# Sample utils\n",
    "sys.path.insert(0, '../../')\n",
    "from sample_utils import (\n",
    "    ckpt_utils, \n",
    "    params_utils, \n",
    "    vis_utils, \n",
    "    file_utils, \n",
    "    inference_utils, \n",
    "    mani_utils,\n",
    ")\n",
    "device = 'cuda' if th.cuda.is_available() and th._C._cuda_getDeviceCount() > 0 else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] Model Path : \n",
      "#0 : /data/mint/model_logs_mount/v7/Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow/\n",
      "[#] Config Path :  ['/home/mint/guided-diffusion/config/CVPRs/Final_Candidates/Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow.yaml']\n",
      "Merging with :  Namespace(cfg='/home/mint/guided-diffusion/config/CVPRs/Final_Candidates/Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow.yaml')\n",
      "\n",
      "[#] Sampling with diffusion_steps = 1000\n",
      "[#] Available ckpt :  ['_000000.pt', '_000000.pt', '_010000.pt', '_010000.pt', '_020000.pt', '_020000.pt', '_030000.pt', '_030000.pt', '_040000.pt', '_040000.pt', '_050000.pt', '_050000.pt', '_060000.pt', '_060000.pt', '_070000.pt', '_070000.pt', '_080000.pt', '_080000.pt', '_090000.pt', '_090000.pt', '_100000.pt', '_100000.pt', '_110000.pt', '_110000.pt', '_120000.pt', '_120000.pt', '_130000.pt', '_130000.pt', '_140000.pt', '_140000.pt', '_150000.pt', '_150000.pt', '_160000.pt', '_160000.pt', '_170000.pt', '_170000.pt', '_180000.pt', '_180000.pt', '_190000.pt', '_190000.pt', '_200000.pt', '_200000.pt', '_210000.pt', '_210000.pt', '_220000.pt', '_220000.pt', '_230000.pt', '_230000.pt', '_240000.pt', '_240000.pt', '_250000.pt', '_250000.pt', '_260000.pt', '_260000.pt']\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Loading.../data/mint/model_logs_mount/v7/Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow//ImgCond_ema_0.9999_050000.pt\n",
      "[#] Loading.../data/mint/model_logs_mount/v7/Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow//ImgEncoder_ema_0.9999_050000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading deca params...: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] Bounding the input of UNet to +-1.0\n",
      "[#] Parameters Conditioning\n",
      "Params keys order :  ['shape', 'pose', 'exp', 'cam', 'faceemb', 'shadow']\n",
      "Remove keys :  []\n",
      "Input Image :  ['raw']\n",
      "Image condition :  ['deca_masked_face_images_woclip']\n",
      "DPM Image condition :  ['faceseg_nohead']\n",
      "creating the FLAME Decoder\n",
      "[#] DECA : SRenderY applied mask\n"
     ]
    }
   ],
   "source": [
    "seed_all(47)\n",
    "\n",
    "################ SETTINGS ################\n",
    "args.cfg_name = \"Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow.yaml\"\n",
    "args.log_dir = \"Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow\"\n",
    "# args.cfg_name = \"Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow.yaml\"\n",
    "# args.log_dir = \"Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow\"\n",
    "# args.cfg_name = \"Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking.yaml\"\n",
    "# args.log_dir = \"Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking\"\n",
    "# args.cfg_name = \"Face+UNet_Bg_share_dpm_noise_masking+shadow.yaml\"\n",
    "# args.log_dir = \"Face+UNet_Bg_share_dpm_noise_masking+shadow\"\n",
    "\n",
    "# args.cfg_name = \"Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking_shadow_256.yaml\"\n",
    "# args.log_dir = \"Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking_shadow_256_cont\"\n",
    "\n",
    "args.step = '050000'\n",
    "args.ckpt_selector = 'ema'\n",
    "args.set = 'valid'\n",
    "# args.sample_pair_json = './sample_json/itw_samples.json'\n",
    "# args.sample_pair_json = './sample_json/ipynb_samples.json'\n",
    "args.sample_pair_json = './sample_json/debug_multipie-Moke.json'\n",
    "args.sample_pair_mode = 'pair'\n",
    "dataset = 'mp'\n",
    "# dataset = 'itw'\n",
    "\n",
    "# Load Ckpt\n",
    "if args.cfg_name is None:\n",
    "    args.cfg_name = args.log_dir + '.yaml'\n",
    "ckpt_loader = ckpt_utils.CkptLoader(log_dir=args.log_dir, cfg_name=args.cfg_name)\n",
    "cfg = ckpt_loader.cfg\n",
    "\n",
    "print(f\"[#] Sampling with diffusion_steps = {args.diffusion_steps}\")\n",
    "cfg.diffusion.diffusion_steps = args.diffusion_steps\n",
    "model_dict, diffusion = ckpt_loader.load_model(ckpt_selector=args.ckpt_selector, step=args.step)\n",
    "model_dict = inference_utils.eval_mode(model_dict)\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "if dataset == 'itw':\n",
    "    img_dataset_path = f\"/data/mint/DPM_Dataset/ITW/itw_images_aligned/\"\n",
    "    deca_dataset_path = f\"/data/mint/DPM_Dataset/ITW/params/\"\n",
    "    img_ext = '.png'\n",
    "    cfg.dataset.training_data = 'ITW'\n",
    "    cfg.dataset.data_dir = f'{cfg.dataset.root_path}/{cfg.dataset.training_data}/itw_images_aligned/'\n",
    "elif dataset == 'ffhq':\n",
    "    img_dataset_path = f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/ffhq_256/\"\n",
    "    deca_dataset_path = f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/params/\"\n",
    "    img_ext = '.jpg'\n",
    "    cfg.dataset.training_data = 'ffhq_256_with_anno'\n",
    "    cfg.dataset.data_dir = f'{cfg.dataset.root_path}/{cfg.dataset.training_data}/ffhq_256/'\n",
    "elif dataset == 'mp':\n",
    "    img_dataset_path = f\"/data/mint/DPM_Dataset/MultiPIE/mp_aligned/\"\n",
    "    deca_dataset_path = f\"/data/mint/DPM_Dataset/MultiPIE/params/\"\n",
    "    img_ext = '.png'\n",
    "    cfg.dataset.training_data = 'MultiPIE'\n",
    "    cfg.dataset.data_dir = f'{cfg.dataset.root_path}/{cfg.dataset.training_data}/mp_aligned/'\n",
    "else: raise NotImplementedError\n",
    "\n",
    "cfg.dataset.deca_dir = f'{cfg.dataset.root_path}/{cfg.dataset.training_data}/params/'\n",
    "cfg.dataset.face_segment_dir = f\"{cfg.dataset.root_path}/{cfg.dataset.training_data}/face_segment/\"\n",
    "cfg.dataset.deca_rendered_dir = f\"{cfg.dataset.root_path}/{cfg.dataset.training_data}/rendered_images/\"\n",
    "cfg.dataset.laplacian_mask_dir = f\"{cfg.dataset.root_path}/{cfg.dataset.training_data}/eyes_segment/\"\n",
    "cfg.dataset.laplacian_dir = f\"{cfg.dataset.root_path}/{cfg.dataset.training_data}/laplacian/\"\n",
    "\n",
    "loader, dataset, avg_dict = load_data_img_deca(\n",
    "    data_dir=img_dataset_path,\n",
    "    deca_dir=deca_dataset_path,\n",
    "    batch_size=int(1e7),\n",
    "    image_size=cfg.img_model.image_size,\n",
    "    deterministic=cfg.train.deterministic,\n",
    "    augment_mode=cfg.img_model.augment_mode,\n",
    "    resize_mode=cfg.img_model.resize_mode,\n",
    "    in_image_UNet=cfg.img_model.in_image,\n",
    "    params_selector=cfg.param_model.params_selector,\n",
    "    rmv_params=cfg.param_model.rmv_params,\n",
    "    set_=args.set,\n",
    "    cfg=cfg,\n",
    "    mode='sampling',\n",
    "    img_ext=img_ext,\n",
    ")\n",
    "\n",
    "# DECA Rendering\n",
    "if np.any(['deca_masked' in n for n in list(filter(None, dataset.condition_image))]):\n",
    "    mask = params_utils.load_flame_mask()\n",
    "else: \n",
    "    mask=None\n",
    "deca_obj = params_utils.init_deca(mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = dataset.__len__()\n",
    "img_path = file_utils._list_image_files_recursively(f\"{img_dataset_path}/{args.set}\")\n",
    "\n",
    "denoised_fn = None\n",
    "pl_sampling = inference_utils.PLSampling(model_dict=model_dict, \n",
    "                                            diffusion=diffusion, \n",
    "                                            reverse_fn=diffusion.ddim_reverse_sample_loop, \n",
    "                                            forward_fn=diffusion.ddim_sample_loop,\n",
    "                                            denoised_fn=denoised_fn,\n",
    "                                            cfg=cfg,\n",
    "                                            args=args)\n",
    "\n",
    "def rgb_to_gray(img):\n",
    "    out = (img[:, 0:1] * 0.2989) + (img[:, 1:2] * 0.5870) + (img[:, 2:3] * 0.1140)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversion(dat, norm_img, cond, norm_space='gray', sdedit=None):\n",
    "    '''\n",
    "    :param dat: images in B x C x H x W\n",
    "    '''\n",
    "    cond['use_cond_xt_fn'] = False\n",
    "    cond['dpm_cond_img'] = None \n",
    "    \n",
    "    if norm_img:\n",
    "        print(f\"[#] Normalize image in {norm_space}...\")\n",
    "       \n",
    "        gray = rgb_to_gray(dat)\n",
    "        sd, mu = th.std_mean(gray, dim=(1, 2, 3), keepdims=True)\n",
    "        # print(sd, mu, gray.shape, dat.shape)\n",
    "        # print(sd.shape, mu.shape, gray.shape, dat.shape)\n",
    "        mu_d, sd_d = 114.49997340551313, 58.050383371049826\n",
    "        mu_d = (mu_d / 127.5) - 1\n",
    "        sd_d = (sd_d / 127.5)\n",
    "        \n",
    "        img = (dat - mu) / sd\n",
    "        img = (img * sd_d) + mu_d\n",
    "        \n",
    "        #img = dat + 0.2\n",
    "        img = th.clip(img, -1, 1)\n",
    "        #show([(img[0] + 1) * 0.5, (dat[0] + 1) * 0.5])\n",
    "        #return None\n",
    "    else:\n",
    "        img = dat\n",
    "\n",
    "    # Reverse   \n",
    "    reverse_ddim_sample = pl_sampling.reverse_proc(x=img, model_kwargs=cond, store_intermediate=False)\n",
    "    noise_map = reverse_ddim_sample['final_output']['sample']\n",
    "    # Forward   \n",
    "    sample_ddim = pl_sampling.forward_proc(noise=noise_map, model_kwargs=cond, store_intermediate=False, sdedit=sdedit)\n",
    "    \n",
    "    out = sample_ddim['final_output']['sample']\n",
    "    if norm_img:\n",
    "        print(\"[#] DeNormalize image...\")\n",
    "\n",
    "        out = (out - mu_d) / sd_d\n",
    "        out = (out * sd) + mu\n",
    "        \n",
    "    sample_frames = (out + 1) * 0.5\n",
    "    \n",
    "    return sample_frames   \n",
    "\n",
    "def reverse(dat, norm_img, cond, norm_space='gray'):\n",
    "    '''\n",
    "    :param dat: images in 1 x C x H x W\n",
    "    '''\n",
    "    \n",
    "    assert dat.shape[0] == 1\n",
    "    if norm_img:\n",
    "        print(f\"[#] Normalize image in {norm_space}...\")\n",
    "       \n",
    "        gray = rgb_to_gray(dat)\n",
    "        sd, mu = th.std_mean(gray)\n",
    "        mu_d, sd_d = 114.49997340551313, 58.050383371049826\n",
    "        mu_d = (mu_d / 127.5) - 1\n",
    "        sd_d = (sd_d / 127.5)\n",
    "        \n",
    "        img = (dat - mu) / sd\n",
    "        img = (img * sd_d) + mu_d\n",
    "        \n",
    "        #img = dat + 0.2\n",
    "        img = th.clip(img, -1, 1)\n",
    "        #show([(img[0] + 1) * 0.5, (dat[0] + 1) * 0.5])\n",
    "        #return None\n",
    "        nf = {'mu':mu, 'sd':sd, 'mu_d':mu_d, 'sd_d':sd_d}\n",
    "    else:\n",
    "        img = dat\n",
    "        nf = {}\n",
    "\n",
    "    # Reverse   \n",
    "    reverse_ddim_sample = pl_sampling.reverse_proc(x=img, model_kwargs=cond, store_intermediate=False)\n",
    "    noise_map = reverse_ddim_sample['final_output']['sample']\n",
    "        \n",
    "    return noise_map, nf \n",
    "    \n",
    "def forward(noise_map, cond, norm_img, nf):\n",
    "    \n",
    "    # Forward   \n",
    "    sample_ddim = pl_sampling.forward_proc(noise=noise_map, model_kwargs=cond, store_intermediate=False)\n",
    "    \n",
    "    out = sample_ddim['final_output']['sample']\n",
    "    if norm_img:\n",
    "        print(\"[#] DeNormalize image...\")\n",
    "        out = (out - nf['mu_d']) / nf['sd_d']\n",
    "        out = (out * nf['sd']) + nf['mu']\n",
    "        \n",
    "    sample_frames = (out + 1) * 0.5\n",
    "    \n",
    "    return sample_frames   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_condition(cond, src_idx, dst_idx, n_step=2, itp_func=None):\n",
    "    condition_img = list(filter(None, dataset.condition_image))\n",
    "    misc = {'condition_img':condition_img,\n",
    "            'src_idx':src_idx,\n",
    "            'dst_idx':dst_idx,\n",
    "            'n_step':n_step,\n",
    "            'avg_dict':avg_dict,\n",
    "            'dataset':dataset,\n",
    "            'args':args,\n",
    "            'itp_func':itp_func,\n",
    "            'img_size':cfg.img_model.image_size,\n",
    "            'deca_obj':deca_obj,\n",
    "            'cfg':cfg\n",
    "            }  \n",
    "    \n",
    "    if itp_func is not None:\n",
    "        cond['use_render_itp'] = False \n",
    "    else:\n",
    "        cond['use_render_itp'] = True\n",
    "        \n",
    "    # This is for the noise_dpm_cond_img\n",
    "    if cfg.img_model.apply_dpm_cond_img:\n",
    "        cond['image'] = th.stack([cond['image'][src_idx]] * n_step, dim=0)\n",
    "        for k in cfg.img_model.dpm_cond_img:\n",
    "            cond[f'{k}_mask'] = th.stack([cond[f'{k}_mask'][src_idx]] * n_step, dim=0)\n",
    "        \n",
    "    cond, clip_ren = inference_utils.build_condition_image(cond=cond, misc=misc)\n",
    "    cond = inference_utils.prepare_cond_sampling(cond=cond, cfg=cfg, use_render_itp=True)\n",
    "    cond['cfg'] = cfg\n",
    "    if (cfg.img_model.apply_dpm_cond_img) and (np.any(n is not None for n in cfg.img_model.noise_dpm_cond_img)):\n",
    "        cond['use_cond_xt_fn'] = True\n",
    "        for k, p in zip(cfg.img_model.dpm_cond_img, cfg.img_model.noise_dpm_cond_img):\n",
    "            cond[f'{k}_img'] = cond[f'{k}_img'].to(device)\n",
    "            if p is not None:\n",
    "                if 'dpm_noise_masking' in p:\n",
    "                    cond[f'{k}_mask'] = cond[f'{k}_mask'].to(device)\n",
    "                    cond['image'] = cond['image'].to(device)\n",
    "    \n",
    "\n",
    "    if 'render_face' in args.interpolate:\n",
    "        interp_set = args.interpolate.copy()\n",
    "        interp_set.remove('render_face')\n",
    "        \n",
    "    # Interpolate non-spatial\n",
    "    interp_cond = mani_utils.iter_interp_cond(cond, interp_set=interp_set, src_idx=src_idx, dst_idx=dst_idx, n_step=n_step, interp_fn=itp_func)\n",
    "    cond.update(interp_cond)\n",
    "        \n",
    "    # Repeated non-spatial\n",
    "    repeated_cond = mani_utils.repeat_cond_params(cond, base_idx=src_idx, n=n_step, key=mani_utils.without(cfg.param_model.params_selector, args.interpolate + ['light', 'img_latent']))\n",
    "    cond.update(repeated_cond)\n",
    "\n",
    "    # Finalize the cond_params\n",
    "    cond = mani_utils.create_cond_params(cond=cond, key=mani_utils.without(cfg.param_model.params_selector, cfg.param_model.rmv_params))\n",
    "    if cfg.img_cond_model.override_cond != '':\n",
    "        to_tensor_key = ['cond_params'] + cfg.param_model.params_selector + [cfg.img_cond_model.override_cond]\n",
    "    else:    \n",
    "        to_tensor_key = ['cond_params'] + cfg.param_model.params_selector\n",
    "    cond = inference_utils.to_tensor(cond, key=to_tensor_key, device=ckpt_loader.device)\n",
    "    \n",
    "    return cond\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_part(mask, part='faceseg_head'):\n",
    "    mask = np.array(mask)\n",
    "    bg = (mask == 0)\n",
    "    skin = (mask == 1)\n",
    "    l_brow = (mask == 2)\n",
    "    r_brow = (mask == 3)\n",
    "    l_eye = (mask == 4)\n",
    "    r_eye = (mask == 5)\n",
    "    eye_g = (mask == 6)\n",
    "    l_ear = (mask == 7)\n",
    "    r_ear = (mask == 8)\n",
    "    ear_r = (mask == 9)\n",
    "    nose = (mask == 10)\n",
    "    mouth = (mask == 11)\n",
    "    u_lip = (mask == 12)\n",
    "    l_lip = (mask == 13)\n",
    "    neck = (mask == 14)\n",
    "    neck_l = (mask == 15)\n",
    "    cloth = (mask == 16)\n",
    "    hair = (mask == 17)\n",
    "    hat = (mask == 18)\n",
    "    face = np.logical_or.reduce((skin, l_brow, r_brow, l_eye, r_eye, eye_g, l_ear, r_ear, ear_r, nose, mouth, u_lip, l_lip))\n",
    "    if part == 'faceseg_head':\n",
    "        seg_m = (face | neck | hair)\n",
    "    return th.tensor(seg_m)\n",
    "\n",
    "def replace_bg(image_name, dilate=None, rpl=None, vis=False, imgsize=128):\n",
    "    mask = PIL.Image.open(f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/face_segment/valid/anno/anno_{image_name}.png\")\n",
    "    img = PIL.Image.open(f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/ffhq_256/valid/{image_name}.jpg\")\n",
    "\n",
    "    # Load to [0, 1] image\n",
    "    mask = face_part(mask)\n",
    "    org_mask = mask.clone()\n",
    "    if dilate is not None:\n",
    "        from scipy import ndimage\n",
    "        print(\"Apply dilation = \", dilate)\n",
    "        mask = ndimage.binary_dilation(mask.cpu().numpy(), iterations=dilate)\n",
    "        mask = th.tensor(mask)\n",
    "        \n",
    "    img = torchvision.transforms.ToTensor()(img) * 2 - 1\n",
    "    rpl_mode = list(rpl.keys())[0]\n",
    "    rpl_val = list(rpl.values())[0]\n",
    "    \n",
    "    if rpl_mode == 'constant':\n",
    "        print(f\"[#] Replace bg with {rpl_val}\")\n",
    "        rpl_bg = (th.randn_like(img) * 0 + rpl_val['val'])\n",
    "    elif rpl_mode == 'balance_rgb':\n",
    "        m_f, m_bg = mask!=0, mask==0\n",
    "        assert (m_f + m_bg) == 256**2\n",
    "        n_f, n_bg, n_pixel = th.sum(m_f)*3, th.sum(m_bg)*3, (256**2)*3\n",
    "        mu_f =  th.sum(m_f * img) / n_f\n",
    "        val = ((0 * n_pixel) - (n_f * mu_f)) / (n_bg)\n",
    "        val = th.clip(val, -1, 1)\n",
    "        rpl_bg = (th.randn_like(img) * 0 + val)\n",
    "        print(f\"[#] Replace bg with {val}\")\n",
    "    elif rpl_mode == 'mu_sd':\n",
    "        rpl_bg = (th.randn_like(img) * rpl_val['sd'] + rpl_val['mu'])\n",
    "    elif rpl_mode == 'image':\n",
    "        bg_img = PIL.Image.open(f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/ffhq_256/valid/{rpl_val}\")\n",
    "        bg_img = torchvision.transforms.ToTensor()(bg_img) * 2 - 1\n",
    "        rpl_bg = bg_img\n",
    "    elif rpl_mode == 'balance_Ndist':\n",
    "        mu_d = 114.49997340551313 / 127.5 - 1\n",
    "        sd = rpl_val['sd']\n",
    "        m_f, m_bg = mask!=0, mask==0\n",
    "        n_f, n_bg, n_pixel = th.sum(m_f)*3, th.sum(m_bg)*3, (256**2)*3\n",
    "        mu_f =  th.sum(m_f * img) / n_f\n",
    "        val = ((mu_d * n_pixel) - (n_f * mu_f)) / (n_bg)\n",
    "        print(f\"[#] Replace bg with Ndist({val}, {rpl_val['sd']}), clip={rpl_val['clip']}\")\n",
    "        rpl_bg = (th.normal(mean=val, std=sd, size=img.shape))\n",
    "        if rpl_val['clip']:\n",
    "            rpl_bg = th.clip(rpl_bg, -1, 1)\n",
    "    else: raise NotImplementedError\n",
    "    \n",
    "    mod_img = img * (mask != 0) + rpl_bg * (mask == 0)\n",
    "    mod_img = torchvision.transforms.Resize((imgsize, imgsize))(mod_img)\n",
    "    bg_img = img * (mask == 0) + rpl_bg * (mask != 0)\n",
    "    bg_img = torchvision.transforms.Resize((imgsize, imgsize))(bg_img)\n",
    "    mask = torchvision.transforms.Resize((imgsize, imgsize), interpolation=PIL.Image.NEAREST)(mask[None])\n",
    "    org_mask = torchvision.transforms.Resize((imgsize, imgsize), interpolation=PIL.Image.NEAREST)(org_mask[None])\n",
    "    img = torchvision.transforms.Resize((imgsize, imgsize))(img)\n",
    "    \n",
    "    print('Mean : ', th.mean(mod_img))\n",
    "    \n",
    "    if vis:\n",
    "        show(torchvision.utils.make_grid((mod_img + 1) * 0.5), figsize=(4, 8))\n",
    "    return img, mod_img, bg_img, mask, org_mask \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relight(dat, model_kwargs, norm_img, n_step=3, sidx=0, didx=1):\n",
    "    show(torchvision.utils.make_grid((dat + 1) * 0.5), figsize=(4, 8))\n",
    "    # Rendering\n",
    "    args.interpolate = ['render_face']\n",
    "    cond = copy.deepcopy(model_kwargs)\n",
    "    cond = make_condition(cond=cond, \n",
    "                        src_idx=sidx, dst_idx=didx, \n",
    "                        n_step=n_step, itp_func=mani_utils.slerp)\n",
    "\n",
    "    # Reverse \n",
    "    if cfg.img_cond_model.apply:\n",
    "        cond_rev = copy.deepcopy(cond)\n",
    "        cond_rev['cond_img'] = cond_rev['cond_img'][0:1, ...]\n",
    "        cond_rev = pl_sampling.forward_cond_network(model_kwargs=cond_rev)\n",
    "        cond_rev = dict_slice(in_d=cond_rev, keys=cond_rev.keys(), n=1)\n",
    "        if cfg.img_model.conditioning:\n",
    "            cond_rev['cond_params'] = cond_rev['cond_params'][0:1, ...]\n",
    "        \n",
    "    reverse_ddim_sample = pl_sampling.reverse_proc(x=dat[0:1, ...], model_kwargs=cond_rev, store_mean=True)\n",
    "    noise_map = reverse_ddim_sample['final_output']['sample']\n",
    "    rev_mean = reverse_ddim_sample['intermediate']\n",
    "    \n",
    "    #NOTE: rev_mean WILL BE MODIFIED; This is for computing the ratio of inversion (brightness correction).\n",
    "    sample_ddim = pl_sampling.forward_proc(\n",
    "        noise=noise_map,\n",
    "        model_kwargs=cond_rev,\n",
    "        store_intermediate=False,\n",
    "        rev_mean=rev_mean)\n",
    "\n",
    "    # Relight!\n",
    "    cond['use_render_itp'] = True\n",
    "    if cfg.img_cond_model.apply:\n",
    "        cond_relight = pl_sampling.forward_cond_network(model_kwargs=cond)\n",
    "        \n",
    "    assert noise_map.shape[0] == 1\n",
    "    rev_mean_first = [x[:1] for x in rev_mean]\n",
    "    \n",
    "    relight_out = pl_sampling.forward_proc(\n",
    "        noise=th.repeat_interleave(noise_map, repeats=cond_relight[\"cond_img\"].shape[0], dim=0),\n",
    "        model_kwargs=cond_relight,\n",
    "        store_intermediate=False,\n",
    "        add_mean=rev_mean_first)\n",
    "    return relight_out[\"final_output\"][\"sample\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_116092/98294601.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m149\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# for idx in range(len(all_img_idx)):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mimg_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_img_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_img_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mn_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# assert False\n",
    "all_img_idx, all_img_name, args.n_subject = mani_utils.get_samples_list(args.sample_pair_json, \n",
    "                                                                        args.sample_pair_mode, \n",
    "                                                                        args.src_dst, img_path, \n",
    "                                                                        args.n_subject)\n",
    "\n",
    "# for idx in [1, 2, 3, 4]:\n",
    "for idx in [149, 0]:\n",
    "# for idx in range(len(all_img_idx)):\n",
    "    img_idx = all_img_idx[idx]\n",
    "    img_name = all_img_name[idx]\n",
    "    n_step = 3\n",
    "\n",
    "    dat = th.utils.data.Subset(dataset, indices=img_idx)\n",
    "    subset_loader = th.utils.data.DataLoader(dat, batch_size=2,\n",
    "                                        shuffle=False, num_workers=24)\n",
    "                                \n",
    "    # Load image & condition                        \n",
    "    # for i in ['60182.jpg', '60008.jpg', '68782.jpg', -1.0, -0.5, 0.0, 0.5, 1.0]:\n",
    "    dat, model_kwargs = list(iter(subset_loader))[0]\n",
    "    d_tmp = []\n",
    "    # rpl_bg = [None, {'image':model_kwargs['image_name'][0]}, {'balance_Ndist':{'sd':0.1, 'clip':True}}, {'balance_Ndist':{'sd':0.5, 'clip':True}}, {'balance_Ndist':{'sd':58.050383371049826/127.5, 'clip':True}}] \n",
    "    # rpl_bg = [None, {'balance_Ndist':{'sd':0.1, 'clip':True}} , {'balance_Ndist':{'sd':58.050383371049826/127.5, 'clip':True}}]\n",
    "    # rpl_bg = [{'balance_Ndist':{'sd':0.1, 'clip':True}}]\n",
    "    rpl_bg = [None]\n",
    "    all_output = []\n",
    "    for i in rpl_bg:\n",
    "        print(\"Replacing : \", i)\n",
    "        if i is not None:\n",
    "            d, fg, bg, m, om = replace_bg(image_name=model_kwargs['image_name'][0].split('.')[0], dilate=10, rpl=i, imgsize=128)\n",
    "            dat = fg[None][0:1, ...].cuda()\n",
    "            model_kwargs['faceseg_bg_noface&nohair_mask'] = ~m[None]\n",
    "            model_kwargs['original'] = d[None]\n",
    "            model_kwargs['original_mask'] = om[None]\n",
    "        else:\n",
    "            dat = dat[0:1, ...].cuda()\n",
    "\n",
    "        #TODO: Override the model_kwargs argument for 'mask'=m, 'image'=dat, 'bg'=bg\n",
    "        # print(model_kwargs.keys())\n",
    "        model_kwargs['faceseg_bg_noface&nohair_img'] = dat\n",
    "        model_kwargs['image'] = dat\n",
    "        \n",
    "        # show(torchvision.utils.make_grid(th.clip((model_kwargs['faceseg_bg_noface&nohair_img']+1)*0.5, 0, 1)), figsize=(10, 15))\n",
    "        # show(torchvision.utils.make_grid(model_kwargs['faceseg_bg_noface&nohair_mask']*1.0), figsize=(10, 15))\n",
    "        # # ['faceseg_bg_noface&nohair_img'] = m[None]\n",
    "        # print(model_kwargs['faceseg_bg_noface&nohair_img'].shape, model_kwargs['faceseg_bg_noface&nohair_mask'].shape)\n",
    "        # print(bg.shape, fg.shape, m.shape)\n",
    "        # assert False\n",
    "        \n",
    "        \n",
    "        out = relight(dat=dat.clone(), model_kwargs=model_kwargs, n_step=3, norm_img=False)\n",
    "        out = th.cat(((dat+1) * 0.5, (out+1) * 0.5), dim=0)\n",
    "        all_output.append(torchvision.utils.make_grid(out))\n",
    "    all_output = th.clip(th.cat((all_output), dim=1), 0, 1)\n",
    "    show(all_output, figsize=(15, 15))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gaussian_pyramid(img, level=5):\n",
    "  pys = [img]\n",
    "  for i in range(level-1):\n",
    "    img = cv2.GaussianBlur(img, (5, 5), 0)\n",
    "    img = cv2.resize(img, (int(img.shape[1] / 2), int(img.shape[0] / 2)), interpolation=cv2.INTER_LINEAR)\n",
    "    pys.append(img)\n",
    "  return pys\n",
    "\n",
    "\n",
    "def create_laplacian_pyramid(img, level=5):\n",
    "  pys = create_gaussian_pyramid(img, level)\n",
    "  for i in range(level-1):\n",
    "    pys[i] = pys[i] - cv2.resize(pys[i+1], (pys[i].shape[1], pys[i].shape[0]))\n",
    "  return pys\n",
    "\n",
    "def laplacian_blending(imgs, masks, level):\n",
    "  summask = masks[0] + masks[1] + 1e-10\n",
    "  img_lp = [None] * 2\n",
    "  mask_lp = [None] * 2\n",
    "\n",
    "  for i in range(2):\n",
    "    img_lp[i] = create_laplacian_pyramid(imgs[i], level)\n",
    "    mask_lp[i] = create_gaussian_pyramid(masks[i] / summask, level)\n",
    "\n",
    "  output_lp = []\n",
    "  for i in range(len(img_lp[0])):\n",
    "    output_lp.append((img_lp[0][i] * mask_lp[0][i] + img_lp[1][i] * mask_lp[1][i]))\n",
    "\n",
    "  output_lp = output_lp[::-1]\n",
    "  prev_lvl = output_lp[0]\n",
    "  for idx in range(len(output_lp)-1):\n",
    "    prev_lvl = cv2.resize(prev_lvl, dsize=(output_lp[idx+1].shape[1], output_lp[idx+1].shape[0]))\n",
    "    prev_lvl += output_lp[idx+1]\n",
    "\n",
    "  return prev_lvl\n",
    "\n",
    "\n",
    "show(torchvision.utils.make_grid(out), figsize=(10, 10))\n",
    "show(torchvision.utils.make_grid(model_kwargs['faceseg_bg_noface&nohair_mask']*1.0), figsize=(3, 3))\n",
    "show(torchvision.utils.make_grid(model_kwargs['original_mask']*1.0), figsize=(3, 3))\n",
    "show(torchvision.utils.make_grid(out * ~model_kwargs['faceseg_bg_noface&nohair_mask'].cuda()), figsize=(10, 10))\n",
    "show(torchvision.utils.make_grid(((model_kwargs['original']+1)*0.5) * model_kwargs['faceseg_bg_noface&nohair_mask']), figsize=(3, 3))\n",
    "show(torchvision.utils.make_grid((model_kwargs['faceseg_bg_noface&nohair_img']+1)*0.5), figsize=(3, 3))\n",
    "show(torchvision.utils.make_grid((model_kwargs['original']+1)*0.5), figsize=(3, 3))\n",
    "\n",
    "out_blended = []\n",
    "for i in range(out.shape[0]):\n",
    "  img1 = out[i].cpu().numpy().transpose(1, 2, 0) * 255.0\n",
    "  img1 = np.clip(img1, 0, 255)\n",
    "  img2 = ((model_kwargs['original'][0] + 1)*127.5).cpu().numpy().transpose(1, 2, 0)\n",
    "  img2 = np.clip(img2, 0, 255)\n",
    "  #NOTE: Use Dilate-Erosion mask\n",
    "  # mask = scipy.ndimage.binary_erosion(model_kwargs['faceseg_bg_noface&nohair_mask'][0].permute(1, 2, 0), iterations=5)\n",
    "  # mask = scipy.ndimage.binary_dilation(model_kwargs['faceseg_bg_noface&nohair_mask'][0].permute(1, 2, 0).cpu().numpy(), iterations=4)\n",
    "  # mask = th.tensor(mask).permute(2, 0, 1)[None]\n",
    "  #NOTE: Use original mask\n",
    "  mask = ~model_kwargs['original_mask']\n",
    "  mask = th.repeat_interleave(mask, repeats=3, dim=1)[0].cpu().numpy().transpose(1, 2, 0)\n",
    "  ovl_mask = th.logical_or(model_kwargs['original_mask'], model_kwargs['faceseg_bg_noface&nohair_mask'])\n",
    "\n",
    "  plt.imshow(np.concatenate((img1, img2, mask*255, ((~mask*255)*0)+1), axis=1).astype(np.uint8))\n",
    "  plt.show()\n",
    "  # blended_out = laplacian_blending(imgs=[img1, img2], masks=[~mask, (mask*0)+1], level=2)\n",
    "  blended_out = laplacian_blending(imgs=[img1, img2], masks=[~mask, mask], level=3)\n",
    "  # plt.imshow(np.clip(blended_out,0, 255).astype(np.uint8))\n",
    "  # plt.show()\n",
    "  out_blended.append(th.tensor(blended_out).permute(2, 0, 1))\n",
    "out_blended = (th.stack(out_blended, dim=0))\n",
    "\n",
    "show(torchvision.utils.make_grid(out), figsize=(15, 15))\n",
    "show(torchvision.utils.make_grid(th.clip(out_blended/255.0, 0, 1)), figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha Bledning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show(torchvision.utils.make_grid(out), figsize=(10, 10))\n",
    "# show(torchvision.utils.make_grid(model_kwargs['faceseg_bg_noface&nohair_mask']*1.0), figsize=(3, 3))\n",
    "# show(torchvision.utils.make_grid(model_kwargs['original_mask']*1.0), figsize=(3, 3))\n",
    "# show(torchvision.utils.make_grid(out * ~model_kwargs['faceseg_bg_noface&nohair_mask'].cuda()), figsize=(10, 10))\n",
    "\n",
    "# show(torchvision.utils.make_grid(th.logical_or(model_kwargs['original_mask'], model_kwargs['faceseg_bg_noface&nohair_mask'])*1.0), figsize=(3, 3))\n",
    "# show(torchvision.utils.make_grid(~th.logical_or(model_kwargs['original_mask'], model_kwargs['faceseg_bg_noface&nohair_mask'])*1.0), figsize=(3, 3))\n",
    "# show(torchvision.utils.make_grid(((model_kwargs['original']+1)*0.5) * model_kwargs['faceseg_bg_noface&nohair_mask']), figsize=(3, 3))\n",
    "# show(torchvision.utils.make_grid((model_kwargs['faceseg_bg_noface&nohair_img']+1)*0.5), figsize=(3, 3))\n",
    "# show(torchvision.utils.make_grid((model_kwargs['original']+1)*0.5), figsize=(3, 3))\n",
    "\n",
    "def alpha_blending(imgs, masks, ovl_mask, a=0.7):\n",
    "    # Tensor\n",
    "    # print(img1.shape, img2.shape, mask1.shape, mask2.shape, ovl_mask.shape)\n",
    "    img1, img2 = imgs[0], imgs[1]\n",
    "    mask1, mask2 = masks[0], masks[1]\n",
    "    # border = ((a * img1) + ((1-a) * img2)) * ovl_mask\n",
    "    border = ((ovl_mask * img1) + ((1-ovl_mask) * ~(ovl_mask==0) * img2))\n",
    "    out = ((img1 * mask1) + (img2 * mask2)) + border\n",
    "    # out = ((img1 * mask1))# + (img2 * mask2))\n",
    "    return out\n",
    "    \n",
    "\n",
    "out_blended = []\n",
    "for i in range(out.shape[0]):\n",
    "  # print(th.max(out), th.min(out))\n",
    "  # img1 = np.clip(img1, 0, 255)\n",
    "  img1 = (out[i] / 0.5) - 1\n",
    "  img2 = model_kwargs['original']\n",
    "  # img2 = np.clip(img2, 0, 255)\n",
    "  #NOTE: Use Dilate-Erosion mask\n",
    "  # mask = scipy.ndimage.binary_erosion(model_kwargs['faceseg_bg_noface&nohair_mask'][0].permute(1, 2, 0), iterations=5)\n",
    "  # mask = scipy.ndimage.binary_dilation(model_kwargs['faceseg_bg_noface&nohair_mask'][0].permute(1, 2, 0).cpu().numpy(), iterations=4)\n",
    "  # mask = th.tensor(mask).permute(2, 0, 1)[None]\n",
    "  #NOTE: Use original mask\n",
    "  inner_m = model_kwargs['original_mask']\n",
    "  inner_m = th.repeat_interleave(inner_m, repeats=3, dim=1)\n",
    "  \n",
    "  outer_m = model_kwargs['faceseg_bg_noface&nohair_mask']\n",
    "  outer_m = th.repeat_interleave(outer_m, repeats=3, dim=1)\n",
    "  \n",
    "  ovl_mask = ~th.logical_or(model_kwargs['original_mask'], model_kwargs['faceseg_bg_noface&nohair_mask'])\n",
    "  ovl_mask = th.repeat_interleave(ovl_mask, repeats=3, dim=1)\n",
    "  \n",
    "  \n",
    "  ovl_mask = ovl_mask[0, 0:1].permute(1, 2, 0)\n",
    "  ovl_mask = cv2.distanceTransform(src=ovl_mask[..., 0:1].cpu().numpy().astype(np.uint8), distanceType=cv2.DIST_L2, maskSize=3)[..., None]\n",
    "  ovl_mask = cv2.normalize(ovl_mask, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)[..., None]\n",
    "  ovl_mask = th.repeat_interleave(th.tensor(ovl_mask), repeats=3, dim=-1)\n",
    "  ovl_mask = ovl_mask.permute(2, 0, 1)[None] / 255\n",
    "  \n",
    "  if i == 0:\n",
    "    show(torchvision.utils.make_grid(inner_m*1.0), figsize=(5, 5))\n",
    "    show(torchvision.utils.make_grid(outer_m*1.0), figsize=(5, 5))\n",
    "    show(torchvision.utils.make_grid((outer_m+inner_m)*1.0), figsize=(5, 5))\n",
    "    show(torchvision.utils.make_grid(ovl_mask*1.0), figsize=(5, 5))\n",
    "\n",
    "  blended_out = alpha_blending(imgs=[img1.cuda(), img2.cuda()], masks=[inner_m.cuda(), outer_m.cuda()], ovl_mask=ovl_mask.cuda())\n",
    "  out_blended.append(blended_out)\n",
    "  \n",
    "out_blended = (th.cat(out_blended, dim=0))\n",
    "print(out_blended.shape)\n",
    "print(th.max(out_blended), th.min(out_blended))\n",
    "show(torchvision.utils.make_grid(th.clip((out_blended+1)*0.5, 0, 1)), figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(torchvision.utils.make_grid(~ovl_mask*1.0), figsize=(5, 5))\n",
    "x = (~ovl_mask[0].permute(1, 2, 0).cpu().numpy()*255).astype(np.uint8)\n",
    "print(x.shape, x.dtype, np.max(x), np.min(x))\n",
    "xd = cv2.distanceTransform(src=x[..., 0:1], distanceType=cv2.DIST_L2, maskSize=3)[..., None]\n",
    "xd_norm = cv2.normalize(xd, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)[..., None]\n",
    "print(np.max(xd_norm), np.min(xd_norm))\n",
    "\n",
    "xd_norm = th.repeat_interleave(th.tensor(xd_norm), repeats=3, dim=-1)\n",
    "show(torchvision.utils.make_grid(xd_norm.permute(2, 0, 1)[None]), figsize=(5, 5))\n",
    "xd = th.repeat_interleave(th.tensor(xd), repeats=3, dim=-1)\n",
    "show(torchvision.utils.make_grid(xd.permute(2, 0, 1)[None]), figsize=(5, 5))\n",
    "\n",
    "\n",
    "gg = scipy.ndimage.distance_transform_edt(input=(~ovl_mask[0].permute(1, 2, 0).cpu().numpy()*255).astype(np.uint8))\n",
    "\n",
    "show(torchvision.utils.make_grid(th.tensor(gg).permute(2, 0, 1)[None]), figsize=(5, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e726457752a9f31dd08e885b4d7ad782b9d2db89819f0ec82c996add1b497d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
