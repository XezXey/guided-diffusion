{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "# Dataset\n",
    "parser.add_argument('--set', type=str, default='valid')\n",
    "# Model/Config\n",
    "parser.add_argument('--step', type=str, default='050000')\n",
    "parser.add_argument('--ckpt_selector', type=str, default='ema')\n",
    "parser.add_argument('--cfg_name', type=str, default=None)\n",
    "parser.add_argument('--log_dir', type=str, default=None)\n",
    "# Interpolation\n",
    "parser.add_argument('--interpolate', nargs='+', default=None)\n",
    "parser.add_argument('--interpolate_step', type=int, default=15)\n",
    "parser.add_argument('--interpolate_noise', action='store_true', default=False)\n",
    "parser.add_argument('--lerp', action='store_true', default=False)\n",
    "parser.add_argument('--slerp', action='store_true', default=False)\n",
    "parser.add_argument('--uncond_sampling', action='store_true', default=False)\n",
    "parser.add_argument('--uncond_sampling_iters', type=int, default=1)\n",
    "parser.add_argument('--reverse_sampling', action='store_true', default=False)\n",
    "parser.add_argument('--separate_reverse_sampling', action='store_true', default=False)\n",
    "# Samples selection\n",
    "parser.add_argument('--n_subject', type=int, default=-1)\n",
    "parser.add_argument('--sample_pair_json', type=str, default=None)\n",
    "parser.add_argument('--sample_pair_mode', type=str, default=None)\n",
    "parser.add_argument('--src_dst', nargs='+', default=[])\n",
    "# Pertubation the image condition\n",
    "parser.add_argument('--perturb_img_cond', action='store_true', default=False)\n",
    "parser.add_argument('--perturb_mode', type=str, default='zero')\n",
    "parser.add_argument('--perturb_where', nargs='+', default=[])\n",
    "\n",
    "# Rendering\n",
    "parser.add_argument('--render_mode', type=str, default=\"shape\")\n",
    "parser.add_argument('--rotate_normals', action='store_true', default=False)\n",
    "# Diffusion\n",
    "parser.add_argument('--diffusion_steps', type=int, default=1000)\n",
    "parser.add_argument('--denoised_clamp', type=float, default=None)\n",
    "# Misc.\n",
    "parser.add_argument('--seed', type=int, default=23)\n",
    "parser.add_argument('--gpu_id', type=str, default=\"0\")\n",
    "parser.add_argument('--save_intermediate', action='store_true', default=False)\n",
    "parser.add_argument('--postfix', type=str, default='')\n",
    "parser.add_argument('--ovr_img', type=str, default=None)\n",
    "parser.add_argument('--ovr_mod', action='store_true', default=False)\n",
    "parser.add_argument('--norm_img', action='store_true', default=False)\n",
    "parser.add_argument('--use_global_norm', action='store_true', default=False)\n",
    "parser.add_argument('--norm_space', type=str, default='rgb')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "import os, sys, glob\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import PIL, cv2\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "sys.path.insert(0, '../../../')\n",
    "from guided_diffusion.script_util import (\n",
    "    seed_all,\n",
    ")\n",
    "\n",
    "from guided_diffusion.tensor_util import (\n",
    "    make_deepcopyable,\n",
    "    dict_slice,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(imgs, figsize=(8, 6)):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "from guided_diffusion.dataloader.img_deca_datasets import load_data_img_deca\n",
    "\n",
    "# Sample utils\n",
    "sys.path.insert(0, '../../')\n",
    "from sample_utils import (\n",
    "    ckpt_utils, \n",
    "    params_utils, \n",
    "    vis_utils, \n",
    "    file_utils, \n",
    "    inference_utils, \n",
    "    mani_utils,\n",
    ")\n",
    "device = 'cuda' if th.cuda.is_available() and th._C._cuda_getDeviceCount() > 0 else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] Model Path : \n",
      "#0 : /data/mint/model_logs_mount/v13/Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow/\n",
      "[#] Config Path :  ['/home/mint/guided-diffusion/config/CVPRs/ShadowScore/Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow.yaml']\n",
      "Merging with :  Namespace(cfg='/home/mint/guided-diffusion/config/CVPRs/ShadowScore/Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow.yaml')\n",
      "\n",
      "[#] Sampling with diffusion_steps = 1000\n",
      "[#] Available ckpt :  ['_000000.pt', '_000000.pt', '_010000.pt', '_010000.pt', '_020000.pt', '_020000.pt', '_030000.pt', '_030000.pt', '_040000.pt', '_040000.pt', '_050000.pt', '_050000.pt', '_060000.pt', '_060000.pt', '_070000.pt', '_070000.pt', '_080000.pt', '_080000.pt', '_090000.pt', '_090000.pt', '_100000.pt', '_100000.pt', '_110000.pt', '_110000.pt', '_120000.pt', '_120000.pt', '_130000.pt', '_130000.pt', '_140000.pt', '_140000.pt', '_150000.pt', '_150000.pt', '_160000.pt', '_160000.pt', '_170000.pt', '_170000.pt', '_180000.pt', '_180000.pt', '_190000.pt', '_190000.pt', '_200000.pt', '_200000.pt', '_210000.pt', '_210000.pt', '_220000.pt', '_220000.pt', '_230000.pt', '_230000.pt']\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Loading.../data/mint/model_logs_mount/v13/Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow//ImgCond_ema_0.9999_050000.pt\n",
      "[#] Loading.../data/mint/model_logs_mount/v13/Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow//ImgEncoder_ema_0.9999_050000.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading deca params...: 100%|██████████| 10/10 [00:05<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] Bounding the input of UNet to +-1.0\n",
      "[#] Parameters Conditioning\n",
      "Params keys order :  ['shape', 'pose', 'exp', 'cam', 'faceemb', 'shadow']\n",
      "Remove keys :  []\n",
      "Input Image :  ['raw']\n",
      "Image condition :  ['deca_masked_face_images_woclip']\n",
      "DPM Image condition :  ['faceseg_bg_noface&nohair']\n",
      "creating the FLAME Decoder\n",
      "[#] DECA : SRenderY applied mask\n"
     ]
    }
   ],
   "source": [
    "seed_all(47)\n",
    "\n",
    "################ SETTINGS ################\n",
    "# args.cfg_name = \"Masked_Face_woclip_segface_nonspatial.yaml\"\n",
    "# args.log_dir = \"Masked_Face_woclip_segface_nonspatial\"\n",
    "args.cfg_name = \"Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow.yaml\"\n",
    "args.log_dir = \"Masked_Face_woclip+UNet_Bg_share_dpm_noise_masking+shadow\"\n",
    "\n",
    "args.step = '050000'\n",
    "args.ckpt_selector = 'ema'\n",
    "args.set = 'valid'\n",
    "args.sample_pair_json = './sample_json/ipynb_samples.json'\n",
    "args.sample_pair_mode = 'pair'\n",
    "\n",
    "# Load Ckpt\n",
    "if args.cfg_name is None:\n",
    "    args.cfg_name = args.log_dir + '.yaml'\n",
    "ckpt_loader = ckpt_utils.CkptLoader(log_dir=args.log_dir, cfg_name=args.cfg_name)\n",
    "cfg = ckpt_loader.cfg\n",
    "\n",
    "print(f\"[#] Sampling with diffusion_steps = {args.diffusion_steps}\")\n",
    "cfg.diffusion.diffusion_steps = args.diffusion_steps\n",
    "model_dict, diffusion = ckpt_loader.load_model(ckpt_selector=args.ckpt_selector, step=args.step)\n",
    "model_dict = inference_utils.eval_mode(model_dict)\n",
    "\n",
    "# Load dataset\n",
    "if args.set == 'itw':\n",
    "    img_dataset_path = \"../../itw_images/aligned/\"\n",
    "    deca_dataset_path = None\n",
    "elif args.set == 'train' or args.set == 'valid':\n",
    "    img_dataset_path = f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/ffhq_256/\"\n",
    "    deca_dataset_path = f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/params/\"\n",
    "else: raise NotImplementedError\n",
    "\n",
    "loader, dataset, avg_dict = load_data_img_deca(\n",
    "    data_dir=img_dataset_path,\n",
    "    deca_dir=deca_dataset_path,\n",
    "    batch_size=int(1e7),\n",
    "    image_size=cfg.img_model.image_size,\n",
    "    deterministic=cfg.train.deterministic,\n",
    "    augment_mode=cfg.img_model.augment_mode,\n",
    "    resize_mode=cfg.img_model.resize_mode,\n",
    "    in_image_UNet=cfg.img_model.in_image,\n",
    "    params_selector=cfg.param_model.params_selector,\n",
    "    rmv_params=cfg.param_model.rmv_params,\n",
    "    set_=args.set,\n",
    "    cfg=cfg,\n",
    "    mode='sampling'\n",
    ")\n",
    "\n",
    "# DECA Rendering\n",
    "if np.any(['deca_masked' in n for n in list(filter(None, dataset.condition_image))]):\n",
    "    mask = params_utils.load_flame_mask()\n",
    "else: \n",
    "    mask=None\n",
    "deca_obj = params_utils.init_deca(mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = dataset.__len__()\n",
    "img_path = file_utils._list_image_files_recursively(f\"{img_dataset_path}/{args.set}\")\n",
    "\n",
    "\n",
    "denoised_fn = None\n",
    "pl_sampling = inference_utils.PLSampling(model_dict=model_dict, \n",
    "                                            diffusion=diffusion, \n",
    "                                            reverse_fn=diffusion.ddim_reverse_sample_loop, \n",
    "                                            forward_fn=diffusion.ddim_sample_loop,\n",
    "                                            denoised_fn=denoised_fn,\n",
    "                                            cfg=cfg,\n",
    "                                            args=args)\n",
    "\n",
    "def rgb_to_gray(img):\n",
    "    out = (img[:, 0:1] * 0.2989) + (img[:, 1:2] * 0.5870) + (img[:, 2:3] * 0.1140)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversion(dat, norm_img, cond, norm_space='gray', sdedit=None):\n",
    "    '''\n",
    "    :param dat: images in B x C x H x W\n",
    "    '''\n",
    "    cond['use_cond_xt_fn'] = False\n",
    "    cond['dpm_cond_img'] = None \n",
    "    \n",
    "    if norm_img:\n",
    "        print(f\"[#] Normalize image in {norm_space}...\")\n",
    "       \n",
    "        gray = rgb_to_gray(dat)\n",
    "        sd, mu = th.std_mean(gray, dim=(1, 2, 3), keepdims=True)\n",
    "        # print(sd, mu, gray.shape, dat.shape)\n",
    "        # print(sd.shape, mu.shape, gray.shape, dat.shape)\n",
    "        mu_d, sd_d = 114.49997340551313, 58.050383371049826\n",
    "        mu_d = (mu_d / 127.5) - 1\n",
    "        sd_d = (sd_d / 127.5)\n",
    "        \n",
    "        img = (dat - mu) / sd\n",
    "        img = (img * sd_d) + mu_d\n",
    "        \n",
    "        #img = dat + 0.2\n",
    "        img = th.clip(img, -1, 1)\n",
    "        #show([(img[0] + 1) * 0.5, (dat[0] + 1) * 0.5])\n",
    "        #return None\n",
    "    else:\n",
    "        img = dat\n",
    "\n",
    "    # Reverse   \n",
    "    reverse_ddim_sample = pl_sampling.reverse_proc(x=img, model_kwargs=cond, store_intermediate=False)\n",
    "    noise_map = reverse_ddim_sample['final_output']['sample']\n",
    "    # Forward   \n",
    "    sample_ddim = pl_sampling.forward_proc(noise=noise_map, model_kwargs=cond, store_intermediate=False, sdedit=sdedit)\n",
    "    \n",
    "    out = sample_ddim['final_output']['sample']\n",
    "    if norm_img:\n",
    "        print(\"[#] DeNormalize image...\")\n",
    "\n",
    "        out = (out - mu_d) / sd_d\n",
    "        out = (out * sd) + mu\n",
    "        \n",
    "    sample_frames = (out + 1) * 0.5\n",
    "    \n",
    "    return sample_frames   \n",
    "\n",
    "def reverse(dat, norm_img, cond, norm_space='gray'):\n",
    "    '''\n",
    "    :param dat: images in 1 x C x H x W\n",
    "    '''\n",
    "    \n",
    "    assert dat.shape[0] == 1\n",
    "    if norm_img:\n",
    "        print(f\"[#] Normalize image in {norm_space}...\")\n",
    "       \n",
    "        gray = rgb_to_gray(dat)\n",
    "        sd, mu = th.std_mean(gray)\n",
    "        mu_d, sd_d = 114.49997340551313, 58.050383371049826\n",
    "        mu_d = (mu_d / 127.5) - 1\n",
    "        sd_d = (sd_d / 127.5)\n",
    "        \n",
    "        img = (dat - mu) / sd\n",
    "        img = (img * sd_d) + mu_d\n",
    "        \n",
    "        #img = dat + 0.2\n",
    "        img = th.clip(img, -1, 1)\n",
    "        #show([(img[0] + 1) * 0.5, (dat[0] + 1) * 0.5])\n",
    "        #return None\n",
    "        nf = {'mu':mu, 'sd':sd, 'mu_d':mu_d, 'sd_d':sd_d}\n",
    "    else:\n",
    "        img = dat\n",
    "        nf = {}\n",
    "\n",
    "    # Reverse   \n",
    "    reverse_ddim_sample = pl_sampling.reverse_proc(x=img, model_kwargs=cond, store_intermediate=False)\n",
    "    noise_map = reverse_ddim_sample['final_output']['sample']\n",
    "        \n",
    "    return noise_map, nf \n",
    "    \n",
    "def forward(noise_map, cond, norm_img, nf):\n",
    "    \n",
    "    # Forward   \n",
    "    sample_ddim = pl_sampling.forward_proc(noise=noise_map, model_kwargs=cond, store_intermediate=False)\n",
    "    \n",
    "    out = sample_ddim['final_output']['sample']\n",
    "    if norm_img:\n",
    "        print(\"[#] DeNormalize image...\")\n",
    "        out = (out - nf['mu_d']) / nf['sd_d']\n",
    "        out = (out * nf['sd']) + nf['mu']\n",
    "        \n",
    "    sample_frames = (out + 1) * 0.5\n",
    "    \n",
    "    return sample_frames   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manipulate the condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_condition(cond, src_idx, dst_idx, n_step=2, itp_func=None):\n",
    "    condition_img = list(filter(None, dataset.condition_image))\n",
    "    misc = {'condition_img':condition_img,\n",
    "            'src_idx':src_idx,\n",
    "            'dst_idx':dst_idx,\n",
    "            'n_step':n_step,\n",
    "            'avg_dict':avg_dict,\n",
    "            'dataset':dataset,\n",
    "            'args':args,\n",
    "            'itp_func':itp_func,\n",
    "            'img_size':cfg.img_model.image_size,\n",
    "            'deca_obj':deca_obj,\n",
    "            'cfg':cfg\n",
    "            }  \n",
    "    \n",
    "    if itp_func is not None:\n",
    "        cond['use_render_itp'] = False \n",
    "    else:\n",
    "        cond['use_render_itp'] = True\n",
    "        \n",
    "    # This is for the noise_dpm_cond_img\n",
    "    if cfg.img_model.apply_dpm_cond_img:\n",
    "        cond['image'] = th.stack([cond['image'][src_idx]] * n_step, dim=0)\n",
    "        for k in cfg.img_model.dpm_cond_img:\n",
    "            cond[f'{k}_mask'] = th.stack([cond[f'{k}_mask'][src_idx]] * n_step, dim=0)\n",
    "        \n",
    "    cond, clip_ren = inference_utils.build_condition_image(cond=cond, misc=misc)\n",
    "    cond = inference_utils.prepare_cond_sampling(cond=cond, cfg=cfg, use_render_itp=True)\n",
    "    cond['cfg'] = cfg\n",
    "    if (cfg.img_model.apply_dpm_cond_img) and (np.any(n is not None for n in cfg.img_model.noise_dpm_cond_img)):\n",
    "        cond['use_cond_xt_fn'] = True\n",
    "        for k, p in zip(cfg.img_model.dpm_cond_img, cfg.img_model.noise_dpm_cond_img):\n",
    "            cond[f'{k}_img'] = cond[f'{k}_img'].to(device)\n",
    "            if p is not None:\n",
    "                if 'dpm_noise_masking' in p:\n",
    "                    cond[f'{k}_mask'] = cond[f'{k}_mask'].to(device)\n",
    "                    cond['image'] = cond['image'].to(device)\n",
    "    \n",
    "\n",
    "    if 'render_face' in args.interpolate:\n",
    "        interp_set = args.interpolate.copy()\n",
    "        interp_set.remove('render_face')\n",
    "        \n",
    "    # Interpolate non-spatial\n",
    "    interp_cond = mani_utils.iter_interp_cond(cond, interp_set=interp_set, src_idx=src_idx, dst_idx=dst_idx, n_step=n_step, interp_fn=itp_func)\n",
    "    cond.update(interp_cond)\n",
    "        \n",
    "    # Repeated non-spatial\n",
    "    repeated_cond = mani_utils.repeat_cond_params(cond, base_idx=src_idx, n=n_step, key=mani_utils.without(cfg.param_model.params_selector, args.interpolate + ['light', 'img_latent']))\n",
    "    cond.update(repeated_cond)\n",
    "\n",
    "    # Finalize the cond_params\n",
    "    cond = mani_utils.create_cond_params(cond=cond, key=mani_utils.without(cfg.param_model.params_selector, cfg.param_model.rmv_params))\n",
    "    if cfg.img_cond_model.override_cond != '':\n",
    "        to_tensor_key = ['cond_params'] + cfg.param_model.params_selector + [cfg.img_cond_model.override_cond]\n",
    "    else:    \n",
    "        to_tensor_key = ['cond_params'] + cfg.param_model.params_selector\n",
    "    cond = inference_utils.to_tensor(cond, key=to_tensor_key, device=ckpt_loader.device)\n",
    "    \n",
    "    return cond\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_part(mask, part='faceseg_head'):\n",
    "    mask = np.array(mask)\n",
    "    bg = (mask == 0)\n",
    "    skin = (mask == 1)\n",
    "    l_brow = (mask == 2)\n",
    "    r_brow = (mask == 3)\n",
    "    l_eye = (mask == 4)\n",
    "    r_eye = (mask == 5)\n",
    "    eye_g = (mask == 6)\n",
    "    l_ear = (mask == 7)\n",
    "    r_ear = (mask == 8)\n",
    "    ear_r = (mask == 9)\n",
    "    nose = (mask == 10)\n",
    "    mouth = (mask == 11)\n",
    "    u_lip = (mask == 12)\n",
    "    l_lip = (mask == 13)\n",
    "    neck = (mask == 14)\n",
    "    neck_l = (mask == 15)\n",
    "    cloth = (mask == 16)\n",
    "    hair = (mask == 17)\n",
    "    hat = (mask == 18)\n",
    "    face = np.logical_or.reduce((skin, l_brow, r_brow, l_eye, r_eye, eye_g, l_ear, r_ear, ear_r, nose, mouth, u_lip, l_lip))\n",
    "    if part == 'faceseg_head':\n",
    "        seg_m = (face | neck | hair)\n",
    "    return th.tensor(seg_m)\n",
    "\n",
    "def replace_bg(image_name, rpl=None, vis=False):\n",
    "    mask = PIL.Image.open(f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/face_segment/valid/anno/anno_{image_name}.png\")\n",
    "    img = PIL.Image.open(f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/ffhq_256/valid/{image_name}.jpg\")\n",
    "\n",
    "    # Load to [0, 1] image\n",
    "    mask = face_part(mask)\n",
    "    img = torchvision.transforms.ToTensor()(img) * 2 - 1\n",
    "    rpl_mode = list(rpl.keys())[0]\n",
    "    rpl_val = list(rpl.values())[0]\n",
    "    \n",
    "    if rpl_mode == 'constant':\n",
    "        print(f\"[#] Replace bg with {rpl_val}\")\n",
    "        rpl_bg = (th.randn_like(img) * 0 + rpl_val['val'])\n",
    "    elif rpl_mode == 'balance_rgb':\n",
    "        m_f, m_bg = mask!=0, mask==0\n",
    "        assert (m_f + m_bg) == 256**2\n",
    "        n_f, n_bg, n_pixel = th.sum(m_f)*3, th.sum(m_bg)*3, (256**2)*3\n",
    "        mu_f =  th.sum(m_f * img) / n_f\n",
    "        val = ((0 * n_pixel) - (n_f * mu_f)) / (n_bg)\n",
    "        val = th.clip(val, -1, 1)\n",
    "        rpl_bg = (th.randn_like(img) * 0 + val)\n",
    "        print(f\"[#] Replace bg with {val}\")\n",
    "    elif rpl_mode == 'mu_sd':\n",
    "        rpl_bg = (th.randn_like(img) * rpl_val['sd'] + rpl_val['mu'])\n",
    "    elif rpl_mode == 'image':\n",
    "        bg_img = PIL.Image.open(f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/ffhq_256/valid/{rpl_val}\")\n",
    "        bg_img = torchvision.transforms.ToTensor()(bg_img) * 2 - 1\n",
    "        rpl_bg = bg_img\n",
    "    elif rpl_mode == 'balance_Ndist':\n",
    "        mu_d = 114.49997340551313 / 127.5 - 1\n",
    "        sd = rpl_val['sd']\n",
    "        m_f, m_bg = mask!=0, mask==0\n",
    "        n_f, n_bg, n_pixel = th.sum(m_f)*3, th.sum(m_bg)*3, (256**2)*3\n",
    "        mu_f =  th.sum(m_f * img) / n_f\n",
    "        val = ((mu_d * n_pixel) - (n_f * mu_f)) / (n_bg)\n",
    "        print(f\"[#] Replace bg with Ndist({val}, {rpl_val['sd']}), clip={rpl_val['clip']}\")\n",
    "        rpl_bg = (th.normal(mean=val, std=sd, size=img.shape))\n",
    "        if rpl_val['clip']:\n",
    "            rpl_bg = th.clip(rpl_bg, -1, 1)\n",
    "    else: raise NotImplementedError\n",
    "    \n",
    "    mod_img = img * (mask != 0) + rpl_bg * (mask == 0)\n",
    "    mod_img = torchvision.transforms.Resize((128, 128))(mod_img)\n",
    "    bg_img = img * (mask == 0) + rpl_bg * (mask != 0)\n",
    "    bg_img = torchvision.transforms.Resize((128, 128))(bg_img)\n",
    "    mask = torchvision.transforms.Resize((128, 128), interpolation=PIL.Image.NEAREST)(mask[None])\n",
    "    img = torchvision.transforms.Resize((128, 128))(img)\n",
    "    \n",
    "    print('Mean : ', th.mean(mod_img))\n",
    "    \n",
    "    if vis:\n",
    "        show(torchvision.utils.make_grid((mod_img + 1) * 0.5), figsize=(4, 8))\n",
    "    return img, mod_img, bg_img, mask \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relight(dat, model_kwargs, norm_img, n_step=3, sidx=0, didx=1):\n",
    "    show(torchvision.utils.make_grid((dat + 1) * 0.5), figsize=(4, 8))\n",
    "    # Rendering\n",
    "    args.interpolate = ['render_face']\n",
    "    cond = copy.deepcopy(model_kwargs)\n",
    "    cond = make_condition(cond=cond, \n",
    "                        src_idx=sidx, dst_idx=didx, \n",
    "                        n_step=n_step, itp_func=mani_utils.slerp)\n",
    "\n",
    "    # Reverse \n",
    "    if cfg.img_cond_model.apply:\n",
    "        cond_rev = copy.deepcopy(cond)\n",
    "        cond_rev['cond_img'] = cond_rev['cond_img'][0:1, ...]\n",
    "        cond_rev = pl_sampling.forward_cond_network(model_kwargs=cond_rev)\n",
    "        cond_rev = dict_slice(in_d=cond_rev, keys=cond_rev.keys(), n=1)\n",
    "        if cfg.img_model.conditioning:\n",
    "            cond_rev['cond_params'] = cond_rev['cond_params'][0:1, ...]\n",
    "        # if cfg.img_model.apply_dpm_cond_img and cond['dpm_cond_img'] is not None:\n",
    "        print(cond_rev['spatial_latent'][0].shape)\n",
    "        \n",
    "    noise_map, nf = reverse(dat=dat[0:1, ...], norm_img=norm_img, cond=cond_rev)\n",
    "\n",
    "    # Relight!\n",
    "    cond['use_render_itp'] = True\n",
    "    if cfg.img_cond_model.apply:\n",
    "        cond = pl_sampling.forward_cond_network(model_kwargs=cond)\n",
    "        print(cond['spatial_latent'][0].shape)\n",
    "    out = forward(noise_map=th.repeat_interleave(noise_map, repeats=n_step, dim=0), cond=cond, norm_img=norm_img, nf=nf)\n",
    "    show(torchvision.utils.make_grid(out), figsize=(4, 8))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing :  None\n",
      "[#] Interpolate with <function slerp at 0x7f8a624c6550>\n",
      "Rendering time :  6.019979238510132\n",
      "[#] Repeating cond :  ['shape', 'pose', 'exp', 'cam', 'faceemb', 'shadow']\n",
      "[#] Condition build from parameters in  ['shape', 'pose', 'exp', 'cam', 'faceemb', 'shadow']\n",
      "(3, 672)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mint/guided-diffusion/sample_scripts/py/relighting_sample_id/sampling/../../../sample_utils/inference_utils.py:266: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  cond[f\"{cond_img_name}\"] = th.tensor(bg_tmp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 128, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5637bc9301e04c5c95866d5ccc46e806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_img_idx, all_img_name, args.n_subject = mani_utils.get_samples_list(args.sample_pair_json, \n",
    "                                                                        args.sample_pair_mode, \n",
    "                                                                        args.src_dst, img_path, \n",
    "                                                                        args.n_subject)\n",
    "\n",
    "idx = 2 # Sample index related to json order\n",
    "img_idx = all_img_idx[idx]\n",
    "img_name = all_img_name[idx]\n",
    "n_step = 3\n",
    "\n",
    "dat = th.utils.data.Subset(dataset, indices=img_idx)\n",
    "subset_loader = th.utils.data.DataLoader(dat, batch_size=2,\n",
    "                                    shuffle=False, num_workers=24)\n",
    "                            \n",
    "# Load image & condition                        \n",
    "# for i in ['60182.jpg', '60008.jpg', '68782.jpg', -1.0, -0.5, 0.0, 0.5, 1.0]:\n",
    "dat, model_kwargs = list(iter(subset_loader))[0]\n",
    "d_tmp = []\n",
    "# rpl_bg = [None, {'image':model_kwargs['image_name'][0]}, {'balance_Ndist':{'sd':0.1, 'clip':True}}, {'balance_Ndist':{'sd':0.5, 'clip':True}}, {'balance_Ndist':{'sd':58.050383371049826/127.5, 'clip':True}}] \n",
    "rpl_bg = [None, {'balance_Ndist':{'sd':0.1, 'clip':True}} , {'balance_Ndist':{'sd':58.050383371049826/127.5, 'clip':True}}]\n",
    "all_output = []\n",
    "for i in rpl_bg:\n",
    "    print(\"Replacing : \", i)\n",
    "    if i is not None:\n",
    "        d, fg, bg, m = replace_bg(image_name=model_kwargs['image_name'][0].split('.')[0], rpl=i)\n",
    "        dat = fg[None][0:1, ...].cuda()\n",
    "        model_kwargs['faceseg_bg_noface&nohair_mask'] = ~m[None]\n",
    "    else:\n",
    "        dat = dat[0:1, ...].cuda()\n",
    "\n",
    "    #TODO: Override the model_kwargs argument for 'mask'=m, 'image'=dat, 'bg'=bg\n",
    "    # print(model_kwargs.keys())\n",
    "    model_kwargs['faceseg_bg_noface&nohair_img'] = dat\n",
    "    model_kwargs['image'] = dat\n",
    "    \n",
    "    # show(torchvision.utils.make_grid(th.clip((model_kwargs['faceseg_bg_noface&nohair_img']+1)*0.5, 0, 1)), figsize=(10, 15))\n",
    "    # show(torchvision.utils.make_grid(model_kwargs['faceseg_bg_noface&nohair_mask']*1.0), figsize=(10, 15))\n",
    "    # # ['faceseg_bg_noface&nohair_img'] = m[None]\n",
    "    # print(model_kwargs['faceseg_bg_noface&nohair_img'].shape, model_kwargs['faceseg_bg_noface&nohair_mask'].shape)\n",
    "    # print(bg.shape, fg.shape, m.shape)\n",
    "    # assert False\n",
    "    \n",
    "    out = relight(dat=dat.clone(), model_kwargs=model_kwargs, n_step=3, norm_img=False)\n",
    "    out = th.cat(((dat+1) * 0.5, out), dim=0)\n",
    "    # show(torchvision.utils.make_grid(out), figsize=(10, 15))\n",
    "    # show(torchvision.utils.make_grid(th.clip(th.cat(((dat+1) * 0.5, out), dim=0), 0, 1)), figsize=(10, 15))\n",
    "    all_output.append(torchvision.utils.make_grid(out))\n",
    "all_output = th.clip(th.cat((all_output), dim=1), 0, 1)\n",
    "show(all_output, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4712/2307003955.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m all_img_idx, all_img_name, args.n_subject = mani_utils.get_samples_list(args.sample_pair_json, \n\u001b[1;32m      3\u001b[0m                                                                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_pair_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                                                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_dst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                         args.n_subject)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert False\n",
    "all_img_idx, all_img_name, args.n_subject = mani_utils.get_samples_list(args.sample_pair_json, \n",
    "                                                                        args.sample_pair_mode, \n",
    "                                                                        args.src_dst, img_path, \n",
    "                                                                        args.n_subject)\n",
    "\n",
    "for idx in range(len(all_img_idx)):\n",
    "    img_idx = all_img_idx[idx]\n",
    "    img_name = all_img_name[idx]\n",
    "    n_step = 3\n",
    "\n",
    "    dat = th.utils.data.Subset(dataset, indices=img_idx)\n",
    "    subset_loader = th.utils.data.DataLoader(dat, batch_size=2,\n",
    "                                        shuffle=False, num_workers=24)\n",
    "                                \n",
    "    # Load image & condition                        \n",
    "    dat, model_kwargs = list(iter(subset_loader))[0]\n",
    "    d_tmp = []\n",
    "\n",
    "    rpl_bg = [None, {'balance_Ndist':{'sd':0.1, 'clip':True}}, {'balance_Ndist':{'sd':0.5, 'clip':True}}, {'balance_Ndist':{'sd':58.050383371049826/127.5, 'clip':True}}] \n",
    "    all_output = []\n",
    "    for i in rpl_bg:\n",
    "        print(\"Replacing : \", i)\n",
    "        if i is not None:\n",
    "            d, fg, bg, m = replace_bg(image_name=model_kwargs['image_name'][0].split('.')[0], rpl=i)\n",
    "            dat = fg[None][0:1, ...].cuda()\n",
    "        else:\n",
    "            dat = dat[0:1, ...].cuda()\n",
    "\n",
    "        # out = BxCxHxW\n",
    "        out = relight(dat=dat.clone(), model_kwargs=model_kwargs, n_step=3, norm_img=False)\n",
    "        out = th.cat(((dat+1) * 0.5, out), dim=0)\n",
    "        all_output.append(torchvision.utils.make_grid(out))\n",
    "    all_output = th.clip(th.cat((all_output), dim=1), 0, 1)\n",
    "    show(all_output, figsize=(15, 15))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e726457752a9f31dd08e885b4d7ad782b9d2db89819f0ec82c996add1b497d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
