{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image sample (DDPM - guided diffusion - Diffusion beats gans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import print_function \n",
    "import argparse\n",
    "import os, sys, glob\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import torch.distributed as dist\n",
    "import datetime\n",
    "from collections import namedtuple\n",
    "sys.path.insert(0, '../')\n",
    "import deca_dpm\n",
    "from guided_diffusion.script_util import (\n",
    "    model_and_diffusion_defaults,\n",
    "    create_deca_and_diffusion,\n",
    "    add_dict_to_argparser,\n",
    "    args_to_dict,\n",
    "    seed_all,\n",
    "    diffusion_defaults,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_argparser(log_dir, deca_model_path,**kwargs):\n",
    "    defaults = dict(\n",
    "        clip_denoised=True,\n",
    "        num_samples=1,\n",
    "        batch_size=15,\n",
    "        use_ddim=False,\n",
    "        deca_model_path=deca_model_path,\n",
    "        log_dir=log_dir,\n",
    "        diffusion_step=1000,\n",
    "        timestep_respacing=1000,\n",
    "        deca_cond=True,\n",
    "        bound=0.25\n",
    "    )\n",
    "\n",
    "    defaults.update(model_and_diffusion_defaults())\n",
    "    return namedtuple('GenericDict', defaults.keys())(**defaults)\n",
    "\n",
    "def model_and_diffusion_defaults():\n",
    "    \"\"\"\n",
    "    Defaults for image training.\n",
    "    \"\"\"\n",
    "    res = dict(\n",
    "        num_channels=128,\n",
    "        num_res_blocks=2,\n",
    "        num_heads=4,\n",
    "        num_heads_upsample=-1,\n",
    "        num_head_channels=-1,\n",
    "        attention_resolutions=\"16,8\",\n",
    "        channel_mult=\"\",\n",
    "        dropout=0.0,\n",
    "        class_cond=False,\n",
    "        use_checkpoint=False,\n",
    "        use_scale_shift_norm=True,\n",
    "        resblock_updown=False,\n",
    "        use_new_attention_order=False,\n",
    "        deca_cond=False\n",
    "    )\n",
    "    res.update(diffusion_defaults())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List model_logs\n",
    "ct = \"\"\n",
    "# sshfs_path = \"/home/mint/mnt_tl_puntawat-ms-7c37/home/puntawat/Mint/Work/Vision/Diffusion/mount/\"\n",
    "sshfs_path = \"/data/mint/model_logs/\"\n",
    "model_logs_path = f\"{sshfs_path}/{ct}/\"\n",
    "\n",
    "# args\n",
    "log_dir = \"deca_bound1\"\n",
    "\n",
    "step = \"050000\"\n",
    "# step = \"150000\"\n",
    "# ckpt = f\"model{step}\"\n",
    "ckpt = f\"ema_0.9999_{step}\"\n",
    "deca_model_path = f\"{model_logs_path}/{log_dir}/Deca_{ckpt}.pt\"\n",
    "\n",
    "args = create_argparser(log_dir=log_dir, deca_model_path=deca_model_path)\n",
    "\n",
    "# Check model_logs\n",
    "if not os.path.isdir(os.path.join(model_logs_path, args.log_dir)):\n",
    "    print(\"No logs folder\")\n",
    "    raise FileNotFoundError\n",
    "else: \n",
    "    if not os.path.isdir(os.path.join(model_logs_path, args.log_dir, \"samples\")):\n",
    "        os.makedirs(os.path.join(model_logs_path, args.log_dir, \"samples\"))\n",
    "\n",
    "model_and_diffusion = model_and_diffusion_defaults()\n",
    "\n",
    "deca_model, diffusion = create_deca_and_diffusion(\n",
    "    **args_to_dict(args, model_and_diffusion.keys())\n",
    ")\n",
    "\n",
    "deca_model.load_state_dict(\n",
    "    th.load(args.deca_model_path, map_location=\"cpu\")\n",
    ")\n",
    "\n",
    "deca_model.to('cuda')\n",
    "deca_model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decolor(s, out_c='rgb'):\n",
    "    if out_c in ['rgb', 'rbg', 'brg', 'bgr', 'grb', 'gbr']:\n",
    "        s_ = ((s + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    elif out_c == 'luv':\n",
    "        s_ = ((s + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    elif out_c == 'ycrcb':\n",
    "        s_ = ((s + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "    elif out_c in ['hsv', 'hls']:\n",
    "        h = (s[..., [0]] + 1) * 90.0 \n",
    "        l_s = (s[..., [1]] + 1) * 127.5\n",
    "        v = (s[..., [2]] + 1) * 127.5\n",
    "        s_ = th.cat((h, l_s, v), axis=2).clamp(0, 255).to(th.uint8)\n",
    "    elif out_c == 'sepia':\n",
    "        s_ = ((s + 1) * 127.5).clamp(0, 255).to(th.uint8)\n",
    "\n",
    "    else: raise NotImplementedError\n",
    "\n",
    "    return s_\n",
    "\n",
    "def plot_sample(img, **kwargs):\n",
    "    columns = 6\n",
    "    rows = 10\n",
    "    fig = plt.figure(figsize=(20, 20), dpi=100)\n",
    "    img = img.permute(0, 2, 3, 1) # BxHxWxC\n",
    "    pt = 0\n",
    "    for i in range(0, img.shape[0]):\n",
    "        s_ = decolor(s=img[i], out_c='rgb')\n",
    "        s_ = s_.detach().cpu().numpy()\n",
    "        fig.add_subplot(rows, columns, pt+1)\n",
    "        plt.imshow(s_)\n",
    "        pt += 1\n",
    "\n",
    "        if kwargs is not None:\n",
    "            # Plot other images\n",
    "            for k in kwargs:\n",
    "                fig.add_subplot(rows, columns, pt+1)\n",
    "                s_ = decolor(s=kwargs[k][i].permute(1, 2, 0), out_c='rgb')\n",
    "                s_ = s_.detach().cpu().numpy()\n",
    "                plt.imshow(s_)\n",
    "                pt += 1\n",
    "    plt.subplots_adjust(left=0.1,\n",
    "                        bottom=0.1, \n",
    "                        right=0.65, \n",
    "                        top=0.9, \n",
    "                        wspace=0.1, \n",
    "                        hspace=0.2)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "class PLInference(pl.LightningModule):\n",
    "    def __init__(self, deca_model, sample_fn):\n",
    "        super(PLInference, self).__init__()\n",
    "        self.deca_model = deca_model\n",
    "        self.sample_fn = sample_fn\n",
    "        self.deca_dpm = deca_dpm.Diffusion_DECA(deca_model=self.deca_model, diffusion=diffusion, bound=args.bound, progress=True)\n",
    "\n",
    "    def forward(self):\n",
    "        seed_all(33)\n",
    "        if self.sample_fn == 'p_sample_loop':\n",
    "            deca_output = self.deca_dpm.p_sample_loop(\n",
    "                                        shape_dict={'deca':(args.batch_size, 159)})\n",
    "\n",
    "        return {\"deca_output\":deca_output}\n",
    "\n",
    "pl_inference = PLInference(deca_model=deca_model, sample_fn='p_sample_loop')\n",
    "sample = pl_inference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch_lightning as pl\n",
    "# class PLInference(pl.LightningModule):\n",
    "#     def __init__(self, img_model, deca_model, sample_fn):\n",
    "#         super(PLInference, self).__init__()\n",
    "#         self.img_model=img_model\n",
    "#         self.deca_model=deca_model\n",
    "#         self.sample_fn = sample_fn\n",
    "\n",
    "#     def forward(self):\n",
    "#         seed_all(33)\n",
    "#         sample = self.sample_fn(\n",
    "#             model=self.img_model,\n",
    "#             shape=(args.batch_size, in_ch, args.image_size, args.image_size),\n",
    "#             clip_denoised=args.clip_denoised,\n",
    "#         )\n",
    "#         return sample\n",
    "\n",
    "# pl_inference = PLInference(img_model=img_model, deca_model=deca_model, sample_fn=diffusion.p_sample_loop)\n",
    "# sample = pl_inference()\n",
    "# plot(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test params_to_model fn\n",
    "def read_params(path):\n",
    "    params = pd.read_csv(path, header=None, sep=\" \", index_col=False, lineterminator='\\n')\n",
    "    params.rename(columns={0:'img_name'}, inplace=True)\n",
    "    params = params.set_index('img_name').T.to_dict('list')\n",
    "    return params\n",
    "\n",
    "def swap_key(params):\n",
    "    params_s = defaultdict(dict)\n",
    "    for params_name, v in params.items():\n",
    "        for img_name, params_value in v.items():\n",
    "            params_s[img_name][params_name] = np.array(params_value).astype(np.float64)\n",
    "\n",
    "    return params_s\n",
    "\n",
    "def normalize(arr, min_val=None, max_val=None, a=-1, b=1):\n",
    "    '''\n",
    "    Normalize any vars to [a, b]\n",
    "    :param a: new minimum value\n",
    "    :param b: new maximum value\n",
    "    :param arr: np.array shape=(N, #params_dim) e.g. deca's params_dim = 159\n",
    "    ref : https://stats.stackexchange.com/questions/178626/how-to-normalize-data-between-1-and-1\n",
    "    '''\n",
    "    if max_val is None and min_val is None:\n",
    "        max_val = np.max(arr, axis=0)    \n",
    "        min_val = np.min(arr, axis=0)\n",
    "\n",
    "    arr_norm = ((b-a) * (arr - min_val) / (max_val - min_val)) + a\n",
    "    return arr_norm, min_val, max_val\n",
    "\n",
    "def denormalize(arr_norm, min_val, max_val, a=-1, b=1):\n",
    "    arr_denorm = (((arr_norm - a) * (max_val - min_val)) / (b - a)) + min_val\n",
    "    return arr_denorm\n",
    "\n",
    "params_key = ['shape', 'pose', 'exp', 'cam']\n",
    "# anno_path = glob.glob(f'/home/mint/mnt_tl_puntawat-ms-7c37/home/puntawat/mnt/sda2/anno/ffhq_256_with_anno/params/*.txt')\n",
    "anno_path = glob.glob(f'/data/mint/ffhq_256_with_anno/params/*.txt')\n",
    "params = {}\n",
    "for k in params_key:\n",
    "    for p in anno_path:\n",
    "        # Params\n",
    "        if k in p:\n",
    "            print(f'Key=> {k} : Filename=>{p}')\n",
    "            params[k] = read_params(path=p)\n",
    "\n",
    "params_s = swap_key(params)\n",
    "\n",
    "all_params = []\n",
    "for img_name in params_s:\n",
    "    each_img = []\n",
    "    for k in params_key:\n",
    "        each_img.append(params_s[img_name][k])\n",
    "    all_params.append(np.concatenate(each_img))\n",
    "all_params = np.stack(all_params)\n",
    "_, min_val, max_val = normalize(a=-args.bound, b=args.bound, arr=all_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import Expression, arg, parse\n",
    "from pickle import PickleError\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import glob, os\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "from model_3d.FLAME import FLAME\n",
    "from model_3d.FLAME.config import cfg as flame_cfg\n",
    "from collections import defaultdict\n",
    "from model_3d.FLAME.utils.renderer import SRenderY\n",
    "import model_3d.FLAME.utils.util as util\n",
    "import model_3d.FLAME.utils.detectors as detectors\n",
    "from skimage.io import imread, imsave\n",
    "from skimage.transform import estimate_transform, warp, resize, rescale\n",
    "\n",
    "\n",
    "def params_to_model(shape, exp, pose, cam, i, uvdn=None):\n",
    "    verts, landmarks2d, landmarks3d = flame(shape_params=shape, \n",
    "            expression_params=exp, \n",
    "            pose_params=pose)\n",
    "\n",
    "    renderer = SRenderY(image_size=256, obj_filename=flame_cfg.model.topology_path, uv_size=flame_cfg.model.uv_size).cuda()\n",
    "\n",
    "    ## projection\n",
    "    landmarks2d = util.batch_orth_proj(landmarks2d, cam)[:,:,:2]; landmarks2d[:,:,1:] = -landmarks2d[:,:,1:]#; landmarks2d = landmarks2d*self.image_size/2 + self.image_size/2\n",
    "    landmarks3d = util.batch_orth_proj(landmarks3d, cam); landmarks3d[:,:,1:] = -landmarks3d[:,:,1:] #; landmarks3d = landmarks3d*self.image_size/2 + self.image_size/2\n",
    "    trans_verts = util.batch_orth_proj(verts, cam); trans_verts[:,:,1:] = -trans_verts[:,:,1:]\n",
    "\n",
    "    ## rendering\n",
    "    shape_images = renderer.render_shape(verts, trans_verts)\n",
    "\n",
    "    opdict = {'verts' : verts,}\n",
    "\n",
    "    os.makedirs('./rendered_obj', exist_ok=True)\n",
    "    save_obj(renderer=renderer, filename=(f'./rendered_obj/{i}.obj'), opdict=opdict)\n",
    "    # img_uvdn = th.unsqueeze(th.tensor(params['uv_detail_normals']).permute((2, 0, 1)), dim=0).cuda().to(flame.dtype)\n",
    "\n",
    "    # detail_normal_images = F.grid_sample(img_uvdn, ops['grid'], align_corners=False)*ops['alpha_images']\n",
    "    # shape_detail_images = renderer.render_shape(verts, trans_verts, detail_normal_images=detail_normal_images)\n",
    "\n",
    "#     plt.imshow(np.concatenate((shape_images[0].detach().cpu().numpy().transpose([1, 2, 0]), \n",
    "#             ops['images'][0].detach().cpu().numpy().transpose([1, 2, 0]),\n",
    "#             shape_detail_images[0].detach().cpu().numpy().transpose([1, 2, 0])\n",
    "#             ), axis=1))\n",
    "#     plt.show()\n",
    "    return {\"shape_images\":shape_images}\n",
    "\n",
    "def save_obj(renderer, filename, opdict):\n",
    "    '''\n",
    "    vertices: [nv, 3], tensor\n",
    "    texture: [3, h, w], tensor\n",
    "    '''\n",
    "    i = 0\n",
    "    vertices = opdict['verts'][i].cpu().numpy()\n",
    "    faces = renderer.faces[0].cpu().numpy()\n",
    "    colors = np.ones(shape=vertices.shape) * 127.5\n",
    "\n",
    "    # save coarse mesh, with texture and normal map\n",
    "    # normal_map = util.tensor2image(opdict['uv_detail_normals'][i]*0.5 + 0.5)\n",
    "    util.write_obj(filename, vertices, faces, colors=colors)\n",
    "\n",
    "\n",
    "flame = FLAME.FLAME(flame_cfg.model).cuda()\n",
    "\n",
    "img_ = []\n",
    "from tqdm.auto import tqdm\n",
    "for i in tqdm(range(sample['deca_output'].shape[0])):\n",
    "    deca_params = sample['deca_output'][i].clone()\n",
    "    deca_params = denormalize(deca_params, min_val=th.tensor(min_val).cuda(), max_val=th.tensor(max_val).cuda(), a=-args.bound, b=args.bound).float()\n",
    "    shape = deca_params[None, :100]\n",
    "    pose = deca_params[None, 100:106]\n",
    "    exp = deca_params[None, 106:156]\n",
    "    cam = deca_params[None, 156:]\n",
    "    img = params_to_model(shape=shape, exp=exp, pose=pose, cam=cam, i=i)\n",
    "    img_.append(img[\"shape_images\"])\n",
    "\n",
    "plot_sample(th.cat(img_, dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Face params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test params_to_model fn\n",
    "def read_params(path):\n",
    "    params = pd.read_csv(path, header=None, sep=\" \", index_col=False, lineterminator='\\n')\n",
    "    params.rename(columns={0:'img_name'}, inplace=True)\n",
    "    params = params.set_index('img_name').T.to_dict('list')\n",
    "    return params\n",
    "\n",
    "def swap_key(params):\n",
    "    params_s = defaultdict(dict)\n",
    "    for params_name, v in params.items():\n",
    "        for img_name, params_value in v.items():\n",
    "            params_s[img_name][params_name] = np.array(params_value).astype(np.float64)\n",
    "\n",
    "    return params_s\n",
    "\n",
    "\n",
    "params_key = ['shape', 'pose', 'exp', 'cam']\n",
    "# anno_path = glob.glob(f'/home/mint/mnt_tl_puntawat-ms-7c37/home/puntawat/mnt/sda2/anno/ffhq_256_with_anno/params/*.txt')\n",
    "anno_path = glob.glob(f'/data/mint/ffhq_256_with_anno/params/*.txt')\n",
    "params = {}\n",
    "for k in params_key:\n",
    "    for p in anno_path:\n",
    "        # Params\n",
    "        if k in p:\n",
    "            print(f'Key=> {k} : Filename=>{p}')\n",
    "            params[k] = read_params(path=p)\n",
    "\n",
    "params_s = swap_key(params)\n",
    "\n",
    "img_dataset_path = '/data/mint/ffhq_256_with_anno/ffhq_256/train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name_list = [str(n) + '.jpg' for n in list(np.random.randint(0, 60000, 10))]\n",
    "\n",
    "render_img_list = []\n",
    "src_img_list = []\n",
    "img_list = []\n",
    "for img_name in img_name_list:\n",
    "    shape = th.tensor(params_s[img_name]['shape'][None, :]).float().cuda()\n",
    "    pose = th.tensor(params_s[img_name]['pose'][None, :]).float().cuda()\n",
    "    exp = th.tensor(params_s[img_name]['exp'][None, :]).float().cuda()\n",
    "    cam = th.tensor(params_s[img_name]['cam'][None, :]).float().cuda()\n",
    "    # print(cam)\n",
    "    # assert False\n",
    "\n",
    "    src_img = PIL.Image.open(img_dataset_path + img_name)\n",
    "    src_img = (th.tensor(np.transpose(src_img, (2, 0, 1)))[None, :] / 127.5) - 1\n",
    "    src_img_list.append(src_img)\n",
    "\n",
    "    render_img = params_to_model(shape=shape, exp=exp, pose=pose, cam=cam, i=img_name)\n",
    "    render_img_list.append(render_img[\"shape_images\"])\n",
    "\n",
    "src_img = th.cat(src_img_list, dim=0)\n",
    "render_img = th.cat(render_img_list, dim=0)\n",
    "print(src_img.shape, render_img.shape)\n",
    "plot_sample(img=src_img, render_img=render_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample[\"deca_output\"].shape)\n",
    "sample_params = {'shape':[], 'pose':[], 'exp':[], 'cam':[]}\n",
    "for i in range(sample[\"deca_output\"].shape[0]):\n",
    "    deca_params = sample[\"deca_output\"][i].clone()\n",
    "    deca_params = denormalize(deca_params, min_val=th.tensor(min_val).cuda(), max_val=th.tensor(max_val).cuda(), a=-args.bound, b=args.bound).float()\n",
    "    shape = deca_params[None, :100].cpu().numpy()\n",
    "    pose = deca_params[None, 100:106].cpu().numpy()\n",
    "    exp = deca_params[None, 106:156].cpu().numpy()\n",
    "    cam = deca_params[None, 156:].cpu().numpy()\n",
    "    sample_params['shape'].append(shape)\n",
    "    sample_params['pose'].append(pose)\n",
    "    sample_params['exp'].append(exp)\n",
    "    sample_params['cam'].append(cam)\n",
    "\n",
    "for k in sample_params.keys():\n",
    "    sample_params[k] = np.concatenate(sample_params[k], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_analyse(params, key, sample_params=None):\n",
    "    '''\n",
    "    :params: tensor in shape BxD\n",
    "    '''\n",
    "    print(\"[#] Across all params&images\")\n",
    "    print(f\"Train set : {key}-params range : min={np.min(params)}, max={np.max(params)}\")\n",
    "    print(f\"Train set : {key}-params range : mean={np.mean(params)}, std={np.std(params)}\")\n",
    "    print(\"[#] Across all images\")\n",
    "    fig = plt.figure(figsize=(5, 5), dpi=100)\n",
    "    plt.bar(x=np.arange(params.shape[-1]), height=np.max(params, axis=0)-np.min(params, axis=0), bottom=np.min(params, axis=0), color=(0, 0, 1, 0.25), label=\"Train set\")\n",
    "\n",
    "\n",
    "    if sample is not None:\n",
    "        plt.bar(x=np.arange(sample_params.shape[-1]), height=np.max(sample_params, axis=0)-np.min(sample_params, axis=0), bottom=np.min(sample_params, axis=0), color=(1, 0, 0, 0.5), label=\"Sampling\")\n",
    "\n",
    "    plt.legend()\n",
    "    fig.show()\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "def dist_analyse(params, key, sample_params=None):\n",
    "    group_labels = [f'comp-{i}' for i in range(params.shape[-1])]\n",
    "    hist_data_params = [params[:, i] for i in range(params.shape[-1])]\n",
    "    fig = ff.create_distplot(hist_data_params, group_labels, show_rug=False)\n",
    "    fig.write_html(f\"./{key}-dist.html\")\n",
    "\n",
    "\n",
    "params_list = ['shape', 'pose', 'exp', 'cam']\n",
    "# params_list = ['cam']\n",
    "for each_p in params_list:\n",
    "    p = np.concatenate([params_s[k][each_p][None, :] for k in params_s.keys()], axis=0)\n",
    "    simple_analyse(params=p, key=each_p, sample_params=sample_params[each_p])\n",
    "    # dist_analyse(params=p, key=each_p, sample_params=sample_params[each_p])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
