{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "# Dataset\n",
    "parser.add_argument('--set', type=str, default='valid')\n",
    "# Model/Config\n",
    "parser.add_argument('--step', type=str, default='050000')\n",
    "parser.add_argument('--ckpt_selector', type=str, default='ema')\n",
    "parser.add_argument('--cfg_name', type=str, default=None)\n",
    "parser.add_argument('--log_dir', type=str, default=None)\n",
    "# Interpolation\n",
    "parser.add_argument('--interpolate', nargs='+', default=None)\n",
    "parser.add_argument('--interpolate_step', type=int, default=15)\n",
    "parser.add_argument('--interpolate_noise', action='store_true', default=False)\n",
    "parser.add_argument('--lerp', action='store_true', default=False)\n",
    "parser.add_argument('--slerp', action='store_true', default=False)\n",
    "parser.add_argument('--uncond_sampling', action='store_true', default=False)\n",
    "parser.add_argument('--uncond_sampling_iters', type=int, default=1)\n",
    "parser.add_argument('--reverse_sampling', action='store_true', default=False)\n",
    "parser.add_argument('--separate_reverse_sampling', action='store_true', default=False)\n",
    "# Samples selection\n",
    "parser.add_argument('--n_subject', type=int, default=-1)\n",
    "parser.add_argument('--sample_pair_json', type=str, default=None)\n",
    "parser.add_argument('--sample_pair_mode', type=str, default=None)\n",
    "parser.add_argument('--src_dst', nargs='+', default=[])\n",
    "# Pertubation the image condition\n",
    "parser.add_argument('--perturb_img_cond', action='store_true', default=False)\n",
    "parser.add_argument('--perturb_mode', type=str, default='zero')\n",
    "parser.add_argument('--perturb_where', nargs='+', default=[])\n",
    "\n",
    "# Rendering\n",
    "parser.add_argument('--render_mode', type=str, default=\"shape\")\n",
    "parser.add_argument('--rotate_normals', action='store_true', default=False)\n",
    "# Diffusion\n",
    "parser.add_argument('--diffusion_steps', type=int, default=1000)\n",
    "parser.add_argument('--denoised_clamp', type=float, default=None)\n",
    "# Misc\n",
    "parser.add_argument('--seed', type=int, default=23)\n",
    "parser.add_argument('--gpu_id', type=str, default=\"0\")\n",
    "parser.add_argument('--save_intermediate', action='store_true', default=False)\n",
    "parser.add_argument('--postfix', type=str, default='')\n",
    "parser.add_argument('--ovr_img', type=str, default=None)\n",
    "parser.add_argument('--ovr_mod', action='store_true', default=False)\n",
    "parser.add_argument('--norm_img', action='store_true', default=False)\n",
    "parser.add_argument('--use_global_norm', action='store_true', default=False)\n",
    "parser.add_argument('--norm_space', type=str, default='rgb')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "import os, sys, glob\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "import PIL, cv2\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "import scipy\n",
    "sys.path.insert(0, '../../')\n",
    "from guided_diffusion.script_util import (\n",
    "    seed_all,\n",
    ")\n",
    "\n",
    "from guided_diffusion.tensor_util import (\n",
    "    make_deepcopyable,\n",
    "    dict_slice,\n",
    ")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "def show(imgs, figsize=(8, 6)):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=figsize)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "\n",
    "from guided_diffusion.dataloader.img_deca_datasets import load_data_img_deca\n",
    "\n",
    "# Sample utils\n",
    "sys.path.insert(0, '../')\n",
    "from sample_utils import (\n",
    "    ckpt_utils, \n",
    "    params_utils, \n",
    "    vis_utils, \n",
    "    file_utils, \n",
    "    inference_utils, \n",
    "    mani_utils,\n",
    ")\n",
    "device = 'cuda' if th.cuda.is_available() and th._C._cuda_getDeviceCount() > 0 else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#] Model Path : \n",
      "#0 : /data/mint/model_logs_mount/v12/Masked_Face_woclip+BgNoHead+shadow/\n",
      "[#] Config Path :  ['/home/mint/guided-diffusion/config/CVPRs/Final_Candidates/Masked_Face_woclip+BgNoHead+shadow.yaml']\n",
      "Merging with :  Namespace(cfg='/home/mint/guided-diffusion/config/CVPRs/Final_Candidates/Masked_Face_woclip+BgNoHead+shadow.yaml')\n",
      "\n",
      "[#] Sampling with diffusion_steps = 1000\n",
      "[#] Available ckpt :  ['_000000.pt', '_000000.pt', '_010000.pt', '_010000.pt', '_020000.pt', '_020000.pt', '_030000.pt', '_030000.pt', '_040000.pt', '_040000.pt', '_050000.pt', '_050000.pt', '_060000.pt', '_060000.pt', '_070000.pt', '_070000.pt', '_080000.pt', '_080000.pt', '_090000.pt', '_090000.pt', '_100000.pt', '_100000.pt', '_110000.pt', '_110000.pt', '_120000.pt', '_120000.pt', '_130000.pt', '_130000.pt', '_140000.pt', '_140000.pt', '_150000.pt', '_150000.pt', '_160000.pt', '_160000.pt', '_170000.pt', '_170000.pt', '_180000.pt', '_180000.pt', '_190000.pt', '_190000.pt', '_200000.pt', '_200000.pt', '_210000.pt', '_210000.pt', '_220000.pt', '_220000.pt', '_230000.pt', '_230000.pt', '_240000.pt', '_240000.pt', '_250000.pt', '_250000.pt', '_260000.pt', '_260000.pt', '_270000.pt', '_270000.pt', '_280000.pt', '_280000.pt', '_290000.pt', '_290000.pt', '_300000.pt', '_300000.pt', '_310000.pt', '_310000.pt', '_320000.pt', '_320000.pt', '_330000.pt', '_330000.pt', '_340000.pt', '_340000.pt', '_350000.pt', '_350000.pt', '_360000.pt', '_360000.pt']\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Use Hadamart-tanh\n",
      "[#] Loading.../data/mint/model_logs_mount/v12/Masked_Face_woclip+BgNoHead+shadow//ImgCond_ema_0.9999_050000.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_47562/2459885481.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"[#] Sampling with diffusion_steps = {args.diffusion_steps}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiffusion_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mmodel_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiffusion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_selector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mckpt_selector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mmodel_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minference_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/guided-diffusion/sample_scripts/debug_script/../sample_utils/ckpt_utils.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, ckpt_selector, step)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             )\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mmodel_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dpm_sampling_deca/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/miniconda3/envs/dpm_sampling_deca/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dpm_sampling_deca/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dpm_sampling_deca/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/dpm_sampling_deca/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    608\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_format\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "seed_all(47)\n",
    "\n",
    "################ SETTINGS ################\n",
    "# args.cfg_name = \"Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow.yaml\"\n",
    "# args.log_dir = \"Masked_Face_woclip+UNet_BgNoHead_share_dpm_noise_masking+shadow\"\n",
    "# args.cfg_name = \"Masked_Face_woclip+BgNoHead+shadow.yaml\"\n",
    "# args.log_dir = \"Masked_Face_woclip+BgNoHead+shadow\"\n",
    "\n",
    "args.cfg_name = \"Masked_Face_woclip+BgNoHead+shadow.yaml\"\n",
    "args.log_dir = \"Masked_Face_woclip+BgNoHead+shadow\"\n",
    "\n",
    "args.step = '050000'\n",
    "args.ckpt_selector = 'ema'\n",
    "args.set = 'valid'\n",
    "# args.sample_pair_json = './sample_json/itw_samples.json'\n",
    "# args.sample_pair_json = './sample_json/ipynb_samples.json'\n",
    "# args.sample_pair_json = './sample_json/debug_multipie.json'\n",
    "args.sample_pair_json = '/home/mint/guided-diffusion/sample_scripts/py/relighting_sample_id/sampling/sample_json/top25_shadow_pair.json'\n",
    "args.sample_pair_mode = 'pair'\n",
    "# dataset = 'mp'\n",
    "dataset = 'ffhq'\n",
    "# dataset = 'itw'\n",
    "\n",
    "# Load Ckpt\n",
    "if args.cfg_name is None:\n",
    "    args.cfg_name = args.log_dir + '.yaml'\n",
    "ckpt_loader = ckpt_utils.CkptLoader(log_dir=args.log_dir, cfg_name=args.cfg_name)\n",
    "cfg = ckpt_loader.cfg\n",
    "\n",
    "print(f\"[#] Sampling with diffusion_steps = {args.diffusion_steps}\")\n",
    "cfg.diffusion.diffusion_steps = args.diffusion_steps\n",
    "model_dict, diffusion = ckpt_loader.load_model(ckpt_selector=args.ckpt_selector, step=args.step)\n",
    "model_dict = inference_utils.eval_mode(model_dict)\n",
    "\n",
    "# Load dataset\n",
    "\n",
    "if dataset == 'itw':\n",
    "    img_dataset_path = f\"/data/mint/DPM_Dataset/ITW/itw_images_aligned/\"\n",
    "    deca_dataset_path = f\"/data/mint/DPM_Dataset/ITW/params/\"\n",
    "    img_ext = '.png'\n",
    "    cfg.dataset.training_data = 'ITW'\n",
    "    cfg.dataset.data_dir = f'{cfg.dataset.root_path}/{cfg.dataset.training_data}/itw_images_aligned/'\n",
    "elif dataset == 'ffhq':\n",
    "    img_dataset_path = f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/ffhq_256/\"\n",
    "    deca_dataset_path = f\"/data/mint/DPM_Dataset/ffhq_256_with_anno/params/\"\n",
    "    img_ext = '.jpg'\n",
    "    cfg.dataset.training_data = 'ffhq_256_with_anno'\n",
    "    cfg.dataset.data_dir = f'{cfg.dataset.root_path}/{cfg.dataset.training_data}/ffhq_256/'\n",
    "elif dataset == 'mp':\n",
    "    img_dataset_path = f\"/data/mint/DPM_Dataset/MultiPIE/mp_aligned/\"\n",
    "    deca_dataset_path = f\"/data/mint/DPM_Dataset/MultiPIE/params/\"\n",
    "    img_ext = '.png'\n",
    "    cfg.dataset.training_data = 'MultiPIE'\n",
    "    cfg.dataset.data_dir = f'{cfg.dataset.root_path}/{cfg.dataset.training_data}/mp_aligned/'\n",
    "else: raise NotImplementedError\n",
    "\n",
    "cfg.dataset.deca_dir = f'{cfg.dataset.root_path}/{cfg.dataset.training_data}/params/'\n",
    "cfg.dataset.face_segment_dir = f\"{cfg.dataset.root_path}/{cfg.dataset.training_data}/face_segment/\"\n",
    "cfg.dataset.deca_rendered_dir = f\"{cfg.dataset.root_path}/{cfg.dataset.training_data}/rendered_images/\"\n",
    "cfg.dataset.laplacian_mask_dir = f\"{cfg.dataset.root_path}/{cfg.dataset.training_data}/eyes_segment/\"\n",
    "cfg.dataset.laplacian_dir = f\"{cfg.dataset.root_path}/{cfg.dataset.training_data}/laplacian/\"\n",
    "\n",
    "loader, dataset, avg_dict = load_data_img_deca(\n",
    "    data_dir=img_dataset_path,\n",
    "    deca_dir=deca_dataset_path,\n",
    "    batch_size=int(1e7),\n",
    "    image_size=cfg.img_model.image_size,\n",
    "    deterministic=cfg.train.deterministic,\n",
    "    augment_mode=cfg.img_model.augment_mode,\n",
    "    resize_mode=cfg.img_model.resize_mode,\n",
    "    in_image_UNet=cfg.img_model.in_image,\n",
    "    params_selector=cfg.param_model.params_selector,\n",
    "    rmv_params=cfg.param_model.rmv_params,\n",
    "    set_=args.set,\n",
    "    cfg=cfg,\n",
    "    mode='sampling',\n",
    "    img_ext=img_ext,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = dataset.__len__()\n",
    "img_path = file_utils._list_image_files_recursively(f\"{img_dataset_path}/{args.set}\")\n",
    "\n",
    "denoised_fn = None\n",
    "pl_sampling = inference_utils.PLSampling(model_dict=model_dict, \n",
    "                                            diffusion=diffusion, \n",
    "                                            reverse_fn=diffusion.ddim_reverse_sample_loop, \n",
    "                                            forward_fn=diffusion.ddim_sample_loop,\n",
    "                                            denoised_fn=denoised_fn,\n",
    "                                            cfg=cfg,\n",
    "                                            args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECA Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, '/home/mint/guided-diffusion/preprocess_scripts/Relighting_preprocessing_tools/DECA/')\n",
    "from decalib.deca import DECA\n",
    "from decalib.datasets import datasets \n",
    "from decalib.utils import util\n",
    "from decalib.utils.config import cfg as deca_cfg\n",
    "from decalib.utils.tensor_cropper import transform_points\n",
    "\n",
    "def deca_estimate(img_path):\n",
    "    mask_dir = '/home/mint/guided-diffusion/preprocess_scripts/Relighting_preprocessing_tools/DECA/data/'\n",
    "    testdata = datasets.TestData(img_path, iscrop=True, face_detector='fan', sample_step=10)\n",
    "    f_mask = np.load(f'{mask_dir}/FLAME_masks_face-id.pkl', allow_pickle=True, encoding='latin1')\n",
    "    v_mask = np.load(f'{mask_dir}/FLAME_masks.pkl', allow_pickle=True, encoding='latin1')\n",
    "    mask={\n",
    "        'v_mask':v_mask['face'].tolist(),\n",
    "        'f_mask':f_mask['face'].tolist()\n",
    "    }\n",
    "    deca_cfg.model.use_tex = False\n",
    "    deca_cfg.rasterizer_type = 'standard'\n",
    "    deca_cfg.model.extract_tex = False\n",
    "    deca = DECA(config = deca_cfg, device=device, mode='deca', mask=mask)\n",
    "    \n",
    "    name = testdata[0]['imagename']\n",
    "    ext = testdata[0]['imageext']\n",
    "    images = testdata[0]['image'].to(device)[None,...]\n",
    "    original_image = testdata[0]['original_image'][None, ...].to(device)\n",
    "    tform = testdata[0]['tform'][None, ...]\n",
    "    \n",
    "    with th.no_grad():\n",
    "        codedict = deca.encode(images)\n",
    "        tform_inv = th.inverse(tform).transpose(1,2).to(device)\n",
    "        _, orig_visdict = deca.decode(codedict, render_orig=True, original_image=original_image, tform=tform_inv, use_template=False, mean_cam=None)    \n",
    "        orig_visdict['inputs'] = original_image\n",
    "        \n",
    "    del deca\n",
    "    \n",
    "    return codedict, orig_visdict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_img_name = \"046_01_01_051_07\"\n",
    "# dst_img_name = \"046_01_01_051_13\"\n",
    "# src_img_name = \"044_01_01_051_04\"\n",
    "# dst_img_name = \"044_01_01_051_13\"\n",
    "src_img_name = \"002_01_01_051_06\"\n",
    "dst_img_name = \"002_01_01_051_01\"\n",
    "\n",
    "# Rendered\n",
    "src_img_path = f'/data/mint/DPM_Dataset/MultiPIE/mp_aligned/valid/{src_img_name}.png'\n",
    "src_deca_p, visdict = deca_estimate(img_path=src_img_path)\n",
    "vis_img = th.cat((visdict['shape_images'], visdict['inputs']))\n",
    "show(torchvision.utils.make_grid(vis_img))\n",
    "\n",
    "dst_img_path = f'/data/mint/DPM_Dataset/MultiPIE/mp_aligned/valid/{dst_img_name}.png'\n",
    "dst_deca_p, visdict = deca_estimate(img_path=dst_img_path)\n",
    "vis_img = th.cat((visdict['shape_images'], visdict['inputs']))\n",
    "show(torchvision.utils.make_grid(vis_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arcface Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(2, '/home/mint/guided-diffusion/preprocess_scripts/Relighting_preprocessing_tools/Arcface/')\n",
    "from get_arcface_emb import get_arcface_emb\n",
    "\n",
    "_ , src_faceemb = get_arcface_emb(img_path=src_img_path, device='cuda', arcface_ckpt_path='../cond_utils/arcface/pretrained/BEST_checkpoint_r18.tar')\n",
    "src_faceemb = src_faceemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Non-spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nonspatial condition\n",
    "cond = {'cond_params':[], 'cond_img':[]}\n",
    "n_step = 3\n",
    "# repeats\n",
    "for p in cfg.param_model.params_selector:\n",
    "    if p == 'faceemb':\n",
    "        cond[p] = np.repeat(src_faceemb, repeats=n_step, axis=0)\n",
    "    elif p == 'shadow':\n",
    "        cond[p] = np.zeros((n_step, 1)) + (-1.6358844929000687)\n",
    "    else:\n",
    "        cond[p] = np.repeat(src_deca_p[p].cpu().numpy(), repeats=n_step, axis=0)\n",
    "    cond[p] = th.tensor(cond[p]).to(device)\n",
    "        \n",
    "# concat\n",
    "for p in cfg.param_model.params_selector:\n",
    "    cond['cond_params'].append(cond[p])\n",
    "cond['cond_params'] = th.cat((cond['cond_params']), dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-spatial condition\n",
    "cond['light'] = src_deca_p['light']\n",
    "cond['light'] = mani_utils.interp_cond(src_cond=src_deca_p['light'],\n",
    "                                       dst_cond=dst_deca_p['light'], \n",
    "                                       n_step=n_step, \n",
    "                                       interp_fn=mani_utils.lerp\n",
    "                                    )\n",
    "cond['light'] = th.tensor(cond['light']).to(device)\n",
    "\n",
    "def deca_render(img_path, cond, B):\n",
    "    mask_dir = '/home/mint/guided-diffusion/preprocess_scripts/Relighting_preprocessing_tools/DECA/data/'\n",
    "    testdata = datasets.TestData(img_path, iscrop=True, face_detector='fan', sample_step=10)\n",
    "    \n",
    "    original_image = testdata[0]['original_image'][None].to(device).float().repeat(B, 1, 1, 1) / 255.0\n",
    "    # original_image = testdata[0]['original_image'][None].repeat(B, 1, 1, 1).to(device) / 255.0\n",
    "    tform = testdata[0]['tform'].repeat(B, 1).to(device)[None].reshape(-1, 3, 3)\n",
    "    tform_inv = th.inverse(tform).transpose(1,2)\n",
    "    cond['images'] = testdata[0]['image'].to(device)[None].float().repeat(B, 1, 1, 1)\n",
    "    \n",
    "    f_mask = np.load(f'{mask_dir}/FLAME_masks_face-id.pkl', allow_pickle=True, encoding='latin1')\n",
    "    v_mask = np.load(f'{mask_dir}/FLAME_masks.pkl', allow_pickle=True, encoding='latin1')\n",
    "    mask={\n",
    "        'v_mask':v_mask['face'].tolist(),\n",
    "        'f_mask':f_mask['face'].tolist()\n",
    "    }\n",
    "    deca_cfg.model.use_tex = False\n",
    "    deca_cfg.rasterizer_type = 'standard'\n",
    "    deca_cfg.model.extract_tex = False\n",
    "    \n",
    "    images = testdata[0]['image'].to(device)[None,...]\n",
    "    deca = DECA(config = deca_cfg, device=device, mode='deca', mask=mask)\n",
    "    \n",
    "    start = time.time()\n",
    "    _, orig_visdict = deca.decode(cond, \n",
    "                                render_orig=True, \n",
    "                                original_image=original_image, \n",
    "                                tform=tform_inv.to(device), \n",
    "                                use_template=False, \n",
    "                                mean_cam=None, \n",
    "                                use_detail=False,\n",
    "                                rotate_normals=False,\n",
    "                            )  \n",
    "    rendered_image = orig_visdict['shape_images']\n",
    "    print(\"Rendering time : \", time.time() - start)\n",
    "    return rendered_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "render_face = deca_render(src_img_path, cond, B=n_step)\n",
    "show(torchvision.utils.make_grid(render_face))\n",
    "img_size = cfg.img_model.image_size\n",
    "tmp = []\n",
    "for j in range(render_face.shape[0]):\n",
    "    r_tmp = render_face[j].cpu().numpy().transpose((1, 2, 0))\n",
    "    r_tmp = cv2.resize(r_tmp, (img_size, img_size), cv2.INTER_AREA)\n",
    "    r_tmp = np.transpose(r_tmp, (2, 0, 1))\n",
    "    tmp.append(r_tmp)\n",
    "render_face = np.stack(tmp, axis=0)\n",
    "render_face = th.tensor(render_face)\n",
    "show(torchvision.utils.make_grid(render_face))\n",
    "cond['deca_masked_face_images_woclip'] = render_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_segment(segment_part, img_path):\n",
    "    face_segment_anno = PIL.Image.open(img_path)\n",
    "\n",
    "    face_segment_anno = np.array(face_segment_anno)\n",
    "    bg = (face_segment_anno == 0)\n",
    "    skin = (face_segment_anno == 1)\n",
    "    l_brow = (face_segment_anno == 2)\n",
    "    r_brow = (face_segment_anno == 3)\n",
    "    l_eye = (face_segment_anno == 4)\n",
    "    r_eye = (face_segment_anno == 5)\n",
    "    eye_g = (face_segment_anno == 6)\n",
    "    l_ear = (face_segment_anno == 7)\n",
    "    r_ear = (face_segment_anno == 8)\n",
    "    ear_r = (face_segment_anno == 9)\n",
    "    nose = (face_segment_anno == 10)\n",
    "    mouth = (face_segment_anno == 11)\n",
    "    u_lip = (face_segment_anno == 12)\n",
    "    l_lip = (face_segment_anno == 13)\n",
    "    neck = (face_segment_anno == 14)\n",
    "    neck_l = (face_segment_anno == 15)\n",
    "    cloth = (face_segment_anno == 16)\n",
    "    hair = (face_segment_anno == 17)\n",
    "    hat = (face_segment_anno == 18)\n",
    "    face = np.logical_or.reduce((skin, l_brow, r_brow, l_eye, r_eye, eye_g, l_ear, r_ear, ear_r, nose, mouth, u_lip, l_lip))\n",
    "    \n",
    "    if segment_part == 'faceseg_nohead':\n",
    "        seg_m = ~(face | neck | hair)\n",
    "    return seg_m\n",
    "\n",
    "# Load image.\n",
    "src_img = PIL.Image.open(src_img_path)#.resize((128, 128), PIL.Image.ANTIALIAS)\n",
    "src_img = np.array(src_img)\n",
    "\n",
    "bg_img_path = f'/data/mint/DPM_Dataset/MultiPIE/face_segment/valid/anno/anno_{src_img_name}.png'\n",
    "mask = face_segment('faceseg_nohead', bg_img_path)[..., None]\n",
    "bg = src_img * mask\n",
    "show(torchvision.utils.make_grid(th.tensor(bg).permute(2, 0, 1)[None]))\n",
    "\n",
    "bg = PIL.Image.fromarray(bg.astype(np.uint8)).resize((img_size, img_size), PIL.Image.ANTIALIAS)\n",
    "bg = np.transpose(bg, [2, 0, 1])\n",
    "bg = (bg / 127.5) - 1\n",
    "bg = th.tensor(bg)\n",
    "cond['faceseg_nohead'] = bg[None]\n",
    "show(torchvision.utils.make_grid((bg + 1) * 0.5))\n",
    "\n",
    "\n",
    "src_img = PIL.Image.open(src_img_path).resize((128, 128), PIL.Image.ANTIALIAS)\n",
    "src_img = th.tensor(np.array(src_img)).permute(2, 0, 1)\n",
    "src_img = (src_img / 127.5) - 1\n",
    "cond['src_img'] = src_img[None]\n",
    "show(torchvision.utils.make_grid((src_img + 1) * 0.5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_f = copy.deepcopy(cond)\n",
    "for k in cond.keys():\n",
    "    if isinstance(cond_f[k], list):\n",
    "        print(k, len(cond_f[k]))\n",
    "    else:\n",
    "        print(k, cond_f[k].shape)\n",
    "        \n",
    "cond_f['src_img'] = th.repeat_interleave(cond_f['src_img'], repeats=3, dim=0)\n",
    "cond_f['faceseg_nohead'] = th.repeat_interleave(cond_f['faceseg_nohead'], repeats=3, dim=0)\n",
    "\n",
    "show(torchvision.utils.make_grid((cond_f['src_img'] + 1) * 0.5))\n",
    "show(torchvision.utils.make_grid((cond_f['faceseg_nohead'] + 1) * 0.5))\n",
    "show(torchvision.utils.make_grid((cond_f['deca_masked_face_images_woclip'])))\n",
    "\n",
    "tmp = []\n",
    "for p in cfg.img_cond_model.in_image:\n",
    "    tmp.append(cond_f[p])\n",
    "\n",
    "cond_f['cond_img'] = th.cat(tmp, axis=1)\n",
    "print(cond_f['cond_img'].shape)\n",
    "show(torchvision.utils.make_grid((cond_f['cond_img'][:, 0:3])))\n",
    "show(torchvision.utils.make_grid((cond_f['cond_img'][:, 3:] + 1) * 0.5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse \n",
    "diffusion.num_timesteps = 1000\n",
    "cond_f['use_cond_xt_fn'] = False\n",
    "cond_f['dpm_cond_img'] = None\n",
    "cond_f['use_render_itp'] = False\n",
    "cond_rev = copy.deepcopy(cond_f)\n",
    "cond_rev = dict_slice(in_d=cond_rev, keys=['cond_params', 'cond_img'], n=1) # Slice only 1st image out for inversion\n",
    "if cfg.img_cond_model.apply:\n",
    "    cond_rev = pl_sampling.forward_cond_network(model_kwargs=cond_rev)\n",
    "    \n",
    "reverse_ddim_sample = pl_sampling.reverse_proc(x=cond_f['src_img'][0:1, ...], \n",
    "                                               model_kwargs=cond_rev, \n",
    "                                               store_mean=True)\n",
    "noise_map = reverse_ddim_sample['final_output']['sample']\n",
    "rev_mean = reverse_ddim_sample['intermediate']\n",
    "\n",
    "#NOTE: rev_mean WILL BE MODIFIED; This is for computing the ratio of inversion (brightness correction).\n",
    "sample_ddim = pl_sampling.forward_proc(\n",
    "    noise=noise_map,\n",
    "    model_kwargs=cond_rev,\n",
    "    store_intermediate=False,\n",
    "    rev_mean=rev_mean)\n",
    "\n",
    "# Relight!\n",
    "cond_f['use_render_itp'] = True\n",
    "cond_relight = copy.deepcopy(cond_f)\n",
    "if cfg.img_cond_model.apply:\n",
    "    cond_relight = pl_sampling.forward_cond_network(model_kwargs=cond_relight)\n",
    "    \n",
    "assert noise_map.shape[0] == 1\n",
    "rev_mean_first = [x[:1] for x in rev_mean]\n",
    "\n",
    "relight_out = pl_sampling.forward_proc(\n",
    "    noise=th.repeat_interleave(noise_map, repeats=n_step, dim=0),\n",
    "    model_kwargs=cond_relight,\n",
    "    store_intermediate=False,\n",
    "    add_mean=rev_mean_first)\n",
    "    \n",
    "out = relight_out[\"final_output\"][\"sample\"]\n",
    "show(torchvision.utils.make_grid((out + 1) * 0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e726457752a9f31dd08e885b4d7ad782b9d2db89819f0ec82c996add1b497d76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
